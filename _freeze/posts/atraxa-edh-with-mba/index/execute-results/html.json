{
  "hash": "9f5cad8464cb097011e4a28bfa1ff9b2",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Analysing Card Choices with Frequent Pattern Mining\"\nauthor: \"Derek Rodriguez\"\ndate: \"2024-11-19\"\ncategories: [Python, Pattern Mining, Market Basket Analysis, Web Scraping, MTG]\nimage: \"atraxa-praetors-voice.jpg\"\n---\n\n\nIn this post, I use frequent pattern mining or association rule mining, which is typically used for market basket analysis, in order to analyze card choices in more than a thousand user-uploaded decks. I will be using Python's **requests**, **beautifulsoup4**, **mtg_parser**, and **mlxtend** packages to perform the main activities.\n\nWe've learned association rule mining in multiple courses in MITB (Customer Analytics, Data Science for Business) primarily for Market Basket Analysis. The same technique should be applicable outside retail, where the questions can be addressed by finding highly correlated items or sets of items.\n\n# Introduction\n\nFor those unaware, [Magic the Gathering](https://magic.wizards.com/en) (MTG) is a trading card game that was released in 1993, and I have been playing it (casually) since 1995. There are a number of formats, or ways to play, in this game, but the most popular for a number of years now is [Commander](https://magic.wizards.com/en/formats/commander). The format requires each player to build a deck of 100 cards which is helmed by, and built around a \"commander\".\n\nThe most popular commander for the past two years, based on the site [EDHREC.com](https://edhrec.com/commanders), as of writing is Atraxa, Praetors' Voice. EDHREC already provides an analysis of typical decklists in their site for [Atraxa](https://edhrec.com/commanders/atraxa-praetors-voice) and other commanders, but I want to see if I can do my own simple analysis using Python.\n\n![](https://cards.scryfall.io/normal/front/d/0/d0d33d52-3d28-4635-b985-51e126289259.jpg?1599707796){fig-align=\"center\" width=\"40%\"}\n\nAt the minimum, I want to be able to find the typical cards that users include in the deck and see if I can use the techniques to identify different themes or builds and find any interesting card choices.\n\n# Getting the Decklists from MTG Goldfish Results pages\n\nTo perform any analysis, we need to get data, and for this one, we need data on individual decks for Atraxa. There are individual decklists uploaded in [MTG Goldfish](https://www.mtggoldfish.com/) which we should be able to use. The only thing is that these lists are not contained in a single file, but are stored in separate pagesâ€“ so the data needs to be scraped of the web.\n\nFor this task, we will first us the Python [Requests](https://pypi.org/project/requests/) package is a package for doing HTTP requests and is the \"basic\" package for web scraping. A request for a webpage is sent via the `get()` function which returns a Response object.\n\nIn the code chunk below, we pass the link to the search results page into `get()` and then check whether the request was successful. (based on a `status_code` value of 200) The webpage, or its source code, will be accessible via the `text` object in the response.\n\n::: {#f08077d4 .cell execution_count=1}\n``` {.python .cell-code}\nimport requests\n\n# Define the URL for the search results\nurl = 'https://www.mtggoldfish.com/deck_searches/create?commit=Search&counter=3&deck_search%5Bdate_range%5D=01%2F01%2F2022+-+11%2F15%2F2024&deck_search%5Bdeck_search_card_filters_attributes%5D%5B0%5D%5Bcard%5D=Atraxa%2C+Praetors%27+Voice&deck_search%5Bdeck_search_card_filters_attributes%5D%5B0%5D%5Bquantity%5D=1&deck_search%5Bdeck_search_card_filters_attributes%5D%5B0%5D%5Btype%5D=commander&deck_search%5Bdeck_search_card_filters_attributes%5D%5B1%5D%5Bcard%5D=&deck_search%5Bdeck_search_card_filters_attributes%5D%5B1%5D%5Bquantity%5D=1&deck_search%5Bdeck_search_card_filters_attributes%5D%5B1%5D%5Btype%5D=maindeck&deck_search%5Bdeck_search_card_filters_attributes%5D%5B2%5D%5Bcard%5D=&deck_search%5Bdeck_search_card_filters_attributes%5D%5B2%5D%5Bquantity%5D=1&deck_search%5Bdeck_search_card_filters_attributes%5D%5B2%5D%5Btype%5D=maindeck&deck_search%5Bformat%5D=&deck_search%5Bname%5D=&deck_search%5Bplayer%5D=&deck_search%5Btypes%5D%5B%5D=&deck_search%5Btypes%5D%5B%5D=tournament&deck_search%5Btypes%5D%5B%5D=user&page=1&utf8=%E2%9C%93'\n\n# Make an HTTP GET request to fetch the webpage content\nresponse = requests.get(url)\n\n# Check if the request was successful\nif response.status_code == 200:\n    webpage = response.text\n    print(\"Page fetched successfully!\")\nelse:\n    print(\"Failed to fetch the page:\", response.status_code)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPage fetched successfully!\n```\n:::\n:::\n\n\nFor web scraping, [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) is helpful in navigating pulled webpages. Aside from being able to show the code in a more readable format, it is very useful in finding tags in the code which is what we will use it for here.\n\nWe first import the package, specifically the `BeautifulSoup()` function, and use it to convert the webpage results into a BeautifulSoup object.\n\n::: {#d6bf9b85 .cell execution_count=2}\n``` {.python .cell-code}\nfrom bs4 import BeautifulSoup\n\n# Parse the HTML content\nsoup = BeautifulSoup(webpage, 'html.parser')\n```\n:::\n\n\nWe then need to inspect the code to identify what it is that we need to get from the webpage. In our case we see that the decklist URL is stored within items tagged 'a' within ones tagged as 'td'.\n\nWe test this using the code chunk below. Note that the decklist URL is actually just the suffix of the URL and not the whole URL.\n\n::: {#81f10bd7 .cell execution_count=3}\n``` {.python .cell-code}\nresults = soup.find_all('td')\ntd_elements = soup.find_all('td')\nhrefs = []\nfor td in td_elements:\n  a_tag = td.find('a')\n  if a_tag:\n    hrefs.append(a_tag['href'])\n    \nprint(hrefs)\n```\n:::\n\n\nAs the approach was successful, we modify the preceding chunks of code to go through all 67 results pages to do the page request and then extracting the decklist URLs.\n\n::: {#a33071fc .cell execution_count=4}\n``` {.python .cell-code}\n# Create the template url\nurlstart = 'https://www.mtggoldfish.com/deck_searches/create?commit=Search&counter=3&deck_search%5Bdate_range%5D=01%2F01%2F2022+-+11%2F15%2F2024&deck_search%5Bdeck_search_card_filters_attributes%5D%5B0%5D%5Bcard%5D=Atraxa%2C+Praetors%27+Voice&deck_search%5Bdeck_search_card_filters_attributes%5D%5B0%5D%5Bquantity%5D=1&deck_search%5Bdeck_search_card_filters_attributes%5D%5B0%5D%5Btype%5D=commander&deck_search%5Bdeck_search_card_filters_attributes%5D%5B1%5D%5Bcard%5D=&deck_search%5Bdeck_search_card_filters_attributes%5D%5B1%5D%5Bquantity%5D=1&deck_search%5Bdeck_search_card_filters_attributes%5D%5B1%5D%5Btype%5D=maindeck&deck_search%5Bdeck_search_card_filters_attributes%5D%5B2%5D%5Bcard%5D=&deck_search%5Bdeck_search_card_filters_attributes%5D%5B2%5D%5Bquantity%5D=1&deck_search%5Bdeck_search_card_filters_attributes%5D%5B2%5D%5Btype%5D=maindeck&deck_search%5Bformat%5D=&deck_search%5Bname%5D=&deck_search%5Bplayer%5D=&deck_search%5Btypes%5D%5B%5D=&deck_search%5Btypes%5D%5B%5D=tournament&deck_search%5Btypes%5D%5B%5D=user&page='\nurlend = '&utf8=%E2%9C%93'\nhrefs = []\n\nfor i in range(1,68):\n  url = urlstart + str(i) + urlend\n  response = requests.get(url)\n  if response.status_code == 200:\n      webpage = response.text\n  else:\n      print(\"Failed to fetch the page\", i, \":\", response.status_code)\n  soup = BeautifulSoup(webpage, 'html.parser')\n  td_elements = soup.find_all('td')\n  for td in td_elements:\n    a_tag = td.find('a')\n      if a_tag:\n        hrefs.append(a_tag['href'])\n```\n:::\n\n\n\n\n::: {#fbf79dfd .cell execution_count=6}\n``` {.python .cell-code}\nlen(hrefs)\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\n2136\n```\n:::\n:::\n\n\nWe were able to get 2136 URL (suffixes) from the 67 search result pages. The next step is to extract the individual decklists from each of these URLs.\n\n\n\n## Using MTG Parser as a structured way of pulling MTG decklists\n\nI found a package called [mtg_parser](https://pypi.org/project/mtg_parser/) for easily scraping MTG decklists off popular webpages. Their function `parse_deck()` retuns an iterable containing items of a custom class card which includes the quantity and the name of each card in a decklist.\n\nThe code chunk below loads the package, and then iterates through the list of deck suffixes generated earlier and passes them into `parse_deck()`. We include error handling using `try-except` as we are not sure whether each link is still live or contains a readable decklist. There are also multiple ways the link can be built. (either with or without *#paper*) The result is initially stored as a nested list containing the deck numbers and the card names.\n\n::: {#db47f5d7 .cell execution_count=8}\n``` {.python .cell-code}\nimport mtg_parser\n\ndecklist = []\nfail_count = 0\nfor suffix in hrefs:\n    deck = suffix[-7:]\n    try:\n        try:\n            url = 'https://www.mtggoldfish.com/deck/' + str(deck) + '#paper'\n            cards = mtg_parser.parse_deck(url)\n            for card in cards:\n              decklist.append([deck, card.name])\n        except:\n            url = 'https://www.mtggoldfish.com/deck/' + str(deck)\n            cards = mtg_parser.parse_deck(url)\n            for card in cards:\n              decklist.append([deck, card.name])\n    except:\n        print('Failed for deck', deck)\n        fail_count += 1\n```\n:::\n\n\nWe included a counter to already check how many decks (deck links) didn't work with this method, and it appears that out of the 2136 decks, 5 failed so we still have a good amount of 2131 decks to work with.\n\nAlso note that the last code chunk takes very long to execute as the request is done for each of the 2136 decklist pages. I have saved the results in a file so I don't need to run the code again once I restart the Python session.\n\nWe then convert the resulting list object into a dataframe which will be easier to work with. This is done simply using the **pandas** package and its `DataFrame()` function which converts some collections, like a list or dictionary, into a dataframe. We pass the appropriate variable names in the columns agreement that indicate the first element as the deck (denoted by the link suffix) and the card name.\n\n\n\n::: {#8c398bc8 .cell execution_count=10}\n``` {.python .cell-code}\nimport pandas as pd\ncolumns = [\"deck\", \"card\"]\ndecklist = pd.DataFrame(decklist, columns = columns)\n```\n:::\n\n\n::: {#285d8225 .cell execution_count=11}\n``` {.python .cell-code}\ndecklist.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>deck</th>\n      <th>card</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6755629</td>\n      <td>Atraxa, Praetors' Voice</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6755629</td>\n      <td>Birds of Paradise</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>6755629</td>\n      <td>Deathrite Shaman</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6755629</td>\n      <td>Delighted Halfling</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6755629</td>\n      <td>Esper Sentinel</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n# A Brief Introduction of Market Basket Analysis and Association Rule Mining\n\nMarket Basket Analysis is a technique that retailers or marketers use to understand buying patterns of customers by looking at items that are frequently bought together. When applied in retail, insights from Market Basket Analysis can lead to help suggest better placement of products, or opportunities to bundle or cross-sell products.\n\nMarket basket analysis is typically done by Frequent Pattern or Association Rule Mining. The general idea is that we are interested in finding items that are typically purchased together, or that appear together in a 'market basket'. It is called Association Rule Mining since it looks for interesting or frequent (based on a predefined threshold) rules, which are in the form:\n\n$$\nA \\Rightarrow B \n$$\n\nThis simply means that if a basket contains A, then it contains B. A and B can be single or multiple items.\n\nThere are three basic measures that will be relevant in association rules mining:\n\n1.  **Support** - This is a measure of how often a set of items occurs. It may be denoted as the number of times the set is observed, but is typically represented as a proportion or a probability.\n\n2.  **Confidence** - This is computed per association rule as the support for the rule divided by the support for the *antecedent*, or the left side of the rule. (the right side is called the *consequent*) For the association rule $A \\Rightarrow B$, the confidence will then be $Support(A,B) / Support(A)$ This can be interpreted as the probability of B appearing in a basket, if A is in the basket.\n\n3.  **Lift** - This is the confidence of the rule divided by the support of the consequent. This then translates to $Lift(A \\Rightarrow B) = Confidence(A \\Rightarrow B)/Suppport(A) = Support(A,B) / Support(A) Support(B)$ Lift is a measure of how likely the antecedent is to occur with the antecedent, than in general or than expected. Lift can be viewed as the strength of the rule.\n\nMarket basket analysis is typically interested with rules that have high enough support (occurs often enough) and high enough confidence (have high association) and a lift of at least 1. (occur more often with the antecedent than without)\n\nThere is a lot of material available online and on print on this topic, but what we have covered should be enough to support the use of the technique for our objective.\n\n# Applying Association Rule Mining to the Atraxa Decklists\n\nSo how can Association Rule Mining be used in analyzing decklists? There are a few things that it should be able to provide insights on based on the information we have. We will be using the [**frequent_patterns**](https://rasbt.github.io/mlxtend/api_subpackages/mlxtend.frequent_patterns/) subpackage within **mlextend** which gives access to two useful functions for MBA or Association Rules Mining: `apriori()` and `association_rules()`.\n\nWe first need to transform the data into the correct format, before running the apriori algorithm and then analyzing the results.\n\n## Identifying Staples\n\nWhile it is tempting to use the algorithm for association rules mining right away, it will take too long to run it on the whole dataset that we have. (I tried, but I gave up after more than 12 hours) We can have an idea of how much effort will be required to run the algorithm by checking the size of the dataset. We use `nunique()` to count the unique values for each of the two columns in `decklist`.\n\n::: {#c9601c4f .cell execution_count=12}\n``` {.python .cell-code}\ndecklist[\"deck\"].nunique()\ndecklist[\"card\"].nunique()\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\n6804\n```\n:::\n:::\n\n\nUntouched, this would mean converting this first into a 2131 x 6804 dataframe. The number of itemsets can also be very high as the cap will be $2^n$ where $n$ is the number of unique cards. We can use standard **pandas** functions to identify very frequent cards and very infrequent cardsâ€“ which are not necessary for our other questions about the Atraxa decks.\n\n### Which cards appear in most of the submitted decks?\n\nWe can count the number of times each card appears in a deck by using `value_counts()`. We add a new column to indicate the percentage of the 2136 decks that contain that card.\n\n::: {#ad6828eb .cell execution_count=13}\n``` {.python .cell-code}\ncard_counts = decklist['card'].value_counts().reset_index()\ncard_counts.columns = ['card', 'count']\ncard_counts = card_counts.sort_values(by='count', ascending=False)\ncard_counts['pct'] = round(card_counts['count'] / 2136 * 100, 1)\ncard_counts.head(10)\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>card</th>\n      <th>count</th>\n      <th>pct</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Atraxa, Praetors' Voice</td>\n      <td>2134</td>\n      <td>99.9</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Forest</td>\n      <td>1963</td>\n      <td>91.9</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Island</td>\n      <td>1962</td>\n      <td>91.9</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Plains</td>\n      <td>1943</td>\n      <td>91.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Swamp</td>\n      <td>1937</td>\n      <td>90.7</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Command Tower</td>\n      <td>1888</td>\n      <td>88.4</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Sol Ring</td>\n      <td>1786</td>\n      <td>83.6</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Arcane Signet</td>\n      <td>1583</td>\n      <td>74.1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Exotic Orchard</td>\n      <td>1307</td>\n      <td>61.2</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Evolution Sage</td>\n      <td>1301</td>\n      <td>60.9</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nThe output shows that there are (only) five cards that appear in at least 90% of decksâ€“ Atraxa, and the four basic lands. (which are typical 'energy' sources for the game) There are only three that appear in 70-80% of decks and these are cards that appear in almost every deck in the format. The #9 card, *Evolution Sage* is very specific to decks of this strategy, but it only appears in 61% of decks.\n\nLet's check out the next 10 elements of the list with the following code.\n\n::: {#b2e4f94e .cell execution_count=14}\n``` {.python .cell-code}\nprint(card_counts.iloc[10:20])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                    card  count   pct\n10        Karn's Bastion   1218  57.0\n11  Swords to Plowshares   1147  53.7\n12     Astral Cornucopia   1130  52.9\n13     Tezzeret's Gambit   1122  52.5\n14         Thrummingbird   1104  51.7\n15     Chromatic Lantern   1096  51.3\n16             Cultivate   1057  49.5\n17       Inexorable Tide   1010  47.3\n18               Farseek    999  46.8\n19         Temple Garden    932  43.6\n```\n:::\n:::\n\n\nThe next ten cards include cards specific to this deck's strategy (e.g., *Karn's Bastion*) but also contains generic cards (e..g, *Swords to Plowshares*) or land cards. (e.g., *Temple Garden*) The frequency is getting quite low as the last four cards appear in less than half of the submitted decks.\n\n### How many cards appear in only a handful of decks?\n\nWe can use the same approach as earlier to count the card counts, and then display the twenty lowest counts (most likely 1-20 decks) using `head()`.\n\n::: {#db3c756e .cell execution_count=15}\n``` {.python .cell-code}\ncount_counts = card_counts['count'].value_counts().reset_index()\ncount_counts.columns = ['card_count', 'count']\ncount_counts = count_counts.sort_values(by='card_count', ascending=True)\ncount_counts.head(20)\nprint('\\n', sum(count_counts.head(20)[\"count\"]))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n 5696\n```\n:::\n:::\n\n\nThere are 2320 (of the 6804) cards that only appear in one decklist, and 5696 in total that appear in 20 or less decklists. This means that only 1,108 cards appear in more than 21 decklists.\n\n### Cleaning up the Decklists\n\nWe end this part by trimming down decklist by removing the very frequent and the very infrequent cards. First, we bring the counts into `decklist` by joining it with card_counts using `merge()`.\n\n::: {#cfda8c24 .cell execution_count=16}\n``` {.python .cell-code}\nimport pandas as pd\ndecklist = pd.merge(decklist, card_counts, on = 'card', how = 'left')\n```\n:::\n\n\nWe want to exclude the top 9 cards, which are those that appear in 1309 or more decks, and we also want to exclude the bottom 5696 carrds, or the ones that appear in 20 decks or less.\n\n::: {#80b035ed .cell execution_count=17}\n``` {.python .cell-code}\ndecklist['card'].nunique()\ndecklist_trimmed = decklist[decklist['count'] > 20]\ndecklist_trimmed = decklist_trimmed[decklist_trimmed['count'] < 1309]\ndecklist_trimmed['card'].nunique()\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```\n1100\n```\n:::\n:::\n\n\nWe also know that there are more lands that are very common in the Atraxa decks. We create a list of the most common of these (`other_common_lands`) and then\n\n\n\n::: {#ecceedb7 .cell execution_count=19}\n``` {.python .cell-code}\n# Filter out rows where itemsets contain any of the common lands\ndecklist_trimmed = decklist_trimmed[~decklist_trimmed['card'].apply(lambda x: x in other_common_lands)]\n```\n:::\n\n\n::: {#c7cb5254 .cell execution_count=20}\n``` {.python .cell-code}\ndecklist_trimmed['card'].nunique()\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```\n1077\n```\n:::\n:::\n\n\nThis step reduced the number of unique cards from 6804 to 1100 then to 1077, which could be more workable for the algorithms we are going to apply. We'll remove the unnecessary columns first since we only need the deck id and the card names that we originally started with.\n\n::: {#f8ca8fc6 .cell execution_count=21}\n``` {.python .cell-code}\ndecklist_trimmed = decklist_trimmed.drop(columns =['count', 'pct'])\n```\n:::\n\n\n## Transforming the decklists into the right format\n\nThe `apriori()` function requires a dataframe where each row is a transaction (or basket, customer, or, in our case, a deck) while each column corresponds to an item. (i.e., a card) The value will be a binary (True/False or 1/0) which indicates whether the card is in that specific deck or not.\n\nWe use the code chunk below to perform this transformation, but there should be multiple ways to achieve this. The resulting object, as expected, would be a 2131 x 6804 dataframe. The values are all True/False which are easier for `apriori()` to work with.\n\n::: {#3416345b .cell execution_count=22}\n``` {.python .cell-code}\nimport pandas as pd\ndecklists_encoded = decklist_trimmed.drop_duplicates()\ndecklists_encoded= decklists_encoded.pivot(index='deck', columns='card', values='card')\n\n# Fill NaN values with False (optional)\ndecklists_encoded = decklists_encoded.notna()\n\n# Reset the index if needed\ndecklists_encoded.reset_index(inplace=True)\ndecklists_encoded = decklists_encoded.drop('deck', axis=1)\n```\n:::\n\n\n\n\n\n\n## Running the Algorithm\n\nThe `apriori()` and `fpgrowth()` function is used to identify frequent item sets and returns an object which contains the itemsets and their support. The functions require a dataframe (described earlier) as a mandatory input. These differ by the way they identify frequent itemsets. For larger datasets, `fpgrowth()` will typically be more efficient in finding the itemsets.\n\nThe user can specify a minimum support threshold (`min_support`) for the function, otherwise it defaults to 0.5. This default value is a bit too high especially as we have not done any exploratory analysis to understand what is frequent or infrequent. We will use a value of 0.05 or 5% for our case. We also specify True for the `use_colnames` argument to indicate that the column names and not the indices will be used for the results. We also add a maximum itemset size of 5 using the `max_len` argument in order to limit the number of subsets scanned by the algorithm.\n\n::: {#009077d8 .cell execution_count=25}\n``` {.python .cell-code}\nfrom mlxtend.frequent_patterns import apriori, fpgrowth\nfrequent_itemsets = fpgrowth(decklists_encoded, min_support=0.05, use_colnames=True, max_len = 5)\n```\n:::\n\n\n\n\n\n\n::: callout-warning\n#### Warning\n\nThis code chunk will still run a good amount of time even with the reductions that we made.\n\nConsider increasing the minimum support, trimming down the data, or using an even more efficient algorithm before performing this yourself for your own purpose.\n\nFor MBA, using the optional argument `max_len` is also desirable since it specifies the maximum size of the sets generated. A set size of 2 or 3 will lead to simple and practical use for retail purposes.\n:::\n\n## Identifying Frequent Sets\n\nWhile `apriori()` and `fpgrowth()` does not produce association rules yet, they already generate frequent itemsets based on the minimum support that we indicated. We can use the results to identify staples or very common or typical cards that users have included in their Atraxa decklists. For our analysis, we will consider cards that appear in 85% of decks as staples.\n\nWith the frequent itemset output, we should be able to identify any high frequency sets of cards.\n\n### Counting the itemset sizes\n\nThe first step we want to do before answering the next questions is indicate the number of items. This can be done quickly by just applying the `len()` function to each element of the `itemsets` column.\n\n::: {#7e02fe9b .cell execution_count=28}\n``` {.python .cell-code}\nfrequent_itemsets['size'] = frequent_itemsets['itemsets'].apply(len)\n```\n:::\n\n\nWe should be able to see a preview with the new column using `head()`\n\n::: {#d8218c47 .cell execution_count=29}\n``` {.python .cell-code}\nfrequent_itemsets.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=21}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>support</th>\n      <th>itemsets</th>\n      <th>size</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.612207</td>\n      <td>(Exotic Orchard)</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.537089</td>\n      <td>(Swords to Plowshares)</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.528638</td>\n      <td>(Astral Cornucopia)</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.525352</td>\n      <td>(Tezzeret's Gambit)</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.516432</td>\n      <td>(Thrummingbird)</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n### What are the most common set of cards included in Atraxa decks?\n\nAside from individual cards like the staples mentioned earlier, we expect that there are cards that will recur as a group across different users' decks. Some of these might just be staples, but some might be tied to specific strategies or '*builds*' for the Atraxa deck.\n\nWe can use the code below to find the most frequent set of five cards in the user submitted decks.\n\n::: {#b3b5906e .cell execution_count=30}\n``` {.python .cell-code}\nfrequent_itemsets[frequent_itemsets['size'] == 5].sort_values(by='support', ascending = False).head(1)\nprint(\"\\n\")\nfrequent_itemsets[frequent_itemsets['size'] == 5].sort_values(by='support', ascending = False).head(1)[\"itemsets\"].iloc[0]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=22}\n```\nfrozenset({'Evolution Sage',\n           'Exotic Orchard',\n           \"Karn's Bastion\",\n           'Swords to Plowshares',\n           \"Tezzeret's Gambit\"})\n```\n:::\n:::\n\n\n*Evolution Sage*, *Tezzeret's Gambit*, *Karn's Bastion*, *Exotic Orchard*, and Swords to Plowshares are a set of five cards (excluding the staples and lands that we deleted previously) that appear in 17% of Atraxa decks.\n\n### What is the next most frequent disjoint set of five cards?\n\nIf we just look at the top 5-card sets, we will see the same cards repeating over and over again. What if we wanted to find a unique set of frequently included 5-cards?\n\n::: {#e1e4ad7e .cell execution_count=31}\n``` {.python .cell-code}\nfrequent_itemsets[frequent_itemsets['size'] == 5].sort_values(by='support', ascending = False).head(10)\n```\n\n::: {.cell-output .cell-output-display execution_count=23}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>support</th>\n      <th>itemsets</th>\n      <th>size</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>413</th>\n      <td>0.169484</td>\n      <td>(Karn's Bastion, Evolution Sage, Exotic Orchar...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>382</th>\n      <td>0.167606</td>\n      <td>(Karn's Bastion, Evolution Sage, Exotic Orchar...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>408</th>\n      <td>0.167606</td>\n      <td>(Evolution Sage, Exotic Orchard, Astral Cornuc...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>399</th>\n      <td>0.167606</td>\n      <td>(Karn's Bastion, Evolution Sage, Exotic Orchar...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>18372</th>\n      <td>0.166197</td>\n      <td>(Karn's Bastion, Evolution Sage, Exotic Orchar...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>570</th>\n      <td>0.164319</td>\n      <td>(Karn's Bastion, Evolution Sage, Cultivate, Ex...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1204</th>\n      <td>0.163850</td>\n      <td>(Karn's Bastion, Evolution Sage, Inexorable Ti...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>492</th>\n      <td>0.163850</td>\n      <td>(Evolution Sage, Cultivate, Exotic Orchard, Te...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>846</th>\n      <td>0.163380</td>\n      <td>(Karn's Bastion, Evolution Sage, Exotic Orchar...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>410</th>\n      <td>0.161502</td>\n      <td>(Karn's Bastion, Evolution Sage, Astral Cornuc...</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nWe can compare each of the itemsets with the previous five card list until we find one which does not share any elements with it. We can use the following code which applies a simple function to the itemsets column and use that as a filter.\n\n::: {#be155976 .cell execution_count=32}\n``` {.python .cell-code}\ntop_5cardset = frequent_itemsets[frequent_itemsets['size'] == 5].sort_values(by='support', ascending = False).head(1)[\"itemsets\"].iloc[0]\nfrequent_5cards = frequent_itemsets[frequent_itemsets['size'] == 5].sort_values(by='support', ascending = False)\n\ndef contains_card(itemset, cards):\n  return any(item in cards for item in itemset)\n\nfrequent_5cards_other = frequent_5cards[~frequent_5cards['itemsets'].apply(lambda x: contains_card(x, top_5cardset))]\n```\n:::\n\n\nWe can then call the first element of the new object to find a disjoint set of five cards.\n\n::: {#37aa6009 .cell execution_count=33}\n``` {.python .cell-code}\nfrequent_5cards_other.head(1)\nprint(\"\\n\")\nfrequent_5cards_other.head(1)[\"itemsets\"].iloc[0]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=25}\n```\nfrozenset({'Ezuri, Stalker of Spheres',\n           'Infectious Inquiry',\n           'Prologue to Phyresis',\n           'Tekuthal, Inquiry Dominus',\n           \"Vraska's Fall\"})\n```\n:::\n:::\n\n\nThe output shows that this set of cards includes: *Infectious Inquiry*, *Prologue to Phyresis*, *Vraska's Fall*, *Tekuthal, Inquiry Dominus*, Ezuri, *Stalker of Spheres*. The support for this set is 0.138967-- meaning it appears in 13.9% of Atraxa decks. Note that this does not imply that this set of cards do not occur with the first five identified. We simply wanted to find a unique set of five which might or might not be used with the first five cards.\n\n## Generating Association Rules\n\nWhile we can discover a lot with the frequent itemsets, this is limited to questions about the set frequencies in isolation. We can use `association_rules()` to generate a list of rules which describe the correlation or likelihood of items being present with other items.\n\nThe function requires a frequent itemset dataframe as an input. The user can define the metric and the minimum value to use using the `metric` and `min_threshold` arguments. The former can accept 'support', 'confidence' or 'lift' as metrics.\n\nWe use the code chunk below to generate the association rules with a minimum *lift* of 1. The first line retains only the sets with a length of 1 to 3. This means that we will have at most two elements in the antecedent (left side) or consequent (right side) of each rule. We do this as we will focus on single card associations, and as this will reduce the number of rules significantly. The next line of the code chunk removes the size column that we added to the dataframe to bring it back to the right format.\n\n::: {#10c18a28 .cell execution_count=34}\n``` {.python .cell-code}\nfrom mlxtend.frequent_patterns import association_rules\n\nfrequent_itemsets = frequent_itemsets[frequent_itemsets['size'] < 4]\nfrequent_itemsets = frequent_itemsets.drop(columns =['size'])\n\nrules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1)\n```\n:::\n\n\n\n\n\n\n::: {#9f7ef9d9 .cell execution_count=37}\n``` {.python .cell-code}\nrules.shape\nprint(\"\\n\")\nrules.head()\nprint(\"\\n\")\nrules.columns\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\n\n\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=27}\n```\nIndex(['antecedents', 'consequents', 'antecedent support',\n       'consequent support', 'support', 'confidence', 'lift', 'leverage',\n       'conviction', 'zhangs_metric'],\n      dtype='object')\n```\n:::\n:::\n\n\nThe resulting dataframe has 288,366 rows or rules with 10 columns each. The antecedent and the consequent are each in separate columns. There are three columns for support, which are for the antecedent, the consequent, or for the two combined. Among the other metrics, we also have columns for the confidence and the lift.\n\nThe code chunk below will display the five rules with the highest lift.\n\n::: {#f3914abb .cell execution_count=38}\n``` {.python .cell-code}\ntop_5_rules = rules.sort_values(by='lift', ascending = False).head(5)[['antecedents', 'consequents', 'confidence', 'consequent support', 'lift']]\n\nprint(top_5_rules)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                               antecedents  \\\n190764  (Underground Sea, Tropical Island)   \n190769                            (Tundra)   \n190752         (Savannah, Underground Sea)   \n190757                            (Tundra)   \n190741                     (Tundra, Bayou)   \n\n                               consequents  confidence  consequent support  \\\n190764                            (Tundra)    0.972028            0.071362   \n190769  (Underground Sea, Tropical Island)    0.914474            0.067136   \n190752                            (Tundra)    0.965753            0.071362   \n190757         (Savannah, Underground Sea)    0.927632            0.068545   \n190741                   (Underground Sea)    0.979167            0.073239   \n\n             lift  \n190764  13.621181  \n190769  13.621181  \n190752  13.533255  \n190757  13.533255  \n190741  13.369391  \n```\n:::\n:::\n\n\nThe first rule can be read as: if a decklist contains *Tropical Island* and *Underground Sea*, it is 97% likely to have *Tundra*. The 13.621 lift value means that *Tundra* is 12.6x more likely to be seen with these two cards than in general.\n\n## Using Association Rules to find how cards are (potentially) being used\n\nAside from simply finding high lift rules, we can use the association rules by\n\nThe code below shows five cards that are used in less than 250 decks. We already have the counts in the card_counts dataframe so we just need to select with the appropriate mask.\n\n::: {#05894be4 .cell execution_count=39}\n``` {.python .cell-code}\ncard_counts[card_counts['count'] < 250].head()\n```\n\n::: {.cell-output .cell-output-display execution_count=29}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>card</th>\n      <th>count</th>\n      <th>pct</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>191</th>\n      <td>Simic Signet</td>\n      <td>249</td>\n      <td>11.7</td>\n    </tr>\n    <tr>\n      <th>190</th>\n      <td>Prairie Stream</td>\n      <td>249</td>\n      <td>11.7</td>\n    </tr>\n    <tr>\n      <th>192</th>\n      <td>Yavimaya Coast</td>\n      <td>248</td>\n      <td>11.6</td>\n    </tr>\n    <tr>\n      <th>193</th>\n      <td>Champion of Lambholt</td>\n      <td>248</td>\n      <td>11.6</td>\n    </tr>\n    <tr>\n      <th>194</th>\n      <td>Brokers Confluence</td>\n      <td>247</td>\n      <td>11.6</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nLet's say we focus on the card *Broker's Confluence* which is a fairly recent card. How can we use the association rules to find where this card is being added by players?\n\n![](https://cards.scryfall.io/normal/front/6/5/657ff5fc-1a95-46f9-85f7-fc1ad757c8c4.jpg?1673482329){fig-align=\"center\" width=\"40%\"}\n\nWe can scan the association rules, by checking the ones where the consequent is *Brokers Confluence*, and find the ones where the confidence or lift are sufficiently high. (we use a lift of 1.5 as our filter)\n\n::: {#276786ce .cell execution_count=40}\n``` {.python .cell-code}\nrules[(rules['consequents'] == {'Brokers Confluence'}) & (rules['lift'] > 1.5)][[\"antecedents\",\"confidence\",\"lift\"]].head(10)\n```\n\n::: {.cell-output .cell-output-display execution_count=30}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>antecedents</th>\n      <th>confidence</th>\n      <th>lift</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>257432</th>\n      <td>(Grateful Apparition)</td>\n      <td>0.218147</td>\n      <td>1.881184</td>\n    </tr>\n    <tr>\n      <th>257437</th>\n      <td>(Contentious Plan)</td>\n      <td>0.226069</td>\n      <td>1.949504</td>\n    </tr>\n    <tr>\n      <th>257444</th>\n      <td>(Evolution Sage, Thrummingbird)</td>\n      <td>0.189223</td>\n      <td>1.631762</td>\n    </tr>\n    <tr>\n      <th>257468</th>\n      <td>(Inexorable Tide, Thrummingbird)</td>\n      <td>0.188006</td>\n      <td>1.621270</td>\n    </tr>\n    <tr>\n      <th>257480</th>\n      <td>(Flux Channeler, Evolution Sage)</td>\n      <td>0.176093</td>\n      <td>1.518531</td>\n    </tr>\n    <tr>\n      <th>257486</th>\n      <td>(Flux Channeler, Thrummingbird)</td>\n      <td>0.212727</td>\n      <td>1.834450</td>\n    </tr>\n    <tr>\n      <th>257492</th>\n      <td>(Tezzeret's Gambit, Flux Channeler)</td>\n      <td>0.177419</td>\n      <td>1.529973</td>\n    </tr>\n    <tr>\n      <th>257527</th>\n      <td>(Tekuthal, Inquiry Dominus, Evolution Sage)</td>\n      <td>0.178730</td>\n      <td>1.541272</td>\n    </tr>\n    <tr>\n      <th>257533</th>\n      <td>(Evolution Sage, Ezuri, Stalker of Spheres)</td>\n      <td>0.193811</td>\n      <td>1.671326</td>\n    </tr>\n    <tr>\n      <th>257545</th>\n      <td>(Everflowing Chalice, Evolution Sage)</td>\n      <td>0.175159</td>\n      <td>1.510482</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nIt looks like Brokers Ascendancy is being added to decks 50% more when there are cards like the ones above which are themed around the *proliferate* ability.\n\n# This Exercise's End Step\n\nThis is just a short demonstration so one will find that some of the steps could have definitely been streamlined and more data could have been added to make the filtering and analysis easier or more meaningful.\n\nWe have gone through a possible application of frequent pattern or association rules mining outside market basket analysis. There might be other novel ways to apply the same technique, but a good understanding of the underlying concept is necessary to understand where and how these techniques can potentially be used.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}