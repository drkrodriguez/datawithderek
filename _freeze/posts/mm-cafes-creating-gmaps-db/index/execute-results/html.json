{
  "hash": "bb2f1154a755a48b9da07e2f0429abce",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Creating a database of Cafes in Manila using Google Maps\"\nauthor: \"Derek Rodriguez\"\ndate: \"2025-12-10\"\nimage: \"mm_cafes_1.png\"\ncategories: [Python, APIs, Google Maps, Metro Manila]\nexecute: \n  eval: true\n  echo: true\n  message: false\n  freeze: true\n---\n\n\nIn this post, I start building a database of cafes in Metro Manila (Philippines) primarily using Python and [Google's Places API](https://developers.google.com/maps/documentation/places/web-service/overview). The intent is to use such a database for performing market analysis in the region, which should be useful for existing and new cafe businesses. The resulting workflow and code should also be applicable to other industries or locations, as long as they can be targeted by changin the parameters used in the code.\n\n# Summary\n\nWe are able to build an initial database of 6150 cafes in Metro Manila using a straightforward workflow given in the diagram below:\n\n![](mm_cafe_db_v1flow.png){fig-align=\"center\"}\n\nAside from extracting a list of cafes using Google's Places API, we also add features like additional location information and tag cafes under identified chains. Further data processing and analysis will be done, which will be shared in future posts.\n\n**Note:** This post will only include relevant snippets of code for readability.\n\n# Objectives\n\nWe aim to build a database of cafes in Metro Manila that can be used for market analysis for players in the industry. The resulting database and/or the code should observe the following:\n\n-   The database should give a comprehensive list of cafes in the region using Google Maps data\n\n-   The database should give relevant information on the listed cafes that will allow for further analyses\n\n-   The database should be set up to allow for future updates and monitoring of changes over time\n\n-   Where possible, the code should be efficient and reduce the costs incurred, specifically in the number of calls it makes using the API\n\n# The Places API\n\nOur main source to generate a list of cafes is Google Maps, using data that we will be accessing using [Google's Places API](https://developers.google.com/maps/documentation/places/web-service/overview).\n\nThe Places API lets us access location information in Google Maps using http requests. The request will access a specific service depending on the endpoint / url. For this post, we will be using the Nearby Search service which is uses the following endpoint: `https://places.googleapis.com/v1/places:searchNearby`\n\nIt is worth noting that the API is not free and is tied to a Google Cloud account. For the specific calls made in this post, there is a cost for exceeding 1,000 API calls: **\\$35 per 1,000 calls**. (up to 100K calls) It is therefore imperative that the API key is kept secure and that the code does not make any unnecessary calls, as this cost does add up fast.\n\n# Loading Packages\n\nThe current code currently makes use of the following eight packages:\n\n1.  [requests](https://pypi.org/project/requests/) - This library is the standard for making simple http requests\n\n2.  [csv](https://docs.python.org/3/library/csv.html) - This is for reading and writing CSV files which will be the format used to store our results\n\n3.  [math](https://docs.python.org/3/library/math.html) - This library gives access to common math functions and constants We will mainly use this to perform calculations to split Metro Manila into grid squares for the grid-based search.\n\n4.  [panda](https://pypi.org/project/pandas/) - For handling dataframe data structures in Python. We use this mainly to analyze the data after.\n\n5.  [geopandas](https://pypi.org/project/geopandas/) - This library lets us work with geospatial data in Python\n\n6.  [shapely](https://shapely.readthedocs.io/en/latest/) - This library is used for the manipulation and analysis of planar objects\n\n7.  [shutil](https://docs.python.org/3/library/shutil.html) - This library provides functions for operating on files. We use this mainly to produce backup copies of files that we have just updated\n\n8.  [os](https://docs.python.org/3/library/os.html) - Provides OS functionality. We will use this mainly to identify the filename and generate a backup filename to use in conjunction with the previous package.\n\n::: {#3da94331 .cell execution_count=1}\n``` {.python .cell-code}\nimport requests\nimport time\nimport csv\nimport math\nfrom datetime import datetime\nimport pandas as pd\nimport geopandas as gpd\nfrom shapely.geometry import Point\nimport shutil\nimport os\n```\n:::\n\n\nNote that these packages are specific to the functions required to run the workflow. Functions purely for analysis and visualization (e.g., `matplotlib`) have not been included in this section.\n\n# Step 1: Generating Initial Gridpoints\n\nA [Nearby Search](https://developers.google.com/maps/documentation/places/web-service/nearby-search) using the Places API lets the user perform a search by specifying an area and then specifying the type(s) of places to return. For defining the search area, we will be using coordinates and then defining a search radius to use. At the time of writing, it looks like each search will only return a maximum of 20 results, so we need to try to define search areas that are small enough to ensure that a single query returns all cafes in the area.\n\nTo do this, we can define the total area we want to perform the search on, Metro Manila, and then find a bounding box for that area. The corners of the area are what we need. A web search gave us the following coordinates:\n\n::: {#17723263 .cell execution_count=2}\n``` {.python .cell-code}\n# Upper left and lower right corners of bounding box\nNW_LAT, NW_LNG = 14.7100, 120.9500\nSE_LAT, SE_LNG = 14.4000, 121.1000\n```\n:::\n\n\nWith the bounding box defined, we then need to split it into smaller areas to perform the search on. For now, we will use a distance of 1km vertically or horizontally between the center of adjacent search areas. This will split the total area into grid points or grid squares. We also need to define a radius for the search. The radius of the search needs to cover the diagonal of each grid square, and not only the radius. The diagonal measures $\\sqrt{2}/2$ , which we can round up to 710m.\n\n::: {#c28b953f .cell execution_count=3}\n``` {.python .cell-code}\nGRID_STEP_KM = 1  # Grid square side\nSEARCH_RADIUS_M = GRID_STEP_KM * 710  # Search radius in meters\n```\n:::\n\n\nThe function to generate the initial grid points then needs to move from one corner of the bounding box, in this case, the top left corner, to the opposite corner. The code below moves from left to right before moving to the next \"row\". The function uses an estimate of 111 km for every degree latitude\n\n::: {#a77d533d .cell execution_count=4}\n``` {.python .cell-code}\ndef generate_grid_points(nw_lat, nw_lng, se_lat, se_lng, step_km):\n    points = []\n    lat_step = step_km / 111.0  # approx degrees per km\n    lng_step = step_km / (111.0 * math.cos(math.radians(nw_lat)))\n    lat = nw_lat\n    while lat > se_lat:\n        lng = nw_lng\n        while lng < se_lng:\n            points.append((lat, lng))\n            lng += lng_step\n        lat -= lat_step\n    return points\n```\n:::\n\n\nRunning the function above with the bounding box coordinates, and a 1km spacing, gives us the following grid points.\n\n![](grid_points_1km_no_filter.png){fig-align=\"center\"}\n\nThe function generated 595 grid points. While this appears okay, as we are still within our free cap, we expect to make multiple runs in a month as we test, or add on layers. Two runs will already incur us costs. We also see that a large number of these points fall on water, which means the API call is wasted in such cases. We need to remove these points to make sure we reduce the number of calls that do not generate any results.\n\n# Step 2: Filtering Grid Points (Reduce points falling on water)\n\nOne way to address the issue just mentioned is by filtering the points based on a reference geospatial dataset. For our case, we can choose to filter the points to only the ones that fall within the administrative (land) region of Metro Manila. This data is accessible online using sources like [Human Data Exchange](https://data.humdata.org/dataset/caf116df-f984-4deb-85ca-41b349d3f313). We would need a layer from ADM1 or higher to make sure that we can filter at the regional level.\n\nThe filtering will be using the functions from the [geopandas](https://pypi.org/project/geopandas/) package in order to work with geospatial data and performing geospatial operations. The following code block gives the initial version of the filtering function by performing the following steps:\n\n1.  Loads the shapefile into a geopandas dataframe and then filters for only the Metro Manila region. The resulting dataframe is called `metro_manila`\n2.  Converts the initial list of gridpoints `gp` from the current coordinates into a geo dataframe using the same reference system as the shapefile. This transforms `gp` into `points_gdf`\n3.  Combines metro_manila into a single shape using `union_all`, and then uses `within` to check which points in `points_gdf` fall within `union_all`. These points are the ones returned by the function.\n\n::: {#4e701b86 .cell execution_count=5}\n``` {.python .cell-code}\ndef mm_filter(gp):\n    # Loading the shapefile\n    shapefile_path = \"ADM_1_shapefile_path\"\n    gdf = gpd.read_file(shapefile_path)\n\n    # Filter for Metro Manila (ADM1_PCODE = \"PH13\")\n    metro_manila = gdf[gdf[\"ADM1_PCODE\"] == \"PH13\"]\n\n    # ---------------- CONVERT GRID POINTS TO GeoDataFrame ----------------\n    points_gdf = gpd.GeoDataFrame(\n        pd.DataFrame(gp, columns=[\"lat\", \"lng\"]),\n        geometry=[Point(lng, lat) for lat, lng in gp],\n        crs=\"EPSG:4326\"\n    )\n\n    metro_union = metro_manila.geometry.union_all()\n    filtered_points = points_gdf[points_gdf.within(metro_union)]\n    filtered_list = [(point.y, point.x) for point in filtered_points.geometry]\n    return filtered_list\n```\n:::\n\n\nRunning the initial points in this function generates the points below.\n\n![](grid_points_1km_filtered.png){fig-align=\"center\"}\n\nWe see that there are now no points falling on the large bodies of water. The code actually reduced the number of points from 595 to 440, a 26% reduction.\n\nIf we look closely at this new map, we see some issues though. There are areas close to water which appear to be missing a grid point. A few red boxes have been placed to highlight such places. It looks like the grid points that should be covering some part of these areas fall right on the water and were therefore excluded.\n\nA slight modification to the function is therefore introduced to make sure we still allow gridpoints that fall close to the border. We then modify the original function with the following to achieve this:\n\n1.  Convert the Metro Manila data frame into a meters-based system using `to_crs(epsg=3857)` (make sure to check EPSG codes if using these functions for other countries)\n2.  Introduce a 1km buffer around the combined Metro Manila area using `buffer(1000)`\n3.  Reproject the area back to EPSG 4326, then follow the same steps for checking the input list `gp` and returning the filtered points.\n\n::: {#9af63786 .cell execution_count=6}\n``` {.python .cell-code}\ndef mm_filter_v2(gp):\n    shapefile_path = \"ADM_1_shapefile_path\"\n    gdf = gpd.read_file(shapefile_path)\n    metro_manila = gdf[gdf[\"ADM1_PCODE\"] == \"PH13\"]\n    points_gdf = gpd.GeoDataFrame(\n        pd.DataFrame(gp, columns=[\"lat\", \"lng\"]),\n        geometry=[Point(lng, lat) for lat, lng in gp],\n        crs=\"EPSG:4326\"\n    )\n    \n    # Introduce a buffer of 1km around Metro Manila\n    # Reproject to a CRS in meters\n    metro_manila_proj = metro_manila.to_crs(epsg=3857)\n    metro_union_buffered = metro_manila_proj.geometry.union_all().buffer(1000)\n\n    # Reproject back to WGS84 for comparison\n    metro_union = gpd.GeoSeries([metro_union_buffered], crs=\"EPSG:3857\").to_crs(epsg=4326).iloc[0]\n\n    filtered_points = points_gdf[points_gdf.within(metro_union)]\n    filtered_list = [(point.y, point.x) for point in filtered_points.geometry]\n\n    return filtered_list\n```\n:::\n\n\nRunning the function increases the number of points filtered from 440 to 504, but ensures that we still search all areas that lie close to the water.\n\n# Step 3: Performing the Nearby Search\n\nThe Nearby Search is done by making a call using the [requests](https://pypi.org/project/requests/) package. The request parameters will include the endpoint, and the details for the request which will be passed on in JSON format as the `headers` and `json` parameters below.\n\n::: {#7eb97e6b .cell execution_count=7}\n``` {.python .cell-code}\n# Incomplete function, for illustration only\ndef fetch_places(center_lat, center_lng, radius, included_types):\n    # Endpoint for Nearby Search\n    BASE_URL = \"https://places.googleapis.com/v1/places:searchNearby\"\n    \n    headers = \"define headers value here\"\n    body = \"define json value here\"\n\n    # API call made via:\n    response = requests.post(BASE_URL, headers=headers, json=body)\n\n    if response.status_code != 200:\n        print(f\"Error: {response.status_code}, {response.text}\")\n        return []\n\n    data = response.json()\n    return data.get(\"places\", [])\n```\n:::\n\n\nThe `headers` parameter is where we will include our API key, and the list of fields to return, or the field mask. While the field mask can accept the wildcard `\"*\"`, it is not advisable since the Nearby Search covers multiple Nearby Search SKUs based on the \"most expensive\" field requested. The wildcard will trigger pricing for the most expensive SKU (Nearby Search Enterprise + Atmosphere), which we currently do not need.\n\n::: {#6486817e .cell execution_count=8}\n``` {.python .cell-code}\n# Enumerate fields to request\nfields_to_include = [\n         \"places.id\",\n         \"places.displayName\",\n         \"places.formattedAddress\",\n         \"places.location\",\n         \"places.types\",\n         \"places.businessStatus\",\n         \"places.rating\",\n         \"places.userRatingCount\",\n         \"places.websiteUri\",\n         \"places.nationalPhoneNumber\"]\nfields = \",\".join(fields_to_include)  \n\nheaders = {\n      \"Content-Type\": \"application/json\",\n      \"X-Goog-Api-Key\": \"API_KEY_here\",\n      \"X-Goog-FieldMask\": fields\n  }\n```\n:::\n\n\nThe json parameter, which we will pass on the value of body, is where we define the specific Nearby Search parameters. This includes the search area, the type of places to include, and the number of results to return. (up to a maximum of 20) The `rankPreference` can also be defined, and we use \"DISTANCE\" to make sure that results are sorted and picked up based on the proximity to the center. The default mode is \"POPULARITY\" which might mean that big chains that fall on the overlap of search areas have a high chance of being repeated, and displace some of the less popular cafes.\n\n::: {#3f796c8b .cell execution_count=9}\n``` {.python .cell-code}\nincluded_types = [\"cafe\", \"coffee_shop\"]\nbody = {\n        \"locationRestriction\": {\n            \"circle\": {\n                \"center\": {\"latitude\": \"replace_with_gp_lat\",\n                          \"longitude\": \"replace_with_gp_lng\"},\n                \"radius\": SEARCH_RADIUS_M\n            }\n        },\n        \"rankPreference\": \"DISTANCE\",\n        \"includedTypes\": included_types,\n        \"maxResultCount\": 20,}\n```\n:::\n\n\nWith all the Nearby Search parameters defined, we can then run the resulting function using each of the gridpoints generated in the previous step as centers for the search areas.\n\nThe initial run of our script so far produced a dataset of **6150 cafes** in Metro Manila which is stored in a csv file.\n\n![](pbi_initial_output_total_mm.png){fig-align=\"center\"}\n\n# Step 4: Adding barangay and city information\n\nWe use the pandas package to load the data into a dataframe in order to examine the current output.\n\n::: {#73fe1122 .cell execution_count=10}\n``` {.python .cell-code}\nimport pandas as pd\ndf = pd.read_csv('.\\data\\metro_manila_cafes.csv')\ndf.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>place_id</th>\n      <th>name</th>\n      <th>address</th>\n      <th>lat</th>\n      <th>lng</th>\n      <th>types</th>\n      <th>business_status</th>\n      <th>rating</th>\n      <th>user_ratings_total</th>\n      <th>website</th>\n      <th>phone_number</th>\n      <th>data_extracted</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ChIJoaqhLu-zlzMRCw_dDwh7HWo</td>\n      <td>Lazy Latte Cafe</td>\n      <td>62, HP Building, G. Lazaro Rd, Valenzuela, 144...</td>\n      <td>14.707911</td>\n      <td>120.955799</td>\n      <td>coffee_shop,food_store,cafe,food,point_of_inte...</td>\n      <td>OPERATIONAL</td>\n      <td>5.0</td>\n      <td>8.0</td>\n      <td>https://www.facebook.com/profile.php?id=615654...</td>\n      <td>0964 968 4439</td>\n      <td>2025-12-02</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ChIJGxMZJACzlzMRvayy0yTM6tM</td>\n      <td>NG SNACK CAFE</td>\n      <td>Pascual Deato, Manila, Metro Manila, Philippines</td>\n      <td>14.713137</td>\n      <td>120.950482</td>\n      <td>coffee_shop,food_store,cafe,food,point_of_inte...</td>\n      <td>OPERATIONAL</td>\n      <td>5.0</td>\n      <td>2.0</td>\n      <td>NaN</td>\n      <td>0920 529 2943</td>\n      <td>2025-12-02</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ChIJ-awZBgCzlzMR4wgLf3nVNsQ</td>\n      <td>But First, Coffee (BFC) - Polo Valenzuela</td>\n      <td>30 Poblacion, 1 Marcelo H. Del Pilar St, Valen...</td>\n      <td>14.708518</td>\n      <td>120.946553</td>\n      <td>coffee_shop,food_store,cafe,food,point_of_inte...</td>\n      <td>OPERATIONAL</td>\n      <td>5.0</td>\n      <td>3.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2025-12-02</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ChIJ33aDSACzlzMRcrhmFyhrFiQ</td>\n      <td>R2RO’s Café</td>\n      <td>289 Marcelo H. Del Pilar St, Valenzuela, 1444 ...</td>\n      <td>14.715195</td>\n      <td>120.952188</td>\n      <td>coffee_shop,food_store,cafe,food,point_of_inte...</td>\n      <td>OPERATIONAL</td>\n      <td>5.0</td>\n      <td>2.0</td>\n      <td>https://www.facebook.com/share/1CyJcChxx4/?mib...</td>\n      <td>NaN</td>\n      <td>2025-12-02</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ChIJC5UvVgGzlzMRh7_27ZBa0hs</td>\n      <td>Kubo ni lola</td>\n      <td>321 Pasolo Rd, Brgy. Pasolo, Valenzuela, 1444 ...</td>\n      <td>14.708686</td>\n      <td>120.952378</td>\n      <td>coffee_shop,food_store,cafe,food,point_of_inte...</td>\n      <td>OPERATIONAL</td>\n      <td>5.0</td>\n      <td>3.0</td>\n      <td>NaN</td>\n      <td>0968 507 1296</td>\n      <td>2025-12-02</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#ef3689ca .cell execution_count=11}\n``` {.python .cell-code}\ndf.columns\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```\nIndex(['place_id', 'name', 'address', 'lat', 'lng', 'types', 'business_status',\n       'rating', 'user_ratings_total', 'website', 'phone_number',\n       'data_extracted'],\n      dtype='object')\n```\n:::\n:::\n\n\nThe output contains 12 columns that describe each cafe returned by the Nearby search. We want to examine the location information, since a large part of the analysis we will be performing will be location-based as we analyze cafes in a given area. It looks like that aside from the coordinates, (`lat`, `lng`) we have the location information in the address column. We can check out the first few elements using the code chunk below.\n\n::: {#00a969ba .cell execution_count=12}\n``` {.python .cell-code}\nfor i in range(5): print(df['address'][i])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n62, HP Building, G. Lazaro Rd, Valenzuela, 1443 Metro Manila, Philippines\nPascual Deato, Manila, Metro Manila, Philippines\n30 Poblacion, 1 Marcelo H. Del Pilar St, Valenzuela, 1444 Metro Manila, Philippines\n289 Marcelo H. Del Pilar St, Valenzuela, 1444 Metro Manila, Philippines\n321 Pasolo Rd, Brgy. Pasolo, Valenzuela, 1444 Metro Manila, Philippines\n```\n:::\n:::\n\n\nWith just the first five elements, while it looks like we can derive the city for each of them, we see that the cannot identify which barangay each cafe belongs to. The fifth and maybe the second item have the barangay before the city name, but the rest clearly do not. They only show the street name before the city.\n\nAn approach we can take to tag barangay and city information will be through the use of shapefiles, similar to what we did in Step 2. Instead of filtering, we would be using geopandas to find where each cafe is.\n\nWe perform this while creating a \"master file\". Creating a masterfile, is done since we only need to have one record for each cafe. The file generated previously includes a \"data_extracted\" column to track any changes in the cafe details overtime– especially cafe closures, additions, ratings.\n\nThe function below performs the following:\n\n1.  Loads the file with the cafe dataset and, if it exists, the existing masterfile\n2.  Finds cafes that are not in the masterfile that need to be tagged and added to the masterfile\n3.  Converts the cafes into a geopandas dataframe `cafes_gdf` and loads the ADM4 shapefile into another geodataframe `admin_gdf`\n4.  Merges `cafes_gdf` and `admin_gdf` using a spatial join `sjoin`.\n5.  Writes the relevant columns into the masterfile. Additional columns for future steps: `Group`, and `Include` are also added to the file with default values\n\n::: {#2079e8f0 .cell execution_count=13}\n``` {.python .cell-code}\ndef create_master(input_file='.\\data\\metro_manila_cafes.csv', master_file='.\\data\\metro_manila_cafes_master.csv'):\n    # Load ADM4 Shapefile\n    shapefile_path = \"shapefile_path_up_to_ADM4\"\n\n    # Load inputs data\n    cafes_df = pd.read_csv(input_file)\n    \n    # If master file exists, load it and filter new cafes\n    if os.path.exists(master_file):\n        master_df = pd.read_csv(master_file)\n        existing_ids = set(master_df[\"place_id\"])\n        cafes_df = cafes_df[~cafes_df[\"place_id\"].isin(existing_ids)]\n        print(f\"Master file exists. Found {len(cafes_df)} new cafes to process.\")\n    else:\n        master_df = pd.DataFrame(columns=[\n            \"place_id\", \"name\", \"address\", \"lat\", \"lng\", \"data_extracted\",\n            \"ADM2_EN\", \"ADM3_EN\", \"ADM4_EN\", \"Group\", \"Include\"\n        ])\n        print(f\"No master file found. Processing all {len(cafes_df)} cafes.\")\n    \n    # If no new cafes, exit early\n    if cafes_df.empty:\n        print(\"No new cafes to process. Exiting.\")\n    else:\n        # ---------------- CONVERT TO GeoDataFrame ----------------\n        cafes_gdf = gpd.GeoDataFrame(\n            cafes_df,\n            geometry=[Point(xy) for xy in zip(cafes_df.lng, cafes_df.lat)],\n            crs=\"EPSG:4326\"\n        )\n    \n        # ---------------- LOAD SHAPEFILE ----------------\n        admin_gdf = gpd.read_file(shapefile_path)\n    \n        enriched_gdf = gpd.sjoin(cafes_gdf, admin_gdf, how=\"left\", predicate=\"within\")\n    \n        # Retain required columns\n        enriched_df = enriched_gdf[[\n            \"place_id\", \"name\", \"address\", \"lat\", \"lng\", \"data_extracted\", \"ADM2_EN\", \"ADM3_EN\", \"ADM4_EN\"\n        ]].copy()\n    \n        # Add default columns\n        enriched_df[\"Group\"] = \"New Record\"\n        enriched_df[\"Include\"] = \"Yes\"\n    \n        # Append to master\n        master_df = pd.concat([master_df, enriched_df], ignore_index=True)\n    \n        # Save updated master file\n        master_df.to_csv(master_file, index=False, encoding=\"utf-8\")\n        print(f\"Updated master file saved with {len(master_df)} total cafes.\")\n        print(\"Preview of last 5 rows added:\")\n        print(enriched_df.tail())\n```\n:::\n\n\nOnce the code runs, we can use the following code to run a quick query on the top 5 cities and barnagays with the most cafes.\n\n::: {#d8f1c1c6 .cell execution_count=14}\n``` {.python .cell-code}\ndf = pd.read_csv('.\\data\\metro_manila_cafes_master.csv')\nprint(\"Top 5 Cities in Metro Manila with the most cafes\")\ndf[\"ADM3_EN\"].value_counts().head()\nprint(\"\\nTop 5 Barangays in Metro Manila with the most cafes\")\ndf[[\"ADM4_EN\",\"ADM3_EN\"]].value_counts().head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTop 5 Cities in Metro Manila with the most cafes\n\nTop 5 Barangays in Metro Manila with the most cafes\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=14}\n```\nADM4_EN        ADM3_EN          \nB. F. Homes    City of Parañaque    95\nTalon Dos      City of Las Piñas    75\nDon Bosco      City of Parañaque    74\nBatasan Hills  Quezon City          71\nPasong Tamo    Quezon City          65\nName: count, dtype: int64\n```\n:::\n:::\n\n\n# Step 5: Tagging Cafe chains based on names\n\nThe last data processing step we will cover in this post is the addition of tags to identify cafes that belong to the same name or chain. We created a column `Group` in the masterfile to capture this information, but assigned a default value of \"TBC\" for now.\n\nWe will do initial tagging based on an inspection of the cafe names. This is by checking the presence of specific substrings in the name, and then using a dictionary of group and substrings to mark cafes. While this can be done manually, we can use some code to help us identify candidate substrings and cafe chains.\n\nThe idea will be to go through the cafe names and check which sequences of words appear commonly in the field– this should point to potential cafe chains. We limit the length of the sequence to 2-4 words. Single words might not be very useful since we expect a lot of noise with words like coffee, cafe, tea, etc., as well as articles to pop up in the results. We also limit to 4 words since we don't expect brand or chain names to be any longer, most of them should be evident with even less.\n\nThe code below does this by tokenizing the words and creating 2-, 3- and 4-grams, The [Counter class](https://docs.python.org/3/library/collections.html#collections.Counter) from the collections package is used to count each n-gram as the code encounters them as it runs through the list of names. The code also includes displaying the top 20 n-grams.\n\n::: {#4a276bf8 .cell execution_count=15}\n``` {.python .cell-code}\nimport pandas as pd\nfrom collections import Counter\nimport re\n\n# df already is loaded with the masterfile\nnames = df['name'].dropna().tolist()\n\ndef get_ngrams(text, n):\n    words = re.findall(r'\\w+', text.lower())  # tokenize words\n    return zip(*[words[i:] for i in range(n)])  # create n-grams\n\n# Collect all n-grams\nngrams_counter = Counter()\n\nfor name in names:\n    for n in [2, 3, 4]:\n        ngrams_counter.update([' '.join(ngram) for ngram in get_ngrams(name, n)])\n\n# Convert to DataFrame\nngrams_df = pd.DataFrame(ngrams_counter.items(), columns=['ngram', 'count']).sort_values(by='count', ascending=False)\n\nngrams_df.head(20)\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ngram</th>\n      <th>count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1973</th>\n      <td>pickup coffee</td>\n      <td>110</td>\n    </tr>\n    <tr>\n      <th>556</th>\n      <td>s cafe</td>\n      <td>88</td>\n    </tr>\n    <tr>\n      <th>212</th>\n      <td>milk tea</td>\n      <td>80</td>\n    </tr>\n    <tr>\n      <th>590</th>\n      <td>s coffee</td>\n      <td>65</td>\n    </tr>\n    <tr>\n      <th>1379</th>\n      <td>zus coffee</td>\n      <td>63</td>\n    </tr>\n    <tr>\n      <th>631</th>\n      <td>the coffee</td>\n      <td>61</td>\n    </tr>\n    <tr>\n      <th>591</th>\n      <td>coffee shop</td>\n      <td>58</td>\n    </tr>\n    <tr>\n      <th>5330</th>\n      <td>coffee bean</td>\n      <td>46</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>coffee bfc</td>\n      <td>46</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>but first coffee bfc</td>\n      <td>46</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>first coffee</td>\n      <td>46</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>but first</td>\n      <td>46</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>but first coffee</td>\n      <td>46</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>first coffee bfc</td>\n      <td>46</td>\n    </tr>\n    <tr>\n      <th>5332</th>\n      <td>tea leaf</td>\n      <td>44</td>\n    </tr>\n    <tr>\n      <th>19791</th>\n      <td>las piñas</td>\n      <td>43</td>\n    </tr>\n    <tr>\n      <th>529</th>\n      <td>quezon city</td>\n      <td>42</td>\n    </tr>\n    <tr>\n      <th>300</th>\n      <td>coffee tea</td>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>5336</th>\n      <td>the coffee bean</td>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>86</th>\n      <td>kkopi tea</td>\n      <td>39</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nBased on the top 20 n-grams, we are able to spot a few chain names like: **Pickup Coffee**, **Zus Coffee**, **But First, Coffee**, **Coffee Bean and Tea Leaf**, **Kkopi Tea**. The longer list should be examined to find other recurring n-grams. It is also worth noting that this will not surface any chain names with only one words– like **Starbucks**. While the user can rerun the code for only single words, if they want to have the code help them identify these.\n\nThe identified brands/chains and their keywords can be defined in a function like the one below which can be used to check a cafe name and return the chain name if there is a match in the dictionary, or return \"TBC\" otherwise.\n\n::: {#3c2bb3d2 .cell execution_count=16}\n``` {.python .cell-code}\n# Complete dictionary not included\ndef assign_group(name):\n    brand_dict = {\n        \"B1T1 Takeaway Coffee\": [\"b1t1\"],\n        \"Big Brew\": [\"big brew\", \"bigbrew\"],\n        \"Black Scoop Cafe\": [\"black scoop\", \"blackscoop\"],\n        \"But First, Coffee\": [\"but first, coffee\"],\n        \"CBTL\": [\"coffee bean & tea leaf\", \"cbtl\"],\n        \"Dunkin\": [\"dunkin\"],\n        \"Highlands Coffee\": [\"highlands\"],\n        \"Kkopi.Tea\": [\"kkopi.tea\"],\n        \"Starbucks\": [\"starbucks\"],\n        \"Zus Coffee\": [\"zus coffee\"],\n        \"% Arabica\": [\"% arabica\", \"arabica manila\"]\n    }\n    name_lower = name.lower()\n    for brand, keywords in brand_dict.items():\n        if any(keyword in name_lower for keyword in keywords):\n            return brand\n    return \"TBC\"\n```\n:::\n\n\nWe run the current master in the above function, and get the below summary visuals.\n\n::: {#7e5ebd9c .cell execution_count=17}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n\ndf_grouped = pd.read_csv(\".\\data\\metro_manila_cafes_master_grouped.csv\")\n\nprint(\"There are \",sum(df_grouped[\"Group\"]==\"TBC\"),\" cafes marked as TBC\\n\")\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThere are  4912  cafes marked as TBC\n\n```\n:::\n:::\n\n\n::: {#ac88347b .cell execution_count=18}\n``` {.python .cell-code}\n# Get value counts\ngroup_counts = df_grouped[df_grouped[\"Group\"] != \"TBC\"][\"Group\"].value_counts().head(15)\n\n# Plot bar chart\nplt.figure(figsize=(10, 5))\nbars = group_counts.plot(kind='bar', color='skyblue', edgecolor='black')\n\nplt.title(\"Top 15 Tagged Groups with most Cafes (Excluding TBC)\")\nplt.xlabel(\"Group\")\nplt.ylabel(\"Number of Cafes\")\nplt.xticks(rotation=45, ha='right', fontsize=6)\n\n# Add data labels on top of each bar\nfor i, value in enumerate(group_counts):\n    plt.text(i, value + 1, str(value), ha='center', va='bottom', fontsize=6)\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-19-output-1.png){width=950 height=469}\n:::\n:::\n\n\n::: {#38ebaf13 .cell execution_count=19}\n``` {.python .cell-code}\n# Get value counts\ngroup_counts = df_grouped[df_grouped[\"Group\"] != \"TBC\"][\"Group\"].value_counts().tail(15)\n\n# Plot bar chart\nplt.figure(figsize=(10, 5))\nbars = group_counts.plot(kind='bar', color='skyblue', edgecolor='black')\n\nplt.title(\"Bottom 15 Tagged Groups with most Cafes (Excluding TBC)\")\nplt.xlabel(\"Group\")\nplt.ylabel(\"Number of Cafes\")\nplt.xticks(rotation=45, ha='right', fontsize=6)\n\n# Add data labels on top of each bar\nfor i, value in enumerate(group_counts):\n    plt.text(i, value + 1, str(value), ha='center', va='bottom', fontsize=6)\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-20-output-1.png){width=950 height=463}\n:::\n:::\n\n\nA few observations can be made from this initial output:\n\n-   Some cafes, unsurprisingly, appear with high numbers. This includes **Starbucks**, **Pickup Coffee**, **BFC** and **CBTL**. One cafe stands out: **Big Brew**. I am not familiar with this chain, but it looks like there are a lot of branches in the region. It is worth checking this out as they appear to have a very big presence in the overall cafe business.\n\n-   **7-Eleven** is being picked up since they serve coffee, but they only appear twice in the data. This points out inconsistencies in how places are tagged. We need to understand if this is a cafe player that we need to include or exclude, and then modify our workflow accordingly. (i.e., by finding a way to capture 7-11 in the next extract, or filter them out)\n\nAdditional work needs to be done on the tagging, but we are off to a good start since we have captured a large number of the big chains already.\n\n# Next Steps\n\nWe end the post here, but there are still a number of steps needed to build a satisfactory database for us to work with for performing market analysis. Some of these steps include:\n\n1.  Additional data cleaning and data enrichment for the current dataset\n\n2.  Analysis of the cleaned up cafa dataset\n\n3.  Addition of complementary layer(s), like ones to represent demand centers\n\n4.  Incorporate additional data sources with new dimensions\n\n5.  Analysis of the combined data set\n\n6.  Scheduled updates and/or cloud deployment\n\nI will try to cover most of the steps in separate future posts, so stay tuned. I look forward to getting this database in a decent state by the middle of next year, so I hope you don't have to wait too long.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}