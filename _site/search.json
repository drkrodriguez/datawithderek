[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi visitor. I’m Derek, data enthusiast and dog dad to a naughty corgi. I started this blog in November 2024 as I reach the end of my data analytics masters journey. I will use this to share my personal projects, including any interesting tricks or datasets that I encounter."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "data with derek",
    "section": "",
    "text": "My personal projects and discoveries in data analytics.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiscovering Impact of COVID-19 on Thai Tourism Economy\n\n\n\n\n\n\nR\n\n\nGeospatial Analysis\n\n\nThailand\n\n\n\n\n\n\n\n\n\nNov 13, 2024\n\n\nDerek Rodriguez\n\n\n\n\n\n\n\n\n\n\n\n\nGeographically Weighted Modeling of Financial Inclusion in Tanzania\n\n\n\n\n\n\nR\n\n\nGeospatial Analysis\n\n\nTanzania\n\n\n\n\n\n\n\n\n\nNov 13, 2024\n\n\nDerek Rodriguez\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/thai-tourism-covid/index.html",
    "href": "posts/thai-tourism-covid/index.html",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "",
    "text": "This is my second project (take home exercise) for our course Geospatial Analysis. This exercise makes use of techniques in geospatial analysis to discover clustering and hotspots in order to analyze Thai tourism data befor and after Covid."
  },
  {
    "objectID": "posts/thai-tourism-covid/index.html#a.1-background",
    "href": "posts/thai-tourism-covid/index.html#a.1-background",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "A.1 Background",
    "text": "A.1 Background\nTourism is a major industry in Thailand as it made up to 20% of their gross domestic product pre-pandemic. However, like the rest of the world, the industry has taken a hit with COVID-19 in 2020, and has slowly been recovering since 2021. Recent reports are stating that Thailand is already, but still, at 80% of its peak level in 2019.\nWhile we speak about the industry in general, the state of tourism within Thailand, and their recovery status are not the same. For example, tourism revenues have been focused on Bangkok, Phuket and Chonburi pre-pandemic.\nWe are interested in understanding the state of tourism across Thailand with regards to its spatial distribution and time and space distribution– both in absolutes and in terms of the trend with respect to the pandemic."
  },
  {
    "objectID": "posts/thai-tourism-covid/index.html#a.2-objectives",
    "href": "posts/thai-tourism-covid/index.html#a.2-objectives",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "A.2 Objectives",
    "text": "A.2 Objectives\nFor this study, we want to understand the state of tourism in Thailand at a provincial level, and answer the following questions:\n\nAre the key tourism indicators in Thailand (at a province level) independent from space and from space and time?\nIf tourism or any tourism indicators are not independent, what are the clusters, outliers and emerging hotspots and coldspots?\n\nWe will use the appropriate packages in R in order to perform the different analysis (spatial and otherwise) to support our answers to the above questions."
  },
  {
    "objectID": "posts/thai-tourism-covid/index.html#a.3-data-sources",
    "href": "posts/thai-tourism-covid/index.html#a.3-data-sources",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "A.3 Data Sources",
    "text": "A.3 Data Sources\nThe following data sources are used for this analysis:\n\nThailand Domestic Tourism Statistics from Kaggle covering the years 2019-2023 and are at province and month level across 8 indicators:\n\nno_tourist_all - total number of domestic tourists\nno_tourist_foreign - number of foreign tourists\nno_tourist_occupied - number of hotel rooms occupied\nno_tourist_thai - number of Thai tourists\noccupancy_rate - the percentage of occupied travel accommodations (hotel rooms)\nrevenue_all - total tourism revenue, in M-THB (appears as net profit in the raw data)\nrevenue_foreign - revenue generated by foreign tourists, in M-THB (appears as net profit in the raw data)\nrevenue_thai - revenue generated by Thai tourists, in M-THB (appears as net profit in the raw data)\n\nThailand-Subnational Administrative Boundaries from Human Data Exchange in shapefile format"
  },
  {
    "objectID": "posts/thai-tourism-covid/index.html#a.4-importing-and-launching-r-packages",
    "href": "posts/thai-tourism-covid/index.html#a.4-importing-and-launching-r-packages",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "A.4 Importing and Launching R Packages",
    "text": "A.4 Importing and Launching R Packages\nFor this study, the following R packages will be used. A description of the packages and the code, using p_load() of the pacman package, to import them is given below.\n\nPackage DescriptionImport Code\n\n\nThe loaded packages include:\n\nsf - package for importing, managing and processing vector-based geospatial data\ntidyverse - collection of packages for performing data importation, wrangling and visualization\ntmap - package with functions for plotting cartographic quality maps\nsfdep - for handling spatial data\ncoorplot, ggpubr, heatmaply, factoextra - packages for multivariate data visualization and analysis\ncluster, ClustGeo, NbClust - packages for performing cluster analysis\n\n\n\n\npacman::p_load(sf, tmap, spdep, sfdep, tidyverse,\n               ggpubr, heatmaply, factoextra,\n               NbClust, cluster, ClustGeo)\n\n\n\n\nAs we will be performing simulations in the analysis later, it is good practice to define a random seed to be used so that results are consistent for viewers of this report, and the results can be reproduced.\n\nset.seed(1234)"
  },
  {
    "objectID": "posts/thai-tourism-covid/index.html#b.1.-thailand-subnational-boundary-provincial-level",
    "href": "posts/thai-tourism-covid/index.html#b.1.-thailand-subnational-boundary-provincial-level",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "B.1. Thailand Subnational Boundary, Provincial Level",
    "text": "B.1. Thailand Subnational Boundary, Provincial Level\nWe load the Thailand subnational administrative boundary shapefile into an R dataframe using st_read() from the sf package. We need to analyze at the provincial level so we will be using the files suffixed by “1”.\n\nthai_sf &lt;- st_read(dsn=\"data/geospatial\", \n                   layer=\"tha_admbnda_adm1_rtsd_20220121\")\n\nReading layer `tha_admbnda_adm1_rtsd_20220121' from data source \n  `C:\\drkrodriguez\\datawithderek\\posts\\thai-tourism-covid\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 77 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97.34336 ymin: 5.613038 xmax: 105.637 ymax: 20.46507\nGeodetic CRS:  WGS 84\n\n\nThe output states that the object is of multipolygon geometry type containing 77 features (provinces, records) across 16 fields. (columns) We can check the contents of the object using a number of methods. For the code chunk below, we use glimpse() which lists the columns, gives the data type and the first elements.\n\nglimpse(thai_sf)\n\nRows: 77\nColumns: 17\n$ Shape_Leng &lt;dbl&gt; 2.417227, 1.695100, 1.251111, 1.884945, 3.041716, 1.739908,…\n$ Shape_Area &lt;dbl&gt; 0.13133873, 0.07926199, 0.05323766, 0.12698345, 0.21393797,…\n$ ADM1_EN    &lt;chr&gt; \"Bangkok\", \"Samut Prakan\", \"Nonthaburi\", \"Pathum Thani\", \"P…\n$ ADM1_TH    &lt;chr&gt; \"กรุงเทพมหานคร\", \"สมุทรปราการ\", \"นนทบุรี\", \"ปทุมธานี\", \"พระนครศรีอ…\n$ ADM1_PCODE &lt;chr&gt; \"TH10\", \"TH11\", \"TH12\", \"TH13\", \"TH14\", \"TH15\", \"TH16\", \"TH…\n$ ADM1_REF   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ ADM1ALT1EN &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ ADM1ALT2EN &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ ADM1ALT1TH &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ ADM1ALT2TH &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ ADM0_EN    &lt;chr&gt; \"Thailand\", \"Thailand\", \"Thailand\", \"Thailand\", \"Thailand\",…\n$ ADM0_TH    &lt;chr&gt; \"ประเทศไทย\", \"ประเทศไทย\", \"ประเทศไทย\", \"ประเทศไทย\", \"ประเทศ…\n$ ADM0_PCODE &lt;chr&gt; \"TH\", \"TH\", \"TH\", \"TH\", \"TH\", \"TH\", \"TH\", \"TH\", \"TH\", \"TH\",…\n$ date       &lt;date&gt; 2019-02-18, 2019-02-18, 2019-02-18, 2019-02-18, 2019-02-18…\n$ validOn    &lt;date&gt; 2022-01-22, 2022-01-22, 2022-01-22, 2022-01-22, 2022-01-22…\n$ validTo    &lt;date&gt; -001-11-30, -001-11-30, -001-11-30, -001-11-30, -001-11-30…\n$ geometry   &lt;MULTIPOLYGON [°]&gt; MULTIPOLYGON (((100.6139 13..., MULTIPOLYGON (…\n\n\nFor clarity, we can clean up this dataframe by:\n\nKeeping only relevant columns: The province name and code, geometry\nRenaming the columns: change ADM1 to Province\n\nThe following code chunk executes these steps by using select() for the first step and rename() for the second step. We again use glimpse() to give a preview of the dataset’s columns.\n\nthai_sf &lt;- thai_sf %&gt;%\n  select(ADM1_EN, ADM1_PCODE, geometry) %&gt;%\n  rename(Province = ADM1_EN, ProvCode = ADM1_PCODE)\n\nglimpse(thai_sf)\n\nRows: 77\nColumns: 3\n$ Province &lt;chr&gt; \"Bangkok\", \"Samut Prakan\", \"Nonthaburi\", \"Pathum Thani\", \"Phr…\n$ ProvCode &lt;chr&gt; \"TH10\", \"TH11\", \"TH12\", \"TH13\", \"TH14\", \"TH15\", \"TH16\", \"TH17…\n$ geometry &lt;MULTIPOLYGON [°]&gt; MULTIPOLYGON (((100.6139 13..., MULTIPOLYGON (((…\n\n\nWe can check if there are any missing values by using is.na() and then check across each column using colSums() from Base R.\n\ncolSums(is.na(thai_sf))\n\nProvince ProvCode geometry \n       0        0        0 \n\n\nThe output shows that there are no missing values for any of the retained columns.\nFinally, we can quickly check if the object depicts Thailand properly by producing a quick map using qtm() from tmap package.\n\nqtm(thai_sf)"
  },
  {
    "objectID": "posts/thai-tourism-covid/index.html#b.2.-thailand-tourism-data-by-province-jan-2019---feb-2023",
    "href": "posts/thai-tourism-covid/index.html#b.2.-thailand-tourism-data-by-province-jan-2019---feb-2023",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "B.2. Thailand Tourism Data by Province, Jan 2019 - Feb 2023",
    "text": "B.2. Thailand Tourism Data by Province, Jan 2019 - Feb 2023\nThe code chunk below loads the tourism statistics data into a dataframe tourism. We use read_csv() to import the data from the file.\n\ntourism &lt;- read_csv(\"data/aspatial/thailand_domestic_tourism_2019_2023.csv\")\n\nRows: 30800 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): province_thai, province_eng, region_thai, region_eng, variable\ndbl  (1): value\ndate (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWe can check the contents by using the code chunk below.\n\ntourism\n\n# A tibble: 30,800 × 7\n   date       province_thai province_eng   region_thai region_eng variable value\n   &lt;date&gt;     &lt;chr&gt;         &lt;chr&gt;          &lt;chr&gt;       &lt;chr&gt;      &lt;chr&gt;    &lt;dbl&gt;\n 1 2019-01-01 กรุงเทพมหานคร  Bangkok        ภาคกลาง     central    occupan…  93.4\n 2 2019-01-01 ลพบุรี          Lopburi        ภาคกลาง     central    occupan…  61.3\n 3 2019-01-01 พระนครศรีอยุธยา Phra Nakhon S… ภาคกลาง     central    occupan…  73.4\n 4 2019-01-01 สระบุรี         Saraburi       ภาคกลาง     central    occupan…  67.3\n 5 2019-01-01 ชัยนาท         Chainat        ภาคกลาง     central    occupan…  79.3\n 6 2019-01-01 นครปฐม        Nakhon Pathom  ภาคกลาง     central    occupan…  71.7\n 7 2019-01-01 สิงห์บุรี         Sing Buri      ภาคกลาง     central    occupan…  64.6\n 8 2019-01-01 อ่างทอง        Ang Thong      ภาคกลาง     central    occupan…  71.2\n 9 2019-01-01 นนทบุรี         Nonthaburi     ภาคกลาง     central    occupan…  75.1\n10 2019-01-01 ปทุมธานี        Pathum Thani   ภาคกลาง     central    occupan…  60.8\n# ℹ 30,790 more rows\n\n\nThe imported data contains 7 fields and 30,800 records at a province and month level.\nBefore we analyze the dataset, let use remove unnecessary columns and rename the column names, similar to the previous dataset, using the code chunk below. (by using select() and rename())\n\ntourism &lt;- tourism %&gt;%\n  select(date, province_eng, region_eng, variable, value) %&gt;%\n  rename(Date = date, Province = province_eng, Region = region_eng, Indicator = variable, Value = value)\n\nhead(tourism)\n\n# A tibble: 6 × 5\n  Date       Province                 Region  Indicator      Value\n  &lt;date&gt;     &lt;chr&gt;                    &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 2019-01-01 Bangkok                  central occupancy_rate  93.4\n2 2019-01-01 Lopburi                  central occupancy_rate  61.3\n3 2019-01-01 Phra Nakhon Si Ayutthaya central occupancy_rate  73.4\n4 2019-01-01 Saraburi                 central occupancy_rate  67.3\n5 2019-01-01 Chainat                  central occupancy_rate  79.3\n6 2019-01-01 Nakhon Pathom            central occupancy_rate  71.7\n\n\nWe have kept only five of the columns which provides the date, the English descriptions for the location (province and region) as well as the (potential) tourism indicator and its value.\nWe can also check for any missing values across these five columns using the code below. (using is.na() and colSums())\n\ncolSums(is.na(tourism))\n\n     Date  Province    Region Indicator     Value \n        0         0         0         0         0 \n\n\nEach province will be repeated across multiple dates and across multiple indicators. Let us first doublecheck the different values in Indicator. We use unique() in the code chunk below to achieve this.\n\nunique(tourism$Indicator)\n\n[1] \"occupancy_rate\"      \"no_tourist_occupied\" \"no_tourist_all\"     \n[4] \"no_tourist_thai\"     \"no_tourist_foreign\"  \"net_profit_all\"     \n[7] \"net_profit_thai\"     \"net_profit_foreign\" \n\n\nWe are aware that the ‘net_profit’ indicators are actually revenue so it is better to update them now to avoid misunderstanding later. We use recode() from dplyr to replace instances with alternative values.\n\ntourism &lt;- tourism %&gt;%\n  mutate(Indicator = recode(Indicator,\n                            \"net_profit_all\" = \"revenue_all\",\n                            \"net_profit_thai\" = \"revenue_thai\",\n                            \"net_profit_foreign\" = \"revenue_foreign\"))\n\nunique(tourism$Indicator)\n\n[1] \"occupancy_rate\"      \"no_tourist_occupied\" \"no_tourist_all\"     \n[4] \"no_tourist_thai\"     \"no_tourist_foreign\"  \"revenue_all\"        \n[7] \"revenue_thai\"        \"revenue_foreign\"    \n\n\nWe will not define which indicators to use until we perform some EDA (Exploratory Data Analysis) in the next section.\nBefore we move to the next section, we will also introduce some columns into the dataset to make filtering and other analysis easier. For now, we will do this by adding columns for the months and years based on the Date column.\n\ntourism &lt;- tourism %&gt;%\n  mutate(Year = year(Date)) %&gt;%\n  mutate(MonthNum = month(Date)) %&gt;%\n  mutate(Month = month(Date, label = TRUE, abbr = TRUE)) %&gt;%\n  mutate(MonthYear = format(ymd(Date), \"%Y-%m\"))\n\nhead(tourism)\n\n# A tibble: 6 × 9\n  Date       Province      Region Indicator Value  Year MonthNum Month MonthYear\n  &lt;date&gt;     &lt;chr&gt;         &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;ord&gt; &lt;chr&gt;    \n1 2019-01-01 Bangkok       centr… occupanc…  93.4  2019        1 Jan   2019-01  \n2 2019-01-01 Lopburi       centr… occupanc…  61.3  2019        1 Jan   2019-01  \n3 2019-01-01 Phra Nakhon … centr… occupanc…  73.4  2019        1 Jan   2019-01  \n4 2019-01-01 Saraburi      centr… occupanc…  67.3  2019        1 Jan   2019-01  \n5 2019-01-01 Chainat       centr… occupanc…  79.3  2019        1 Jan   2019-01  \n6 2019-01-01 Nakhon Pathom centr… occupanc…  71.7  2019        1 Jan   2019-01"
  },
  {
    "objectID": "posts/thai-tourism-covid/index.html#c.1-tourism-revenue",
    "href": "posts/thai-tourism-covid/index.html#c.1-tourism-revenue",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "C.1 Tourism Revenue",
    "text": "C.1 Tourism Revenue\nWe first look at tourism revenue which is currently reported in million Thai baht. We will use a constant rate of 34.784 THB per USD based on 2023 exchange rates to scale down the numbers and transform it into something more recognizable for most of the readers.\nWe first create a plot for the monthly tourism revenue in total and by foreign and local tourists. The code below selects the relevant data and prepares the line plot using ggplot(). Finally, we use ggplotly() to render it as an interactive chart so we can easily examine the resulting chart.\n\n# Subset the data to just the required indicators\naggregated_data &lt;- tourism %&gt;%\n  filter(Indicator %in% c(\"revenue_all\", \"revenue_thai\", \"revenue_foreign\")) %&gt;%\n  group_by(MonthYear, Indicator) %&gt;%\n  summarise(TotalValue = sum(Value, na.rm = TRUE) / 34.784) %&gt;%\n  ungroup()\n\n`summarise()` has grouped output by 'MonthYear'. You can override using the\n`.groups` argument.\n\n# Create the line chart\np &lt;- ggplot(aggregated_data, aes(x = MonthYear, y = TotalValue, color = Indicator, group = Indicator)) +\n  geom_line(size = 1) +\n  labs(title = \"Thai Tourism Revenue by Month\",\n       y = \"Million USD\",\n       x = \"Month-Year\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),\n        legend.position = \"bottom\") +\n  scale_color_manual(values = c(\"revenue_all\" = \"blue\", \"revenue_thai\" = \"green\", \"revenue_foreign\" = \"red\"))\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n# Convert the ggplot chart to an interactive plotly chart\ninteractive_plot &lt;- ggplotly(p)\n\n# Display the interactive plot\ninteractive_plot\n\n\n\n\n\nThe resulting chart is consistent with the expectation on the impact and recovery from the pandemic. We can view the above chart as a timeline:\n\nUp to (mid)January 2020: Pre-covid. No travel restrictions have been set yet. (Jan 2019 - Jan 2020, 13 months)\nFeb 2020 to November 2021: Covid. Various lockdown measures in place. All foreign non-essential travel is banned. There is some local tourist activity, but another set of measures in May 2021 again prevents non-essential movement (Feb 2020 - Oct 2021, 22 months)\nNovember 2021 onwards: Post-Covid. Travel restrictions have been eased or lifted and tourism revenues have been recovering (Nov 2021 - Feb 2023, 16 months)\n\nPre- and post-covid, we see that foreign tourists contribute more to the overall revenue, and their contribution has a large amount of variance. Local tourists during the same period have contributed a more stable amount month-on-month.\nWe can code the three periods mentioned above into the tourism dataset for convenience. We use the ifelse() function to do this based on the cutoff dates mentioned above.\n\ntourism$Period &lt;- ifelse(tourism$Date &lt; as.Date(\"2020-02-01\"), \"Pre-Covid\",\n                         ifelse(tourism$Date &gt; as.Date(\"2021-10-01\"), \"Post-Covid\", \"Covid\"))\n\nWe can next check the tourism revenue at the province level. As plotting all 77 provinces across all the periods will not produce readable charts, we will focus on top 20 provinces for the different periods. We will also take the average monthly revenue rather than the total since each period has a different number of months.\nWe prepare a new dataframe that summarizes the indicators for each province in tourism. Aside from the average revenue per period, we will also compute for the average number of visitors as well as the average spend per visitor.\n\ntourism_period &lt;- tourism %&gt;%\n  group_by(Province) %&gt;%\n  summarise(\n    PreCovid_Revenue_total = sum(Value[Period == \"Pre-Covid\" & Indicator == \"revenue_all\"], na.rm = TRUE)/13/34.784,\n    Covid_Revenue_total = sum(Value[Period == \"Covid\" & Indicator == \"revenue_all\"], na.rm = TRUE)/22/34.784,\n    PostCovid_Revenue_total = sum(Value[Period == \"Post-Covid\" & Indicator == \"revenue_all\"], na.rm = TRUE)/16/34.784,\n    PreCovid_Revenue_foreign = sum(Value[Period == \"Pre-Covid\" & Indicator == \"revenue_foreign\"], na.rm = TRUE)/13/34.784,\n    Covid_Revenue_foreign = sum(Value[Period == \"Covid\" & Indicator == \"revenue_foreign\"], na.rm = TRUE)/22/34.784,\n    PostCovid_Revenue_foreign = sum(Value[Period == \"Post-Covid\" & Indicator == \"revenue_foreign\"], na.rm = TRUE)/16/34.784,\n    PreCovid_Revenue_thai = sum(Value[Period == \"Pre-Covid\" & Indicator == \"revenue_thai\"], na.rm = TRUE)/13/34.784,\n    Covid_Revenue_thai = sum(Value[Period == \"Covid\" & Indicator == \"revenue_thai\"], na.rm = TRUE)/22/34.784,\n    PostCovid_Revenue_thai = sum(Value[Period == \"Post-Covid\" & Indicator == \"revenue_thai\"], na.rm = TRUE)/16/34.784,\n    PreCovid_tourists_total = sum(Value[Period == \"Pre-Covid\" & Indicator == \"no_tourist_all\"], na.rm = TRUE)/13,\n    Covid_tourists_total = sum(Value[Period == \"Covid\" & Indicator == \"no_tourist_all\"], na.rm = TRUE)/22,\n    PostCovid_tourists_total = sum(Value[Period == \"Post-Covid\" & Indicator == \"no_tourist_all\"], na.rm = TRUE)/16,\n    PreCovid_tourists_foreign = sum(Value[Period == \"Pre-Covid\" & Indicator == \"no_tourist_foreign\"], na.rm = TRUE)/13,\n    Covid_tourists_foreign = sum(Value[Period == \"Covid\" & Indicator == \"no_tourist_foreign\"], na.rm = TRUE)/22,\n    PostCovid_tourists_foreign = sum(Value[Period == \"Post-Covid\" & Indicator == \"no_tourist_foreign\"], na.rm = TRUE)/16,\n    PreCovid_tourists_thai = sum(Value[Period == \"Pre-Covid\" & Indicator == \"no_tourist_thai\"], na.rm = TRUE)/13,\n    Covid_tourists_thai = sum(Value[Period == \"Covid\" & Indicator == \"no_tourist_thai\"], na.rm = TRUE)/22,\n    PostCovid_tourists_thai = sum(Value[Period == \"Post-Covid\" & Indicator == \"no_tourist_thai\"], na.rm = TRUE)/16\n  ) %&gt;%\n  mutate(PreCovidSpend_total = PreCovid_Revenue_total / PreCovid_tourists_total * 1000000) %&gt;%\n  mutate(CovidSpend_total = Covid_Revenue_total / Covid_tourists_total * 1000000) %&gt;%\n  mutate(PostCovidSpend_total = PostCovid_Revenue_total / PostCovid_tourists_total * 1000000) %&gt;%\n  mutate(PreCovidSpend_foreign = PreCovid_Revenue_foreign / PreCovid_tourists_foreign * 1000000) %&gt;%\n  mutate(CovidSpend_foreign = Covid_Revenue_foreign / Covid_tourists_foreign * 1000000) %&gt;%\n  mutate(PostCovidSpend_foreign = PostCovid_Revenue_foreign / PostCovid_tourists_foreign * 1000000) %&gt;%\n  mutate(PreCovidSpend_thai = PreCovid_Revenue_thai / PreCovid_tourists_thai * 1000000) %&gt;%\n  mutate(CovidSpend_thai = Covid_Revenue_thai / Covid_tourists_thai * 1000000) %&gt;%\n  mutate(PostCovidSpend_thai = PostCovid_Revenue_thai / PostCovid_tourists_thai * 1000000)\n\nWith the summarized dataframe prepared, we can now prepare a few visualizations to look at the provinces with regards to the average monthly tourism revenue.\nFirst, let us try using a scatterplot to see both the average revenue pre-Covid (x-axis) and post-Covid. (y-axis) Provinces with the highest pre-Covid revenue will appear the rightmost, while those that have the highest post-Covid revenue will appear the rightmost.\nThe code below uses the plotly package to produce an interactive scatterplot of the pre- and post-covid average monthly revenue for all tourists. With the interactive chart, the province names will be visible by hovering over and the user can zoom in by creating a selection in the chart.\n\nplot &lt;- plot_ly(\n  data = tourism_period,\n  x = ~PreCovid_Revenue_total,\n  y = ~PostCovid_Revenue_total,\n  type = 'scatter',\n  mode = 'markers',\n  text = ~Province,\n  hoverinfo = 'text'\n) %&gt;%\n  layout(\n    title = 'Monthly Pre- and Post-Covid Revenue - M-USD',\n    xaxis = list(title = 'Average PreCovid Revenue'),\n    yaxis = list(title = 'Average PostCovid Revenue')\n  )\n\n# Display the plot\nplot\n\n\n\n\n\nBaed on the plot, we see that Bangkok, Phuket and Chonburi have consistently been the top 3 highest revenue generating before and after the pandemic. When we look further down the list, we see some shifts for some of the provinces.\nTo aid the reader, we recreate the chart with those top 3 provinces excluded using the code chunk below.\n\nplot &lt;- plot_ly(\n  data = filter(tourism_period, !(Province %in% c(\"Bangkok\", \"Phuket\", \"Chonburi\"))),\n  x = ~PreCovid_Revenue_total,\n  y = ~PostCovid_Revenue_total,\n  type = 'scatter',\n  mode = 'markers',\n  text = ~Province,\n  hoverinfo = 'text'\n) %&gt;%\n  layout(\n    title = 'Monthly Pre- and Post-Covid Revenue - M-USD, exc Top 3 Provinces',\n    xaxis = list(title = 'Average PreCovid Revenue'),\n    yaxis = list(title = 'Average PostCovid Revenue')\n  )\n\n# Display the plot\nplot\n\n\n\n\n\nSome key observations from the above charts are:\n\nChiang Mai has moved from top 5 to top 4. A large reason for this is a drop from Krabi. Pre-covid, Krabi was top 4, but has dropped to at least top 10.\nChiang Rai and Prachuap Khiri Khan have risen to top 5 and 6. These provinces were top 9 or lower before.\nSongkhla and Phang Nga were in the top 10 pre-Covid but are also showing a drop in ranking post-Covid\n\nWe can do the same chart for just the revenue from foreign tourists using the code chunk below.\n\nplot &lt;- plot_ly(\n  data = tourism_period,\n  x = ~PreCovid_Revenue_foreign,\n  y = ~PostCovid_Revenue_foreign,\n  type = 'scatter',\n  mode = 'markers',\n  text = ~Province,\n  hoverinfo = 'text'\n) %&gt;%\n  layout(\n    title = 'Monthly Pre- and Post-Covid Revenue from Foreign Tourists - M-USD',\n    xaxis = list(title = 'Average PreCovid Revenue'),\n    yaxis = list(title = 'Average PostCovid Revenue')\n  )\n\n# Display the plot\nplot\n\n\n\n\n\nThe top 3 provinces are the same for both, but there are differences further down the list.\nWe can also show these numbers graphically in a map. Before we do this, let us add one set of measures in tourism_period to indicate the recovery rate. This will be the ratio of the post-covid and pre-covid measures and will indicate how much of the pre-covid level has been achieved. (on average)\nWe perform this using the code chunk below. We do this for all sets of measures across revenue, number of tourists and the average spending.\n\ntourism_period &lt;- tourism_period %&gt;%\n  mutate(Revenue_total_recovery = PostCovid_Revenue_total / PreCovid_Revenue_total) %&gt;%\n  mutate(Revenue_foreign_recovery = PostCovid_Revenue_foreign / PreCovid_Revenue_foreign) %&gt;%\n  mutate(Revenue_thai_recovery = PostCovid_Revenue_thai / PreCovid_Revenue_thai) %&gt;%\n  mutate(Tourists_total_recovery = PostCovid_tourists_total / PreCovid_tourists_total) %&gt;%\n  mutate(Tourists_foreign_recovery = PostCovid_tourists_foreign / PreCovid_tourists_foreign) %&gt;%\n  mutate(Tourists_thai_recovery = PostCovid_tourists_thai / PreCovid_tourists_thai) %&gt;%\n  mutate(Spend_total_recovery = PostCovidSpend_total / PreCovidSpend_total) %&gt;%\n  mutate(Spend_foreign_recovery = PostCovidSpend_foreign / PreCovidSpend_foreign) %&gt;%\n  mutate(Spend_thai_recovery = PostCovidSpend_thai / PreCovidSpend_thai)\n\nWe need to include all the indicators into the sf dataframe. This means merging the tourism_period and the thai_sf dataframes. Let us first check that the naming is the same for both dataframes by checking which values do not have a match. We use the code below which uses left_join() to match and then filter() to check those that do not have matches.\n\n# Identify mismatched Province names in tourism_period\nmismatched_values &lt;- tourism_period %&gt;%\n  left_join(thai_sf, by = \"Province\") %&gt;%\n  filter(is.na(ProvCode)) %&gt;%\n  select(Province)\n\nmismatched_tourism &lt;- mismatched_values$Province\n\n# Identify mismatched Province names in thai_sf\nmismatched_values &lt;- thai_sf %&gt;%\n  left_join(tourism_period, by = \"Province\") %&gt;%\n  filter(is.na(Covid_Revenue_total)) %&gt;%\n  select(Province)\n\nmismatched_thai &lt;- mismatched_values$Province\n\n# Print the mismatched values\nlist(\n  mismatched_in_tourism_period = mismatched_tourism,\n  mismatched_in_thai_sf = mismatched_thai\n)\n\n$mismatched_in_tourism_period\n[1] \"Buriram\"         \"Chainat\"         \"Chonburi\"        \"Lopburi\"        \n[5] \"Nong Bua Lamphu\" \"Phang Nga\"       \"Prachinburi\"     \"Sisaket\"        \n\n$mismatched_in_thai_sf\n[1] \"Lop Buri\"         \"Chai Nat\"         \"Chon Buri\"        \"Prachin Buri\"    \n[5] \"Buri Ram\"         \"Si Sa Ket\"        \"Nong Bua Lam Phu\" \"Phangnga\"        \n\n\nWe see that there are 8 mismatched province names for each of the dataframes. We need to standardize these namings to ensure that the indicators are mapped to the correct province. We will opt to keep the descriptions from tourism_period which gives more compact naming. We use recode() in the code chunk below to accomplish this in a new dataframe.\n\nthaitourism_sf &lt;- thai_sf %&gt;%\n  mutate(Province = recode(Province,\n                            \"Lop Buri\" = \"Lopburi\",\n                            \"Chai Nat\" = \"Chainat\",\n                            \"Chon Buri\" = \"Chonburi\",\n                            \"Prachin Buri\" = \"Prachinburi\",\n                            \"Buri Ram\" = \"Buriram\",\n                            \"Si Sa Ket\" = \"Sisaket\",\n                            \"Nong Bua Lam Phu\" = \"Nong Bua Lamphu\",\n                            \"Phangnga\" = \"Phang Nga\"))\n\nWe can now use leftjoin() in the codechunk below to merge the two datasets.\n\nthaitourism_sf &lt;- left_join(thaitourism_sf, tourism_period,\n                     by=c(\"Province\"=\"Province\"))\n\nThe code chunk below confirms that the new object is still an sf dataframe.\n\nclass(thaitourism_sf)\n\n[1] \"sf\"         \"data.frame\"\n\n\nWe can use the tmap package to produce side-by side maps of Thailand with the average monthly tourism revenue before and after covid using the code chunk below. Given the wide range of values, we will use quantiles for the data classes. We also include the recovery rate of each of the provinces as a third map.\n\ntm_shape(thaitourism_sf) +\n  tm_fill(col = c(\"PreCovid_Revenue_total\", \"PostCovid_Revenue_total\", \"Revenue_total_recovery\"),\n          style = \"quantile\",\n          palette = list(\"viridis\", \"viridis\", \"RdYlGn\"),\n          title = c(\"Monthly Revenue - M-USD\", \"Monthly Revenue - M-USD\", \"Post vs Pre\")) +\n  tm_borders(col = \"black\") +\n  tm_layout(title = c(\"Pre Covid\", \"Post Covid\", \"Recovery Rate\"),\n            title.position = c(\"right\", \"top\"),\n            legend.position = c(\"right\", \"bottom\"),\n            bg.color = \"grey90\")\n\n\n\n\n\n\n\n\nAs also seen in the scatterplot, we also see some change in rankings with the maps. For example, some provinces previously in the top 20% have moved down to the next 20%. (e.g., Khon Kaen and Phang Nga)\nIf we focus on the third map, we also see what seems like a cluster of provinces in the south which are lagging with regards to their recovery on the average tourism revenue. We will watch out for these once we conduct our cluster analyses."
  },
  {
    "objectID": "posts/thai-tourism-covid/index.html#c.2-number-of-tourists",
    "href": "posts/thai-tourism-covid/index.html#c.2-number-of-tourists",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "C.2 Number of Tourists",
    "text": "C.2 Number of Tourists\nThe next measure we can look at is the number of tourists. We produce a similar line chart as we did for tourism revenue with the code chunk below. We display the number of tourists in thousands.\n\n# Subset the data to just the required indicators\naggregated_data &lt;- tourism %&gt;%\n  filter(Indicator %in% c(\"no_tourist_all\", \"no_tourist_thai\", \"no_tourist_foreign\")) %&gt;%\n  group_by(MonthYear, Indicator) %&gt;%\n  summarise(TotalValue = sum(Value, na.rm = TRUE) / 1000) %&gt;%\n  ungroup()\n\n`summarise()` has grouped output by 'MonthYear'. You can override using the\n`.groups` argument.\n\n# Create the line chart\np &lt;- ggplot(aggregated_data, aes(x = MonthYear, y = TotalValue, color = Indicator, group = Indicator)) +\n  geom_line(size = 1) +\n  labs(title = \"Thailand Number of Tourists by Month\",\n       y = \"Tourists, Thousands\",\n       x = \"Month-Year\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),\n        legend.position = \"bottom\") +\n  scale_color_manual(values = c(\"no_tourist_all\" = \"blue\", \"no_tourist_thai\" = \"green\", \"no_tourist_foreign\" = \"red\"))\n\n# Convert the ggplot chart to an interactive plotly chart\ninteractive_plot &lt;- ggplotly(p)\n\n# Display the interactive plot\ninteractive_plot\n\n\n\n\n\nThe trend for the total follows the same general movement as the chart for revenue, however, it looks like tourist numbers are primarily driven by locals than foreigners.\nSimilar to the previous section, we can produce an interactive scatterplot to see the number of tourists each province gets on average before and after the pandemic. We do this first for the total number of tourists.\n\nplot &lt;- plot_ly(\n  data = tourism_period,\n  x = ~PreCovid_tourists_total,\n  y = ~PostCovid_tourists_total,\n  type = 'scatter',\n  mode = 'markers',\n  text = ~Province,\n  hoverinfo = 'text'\n) %&gt;%\n  layout(\n    title = 'Monthly Pre- and Post-Covid No of Tourists',\n    xaxis = list(title = 'PreCovid Average'),\n    yaxis = list(title = 'PostCovid Average')\n  )\n\n# Display the plot\nplot\n\n\n\n\n\nBangkok, Chonburi and Phuket had the highest number of visitors pre-Covid, but Phuket appears to have dropped off in favor of Kanchanaburi post-covid.\nWe can also look at the foreign tourists using the code chunk below. We skip local tourists and focus only on foreign ones as they deviate from the overall number.\n\nplot &lt;- plot_ly(\n  data = tourism_period,\n  x = ~PreCovid_tourists_foreign,\n  y = ~PostCovid_tourists_foreign,\n  type = 'scatter',\n  mode = 'markers',\n  text = ~Province,\n  hoverinfo = 'text'\n) %&gt;%\n  layout(\n    title = 'Monthly Pre- and Post-Covid No of Foreign Tourists',\n    xaxis = list(title = 'PreCovid Average'),\n    yaxis = list(title = 'PostCovid Average')\n  )\n\n# Display the plot\nplot\n\n\n\n\n\nBangkok, Phuket and Chonburi appear as the top 3 destinations for foreign tourists (in terms of number) before and after Covid. We also see that Krabi dropped from the top 3 post covid.\nWe can also produce side-by-side maps for the number of tourists and their recovery rates using the code chunk below.\n\ntm_shape(thaitourism_sf) +\n  tm_fill(col = c(\"PreCovid_tourists_total\", \"PostCovid_tourists_total\", \"Tourists_total_recovery\"),\n          style = \"quantile\",\n          palette = list(\"viridis\", \"viridis\", \"RdYlGn\"),\n          title = c(\"Monthly Tourists\", \"Monthly Tourists\", \"Post vs Pre\")) +\n  tm_borders(col = \"black\") +\n  tm_layout(title = c(\"Pre Covid\", \"Post Covid\", \"Recovery Rate\"),\n            title.position = c(\"right\", \"top\"),\n            legend.position = c(\"right\", \"bottom\"),\n            bg.color = \"grey90\")\n\n\n\n\n\n\n\n\nWe again see the southern region mostly lagging with regards to their recovery post covid also in terms of the number of visitors."
  },
  {
    "objectID": "posts/thai-tourism-covid/index.html#c.3-average-spend-per-visitor",
    "href": "posts/thai-tourism-covid/index.html#c.3-average-spend-per-visitor",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "C.3 Average Spend Per Visitor",
    "text": "C.3 Average Spend Per Visitor\nWe next look at the average per spend per visitor which is the quotient of the tourism revenue and the total number of tourists. This will tell us whether tourists are spending more or less around the pandemic.\nFirst we produce a similar line graph as before to look at the trend at an overall picture using the code chunk below.\n\n# Subset the data to just the required indicators\naggregated_data &lt;&lt;- tourism %&gt;%\n  group_by(MonthYear) %&gt;%\n  summarise(\n    Spend_total = sum(Value[Indicator == \"revenue_all\"]) / sum(Value[Indicator == \"no_tourist_all\"]) * 1000000 / 34.784,\n    Spend_thai = sum(Value[Indicator == \"revenue_thai\"]) / sum(Value[Indicator == \"no_tourist_thai\"]) * 1000000 / 34.784,\n    Spend_foreign = sum(Value[Indicator == \"revenue_foreign\"]) / sum(Value[Indicator == \"no_tourist_foreign\"]) * 1000000 / 34.784\n  ) %&gt;%\n  pivot_longer(cols = starts_with(\"Spend\"), names_to = \"Indicator\", values_to = \"TotalValue\") %&gt;%\n  mutate(Indicator = case_when(\n    Indicator == \"Spend_total\" ~ \"Spend_total\",\n    Indicator == \"Spend_thai\" ~ \"Spend_thai\",\n    Indicator == \"Spend_foreign\" ~ \"Spend_foreign\"\n  ))\n\n# Create the line chart\np &lt;- ggplot(aggregated_data, aes(x = MonthYear, y = TotalValue, color = Indicator, group = Indicator)) +\n  geom_line(size = 1) +\n  labs(title = \"Thailand Average Spend Per Tourist by Month\",\n       y = \"Average Spend, USD\",\n       x = \"Month-Year\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),\n        legend.position = \"bottom\") +\n  scale_color_manual(values = c(\"Spend_total\" = \"blue\", \"Spend_thai\" = \"green\", \"Spend_foreign\" = \"red\"))\n\n# Convert the ggplot chart to an interactive plotly chart\ninteractive_plot &lt;- ggplotly(p)\n\n# Display the interactive plot\ninteractive_plot\n\n\n\n\n\nThe chart shows that the average spend of foreign tourists are much higher than local ones and appears to be the same before and after Covid. There appears to be a shift in the average spend for all tourists overall which is probably driven by an increase in the contribution for the number of local tourists versus foreign tourists. Local tourists appear to show a step decrease after covid as well in terms of their average spending.\nNext, we check the average spending of tourists across provinces using a similar scatterplot as before.\n\nplot &lt;- plot_ly(\n  data = tourism_period,\n  x = ~PreCovidSpend_total,\n  y = ~PostCovidSpend_total,\n  type = 'scatter',\n  mode = 'markers',\n  text = ~Province,\n  hoverinfo = 'text'\n) %&gt;%\n  layout(\n    title = 'Monthly Pre- and Post-Covid Average Spend Per Tourist',\n    xaxis = list(title = 'PreCovid Average $'),\n    yaxis = list(title = 'PostCovid Average $')\n  )\n\n# Display the plot\nplot\n\n\n\n\n\nPhuket saw the highest average spend pre- and post-covid. Krabi was second highest pre-Covid but drop to third, as Bangkok rose from fourth to second over that period.\nWe can do the same for foreign tourists as the earlier chart showed that it was very different from the total. We use the code chunk below tor produce a similar scatterplot but taking the foreign tourist figures.\n\nplot &lt;- plot_ly(\n  data = tourism_period,\n  x = ~PreCovidSpend_foreign,\n  y = ~PostCovidSpend_foreign,\n  type = 'scatter',\n  mode = 'markers',\n  text = ~Province,\n  hoverinfo = 'text'\n) %&gt;%\n  layout(\n    title = 'Monthly Pre- and Post-Covid Average Spend Per Foreign Tourist',\n    xaxis = list(title = 'PreCovid Average $'),\n    yaxis = list(title = 'PostCovid Average $')\n  )\n\n# Display the plot\nplot\n\n\n\n\n\nPhuket, Bangkok and Chonburi have been consistent as top 4 highest spend for foreign tourists. From the chart above, we see that Chiang Rai has risen to the number five spot for highest foreign tourist spending post-Covid.\nWe can also produce a map visualization of the average spend using tmap package. We focus on the total tourist population in the visualization below.\n\ntm_shape(thaitourism_sf) +\n  tm_fill(col = c(\"PreCovidSpend_total\", \"PostCovidSpend_total\", \"Spend_total_recovery\"),\n          style = \"quantile\",\n          palette = list(\"viridis\", \"viridis\", \"RdYlGn\"),\n          title = c(\"Average Tourist Spending\", \"Average Tourist Spending\", \"Post vs Pre\")) +\n  tm_borders(col = \"black\") +\n  tm_layout(title = c(\"Pre Covid\", \"Post Covid\", \"Recovery Rate\"),\n            title.position = c(\"right\", \"top\"),\n            legend.position = c(\"right\", \"bottom\"),\n            bg.color = \"grey90\")\n\n\n\n\n\n\n\n\nWe again see slow recovery in the southern region, but at the same time, this region has the highest average spending pre- and post-covid. (i.e., top 20%)"
  },
  {
    "objectID": "posts/thai-tourism-covid/index.html#c.4-occupancy-rate",
    "href": "posts/thai-tourism-covid/index.html#c.4-occupancy-rate",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "C.4 Occupancy Rate",
    "text": "C.4 Occupancy Rate\nThe final single indicator we will look at is the occupancy rate. We have not prepared and included the data for occupancy rate before as this is a ratio measure which be just summed or averaged. In order to be able to aggregate occupancy rate, we not only need the actual occupancy rate from the data, but we also need the number of rooms occupied which is given by the no_tourist_occupied indicator in the data, and also the number of rooms in total– which is not included in the data.\nThe following code chunk prepares a new dataframe from tourist with the following transformation steps:\n\nRetain MonthYear, Province, and records for occupany rate and number of rooms occupied\nKeep values for the two indicators as separate columns. We use average just in case a province appears multiple times on the same date (to resolve conflicts)\nWe compute the total number of rooms as the number of rooms occupied divided by the occupancy rate\nWe add the tags for the period for pre- and post- covid\n\n\noccupancy_df &lt;- tourism %&gt;%\n  filter(Indicator %in% c(\"occupancy_rate\", \"no_tourist_occupied\")) %&gt;%\n  group_by(Date, MonthYear, Province) %&gt;%\n  summarise(\n    occupancy = mean(Value[Indicator == \"occupancy_rate\"], na.rm = TRUE),\n    occupied_rooms = mean(Value[Indicator == \"no_tourist_occupied\"], na.rm = TRUE)\n  ) %&gt;%\n  ungroup() %&gt;%\n  mutate(total_rooms = ifelse(occupancy == 0, 0, occupied_rooms / occupancy * 100))\n\n`summarise()` has grouped output by 'Date', 'MonthYear'. You can override using\nthe `.groups` argument.\n\noccupancy_df$Period &lt;- ifelse(occupancy_df$Date &lt; as.Date(\"2020-02-01\"), \"Pre-Covid\",\n                         ifelse(occupancy_df$Date &gt; as.Date(\"2021-10-01\"), \"Post-Covid\", \"Covid\"))\n\nhead(occupancy_df)\n\n# A tibble: 6 × 7\n  Date       MonthYear Province      occupancy occupied_rooms total_rooms Period\n  &lt;date&gt;     &lt;chr&gt;     &lt;chr&gt;             &lt;dbl&gt;          &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt; \n1 2019-01-01 2019-01   Amnat Charoen      65.2           8551      13125. Pre-C…\n2 2019-01-01 2019-01   Ang Thong          71.2          19140      26878. Pre-C…\n3 2019-01-01 2019-01   Bangkok            93.4        3334971    3571780. Pre-C…\n4 2019-01-01 2019-01   Bueng Kan          73.0          37974      52055. Pre-C…\n5 2019-01-01 2019-01   Buriram            71.3         113655     159493. Pre-C…\n6 2019-01-01 2019-01   Chachoengsao       59.4          38687      65130. Pre-C…\n\n\nBefore we produce the charts, let us update thaitourism_sf with the aggregated occupancy rates by:\n\nComputing Pre- and Post-covid total number of rooms and occupied rooms per province\nComputing Pre- and Post-covid occupancy rate per province based on step 1\nAdd the new columns into thaitourism_sf using left_join()\n\nThe first two steps are accomplished in the first code block while the third step is accomplished in the second.\n\noccupancy_summary &lt;- occupancy_df %&gt;%\n  group_by(Province) %&gt;%\n  summarise(\n    PreCovid_occupied_rooms = sum(occupied_rooms[Period == \"Pre-Covid\"], na.rm = TRUE),\n    PostCovid_occupied_rooms = sum(occupied_rooms[Period == \"Post-Covid\"], na.rm = TRUE),\n    PreCovid_total_rooms = sum(total_rooms[Period == \"Pre-Covid\"], na.rm = TRUE),\n    PostCovid_total_rooms = sum(total_rooms[Period == \"Post-Covid\"], na.rm = TRUE)\n  ) %&gt;%\n  mutate(\n    PreCovid_occupancy = ifelse(PreCovid_total_rooms == 0, 0, (PreCovid_occupied_rooms / PreCovid_total_rooms) * 100),\n    PostCovid_occupancy = ifelse(PostCovid_total_rooms == 0, 0, (PostCovid_occupied_rooms / PostCovid_total_rooms) * 100)\n  )\n\nhead(occupancy_summary)\n\n# A tibble: 6 × 7\n  Province    PreCovid_occupied_ro…¹ PostCovid_occupied_r…² PreCovid_total_rooms\n  &lt;chr&gt;                        &lt;dbl&gt;                  &lt;dbl&gt;                &lt;dbl&gt;\n1 Amnat Char…                 106667                  91166              189198.\n2 Ang Thong                   208960                 116396              324038.\n3 Bangkok                   39621389               23666935            47956613.\n4 Bueng Kan                   362415                 465507              608851.\n5 Buriram                    1319062                1827050             2137721.\n6 Chachoengs…                 518580                 372576              917461.\n# ℹ abbreviated names: ¹​PreCovid_occupied_rooms, ²​PostCovid_occupied_rooms\n# ℹ 3 more variables: PostCovid_total_rooms &lt;dbl&gt;, PreCovid_occupancy &lt;dbl&gt;,\n#   PostCovid_occupancy &lt;dbl&gt;\n\n\n\nthaitourism_sf &lt;- left_join(thaitourism_sf, occupancy_summary, by = c(\"Province\"=\"Province\"))\n\nclass(thaitourism_sf)\n\n[1] \"sf\"         \"data.frame\"\n\n\nFinally, we add a column for the recovery of the occupancy rate using the code chunk below.\n\nthaitourism_sf &lt;- mutate(thaitourism_sf, Occupancy_recovery = ifelse(PreCovid_occupancy == 0, 0, (PostCovid_occupancy / PreCovid_occupancy)), .before = -1)\n\n\nclass(thaitourism_sf)\n\n[1] \"sf\"         \"data.frame\"\n\n\nFor the first visualization, let us plot the occupancy rate at a total level. The code below summarizes based on occupancy_df and computes a national occupancy rate to plot in a line chart.\n\naggregated_data &lt;&lt;- occupancy_df %&gt;%\n  group_by(MonthYear) %&gt;%\n  summarise(\n    occupancy_rate = sum(occupied_rooms, na.rm = TRUE) / sum(total_rooms, na.rm = TRUE) * 100\n  )\n\nggplot(aggregated_data, aes(x = MonthYear, y = occupancy_rate, group = 1)) +\n  geom_line() +\n  labs(\n    title = \"Occupancy Rate Over Time\",\n    x = \"MonthYear\",\n    y = \"Occupancy Rate (%)\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\n\n\n\n\n\n\n\nThe chart shows that occupancy rate has picked up after October 2021 and appears to have more or less reached pre-covid levels in the most recent months.\nWe can also produce a scatterplot to show pre- and post-covid occupancy rates at a province level. We do this using the code chunk below which produces an interactive plot.\n\nplot &lt;- plot_ly(\n  data = occupancy_summary,\n  x = ~PreCovid_occupancy,\n  y = ~PostCovid_occupancy,\n  type = 'scatter',\n  mode = 'markers',\n  text = ~paste('Province:', Province, '&lt;br&gt;PreCovid Occupancy:', round(PreCovid_occupancy), '&lt;br&gt;PostCovid Occupancy:', round(PostCovid_occupancy)),\n  hoverinfo = 'text'\n) %&gt;%\n  layout(\n    title = 'Monthly Pre- and Post-Covid Occupancy Rate (%)',\n    xaxis = list(title = 'PreCovid Occupancy'),\n    yaxis = list(title = 'PostCovid Occupancy')\n  )\n\n# Display the plot\nplot\n\n\n\n\n\nThe occupancy rates are showing a dispersed pattern as the overall rankings on occupancy rates have significantly changed pre- and post-covid. Bangkok, Chonburi and Suphan Buri reported the highest occupancy rates before covid, but Nan, Chang Rai and Nakhon Phanom have the highest rates post covid.\nThe next visualization for this measure is a side-by-side map for the pre- and post-covid occupancy rates as well as the recovery rate for occupancy. We use the code chunk below which uses tmap package to produce the maps.\n\ntm_shape(thaitourism_sf) +\n  tm_fill(col = c(\"PreCovid_occupancy\", \"PostCovid_occupancy\", \"Occupancy_recovery\"),\n          style = \"quantile\",\n          palette = list(\"viridis\", \"viridis\", \"RdYlGn\"),\n          title = c(\"Average Occupancy Rate\", \"Average Occupancy Rate\", \"Post vs Pre\")) +\n  tm_borders(col = \"black\") +\n  tm_layout(title = c(\"Pre Covid\", \"Post Covid\", \"Recovery Rate\"),\n            title.position = c(\"right\", \"top\"),\n            legend.position = c(\"right\", \"bottom\"),\n            bg.color = \"grey90\")\n\n\n\n\n\n\n\n\nBefore we leave this section, let us try to understand occupancy rate a bit more. Going back to earlier, we want to understand if high occupancy rate post-Covid is being driven by the number of available rooms. To help us answer this, we create a scatterplot of the available rooms against the occupancy rate post-Covid using the code chunk below.\n\nplot &lt;- plot_ly(\n  data = occupancy_summary,\n  x = ~PostCovid_total_rooms,\n  y = ~PostCovid_occupancy,\n  type = 'scatter',\n  mode = 'markers',\n  text = ~paste('Province:', Province, '&lt;br&gt;Number of Rooms:', round(PostCovid_total_rooms), '&lt;br&gt;Occupancy Rate:', round(PostCovid_occupancy)),\n  hoverinfo = 'text'\n) %&gt;%\n  layout(\n    title = 'Post-Covid Number of Rooms and Occupancy Rate (%)',\n    xaxis = list(title = 'Number of Rooms'),\n    yaxis = list(title = 'Occupancy Rate')\n  )\n\n# Display the plot\nplot\n\n\n\n\n\nBelow 10M rooms, there appears to be an upward trend in occupancy rate to the number of rooms. Provinces with more than 10M rooms go against the trend and appear to be capped to 60% occupancy."
  },
  {
    "objectID": "posts/thai-tourism-covid/index.html#d.1-variable-selection",
    "href": "posts/thai-tourism-covid/index.html#d.1-variable-selection",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "D.1 Variable Selection",
    "text": "D.1 Variable Selection\nWe will turn our attention to the first three measures discussed discussed in the previous section: tourism revenue, number of tourists and average tourist spending. We will not analyse the occupancy rate further as it is highly dependent on the number of rooms.\nWe will focus on checking signs of spatial autocorrelation or association before and after covid, as well as the recovery rate at the overall level for these three indicators– so we will be looking at 9 variables for our analysis."
  },
  {
    "objectID": "posts/thai-tourism-covid/index.html#d.2-deriving-the-contiguity-and-weight-matrix",
    "href": "posts/thai-tourism-covid/index.html#d.2-deriving-the-contiguity-and-weight-matrix",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "D.2 Deriving the Contiguity and Weight Matrix",
    "text": "D.2 Deriving the Contiguity and Weight Matrix\nFor the tests for this section, we need to derive a neighbor list as well as a weight matrix for each province to its neighbors. Given the presence of islands, we need to use distance rather than contiguity to define neighbors.\nThe first step is to understand the distribution of distances between nearest neighbors to find a proper cut-off distance. The code chunk below\n\nlongitude &lt;- map_dbl(thaitourism_sf$geometry, ~st_centroid(.x)[[1]])\nlatitude &lt;- map_dbl(thaitourism_sf$geometry, ~st_centroid(.x)[[2]])\ncoords &lt;- cbind(longitude, latitude)\n\nk1 &lt;- knn2nb(knearneigh(coords))\nk1dists &lt;- unlist(nbdists(k1, coords, longlat = TRUE))\nsummary(k1dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  21.55   51.92   64.33   63.24   76.77  110.94 \n\n\nThe maximum distance is 110.94 so setting a distance threshold of 111 should ensure that each province should have at least one neighbor. We then produce a nearest neighbor list for each province using dnearneigh()\n\nwm_d111 &lt;- dnearneigh(coords, 0, 111, longlat = TRUE)\nwm_d111\n\nNeighbour list object:\nNumber of regions: 77 \nNumber of nonzero links: 350 \nPercentage nonzero weights: 5.903188 \nAverage number of links: 4.545455 \n2 disjoint connected subgraphs\n\n\nWe import this as a new column in our sf object and compute for weights using st_weights() based on this.\n\nwm_thai &lt;- thaitourism_sf %&gt;%\n  mutate(nb = I(wm_d111),\n         wt = st_weights(nb,\n                         style=\"W\"),\n         .before=1)"
  },
  {
    "objectID": "posts/thai-tourism-covid/index.html#d.3-global-morans-i-test",
    "href": "posts/thai-tourism-covid/index.html#d.3-global-morans-i-test",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "D.3 Global Moran’s I Test",
    "text": "D.3 Global Moran’s I Test\nGlobal tests of spatial autocorrelation compares the value of each point/province to the overall value in order to conclude on spatial dependence. For this, we will focus on using Global Moran’s I which will work on the following general hypotheses:\n\n\\(H_0\\) - The value of (variable) is randomly distributed across provinces in Thailand\n\\(H_1\\) - The value of (variable) is not randomly distributed across provinces in Thailand\n\nFurther, the value of the test statistic \\(I\\) will also give indication on the underlying pattern:\n\n\\(I &gt; 0\\) - Clustering; observations tend to be similar\n\\(I &lt; 0\\) - Dispersed / regular; observations tend to be dissimilar\nWhere \\(I\\) close to zero - observations are arranged randomly\n\nTo perform Global Moran’s I test, with permutations, we use global_moran_perm() from sfdep package. We will use a 5% significance level for all the testing to be performed, and we will run 100 permutations / simulations for each test.\n\nD.3.1 Global Moran’s Test on Tourism Revenue\nThe code chunks in the tabs below run the Global Moran’s I permutation test on total tourism revenue (per month) pre-Covid, post-Covid and the recovery rate.\n\nPre-Covid RevenuePost-Covid RevenueRevenue Recovery Rate\n\n\n\nglobal_moran_perm(wm_thai$PreCovid_Revenue_total,\n                  wm_thai$nb,\n                  wm_thai$wt,\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.049413, observed rank = 93, p-value = 0.14\nalternative hypothesis: two.sided\n\n\n\n\n\nglobal_moran_perm(wm_thai$PostCovid_Revenue_total,\n                  wm_thai$nb,\n                  wm_thai$wt,\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.020361, observed rank = 84, p-value = 0.32\nalternative hypothesis: two.sided\n\n\n\n\n\nglobal_moran_perm(wm_thai$Revenue_total_recovery,\n                  wm_thai$nb,\n                  wm_thai$wt,\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.43763, observed rank = 100, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\n\n\n\nThe results show no evidence to reject spatial independence for the total revenue pre- and post-Covid. However, it shows signs of clustering for the revenue recovery rate as the p-value is below 0.05 and the statistic is above 1.\n\n\nD.3.2 Global Moran’s Test on Number of Tourists\nThe code chunks in the tabs below run the Global Moran’s I permutation test on total number of tourists (per month) pre-Covid, post-Covid and the recovery rate.\n\nPre-Covid Number of TouristsPost-Covid Number of TouristsNumber of Tourists Recovery Rate\n\n\n\nglobal_moran_perm(wm_thai$PreCovid_tourists_total,\n                  wm_thai$nb,\n                  wm_thai$wt,\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.062493, observed rank = 92, p-value = 0.16\nalternative hypothesis: two.sided\n\n\n\n\n\nglobal_moran_perm(wm_thai$PostCovid_tourists_total,\n                  wm_thai$nb,\n                  wm_thai$wt,\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.10696, observed rank = 95, p-value = 0.1\nalternative hypothesis: two.sided\n\n\n\n\n\nglobal_moran_perm(wm_thai$Tourists_total_recovery,\n                  wm_thai$nb,\n                  wm_thai$wt,\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.27768, observed rank = 100, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\n\n\n\nWe see similar results here. The results show no evidence to reject spatial independence for the total number of tourists pre- and post-Covid. However, it shows signs of clustering for the number of tourists recovery rate as the p-value is below 0.05 and the statistic is above 1.\n\n\nD.3.3 Global Moran’s Test on Average Tourist Spend\nThe code chunks in the tabs below run the Global Moran’s I permutation test on average tourist spend pre-Covid, post-Covid and the recovery rate.\n\nPre-Covid Average SpendPost-Covid Average SpendAverage Spend Recovery Rate\n\n\n\nglobal_moran_perm(wm_thai$PreCovidSpend_total,\n                  wm_thai$nb,\n                  wm_thai$wt,\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.423, observed rank = 100, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\n\n\n\nglobal_moran_perm(wm_thai$PostCovidSpend_total,\n                  wm_thai$nb,\n                  wm_thai$wt,\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.20651, observed rank = 100, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\n\n\n\nglobal_moran_perm(wm_thai$Spend_total_recovery,\n                  wm_thai$nb,\n                  wm_thai$wt,\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.31497, observed rank = 100, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\n\n\n\nAverage tourist spending is showing signs of clustering on all dimensions (pre-Covid, post-Covid and the recovery rate) as the p-value is below 0.05 and the I statistic is above 0 in all cases.\n\n\nD.3.3 Global Moran’s Test Summary\nBased on the results of the testing on the total revenue, number of tourists and spend, we see that the following variables are not exhibiting a random distribution, and show signs of clustering:\n\nTotal tourism revenue recovery rate\nTotal number of tourists recovery rate\nPre-Covid Average spend per Tourist\nPost-Covid Average spend per Tourist\nAverage spend per tourist recovery rate\n\nWe will conduct tests for local association to identify the clusters and outliers among provinces for each of these variables."
  },
  {
    "objectID": "posts/thai-tourism-covid/index.html#e.1-analysing-lisa-total-tourism-recovery-rate",
    "href": "posts/thai-tourism-covid/index.html#e.1-analysing-lisa-total-tourism-recovery-rate",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "E.1 Analysing LISA: Total tourism recovery rate",
    "text": "E.1 Analysing LISA: Total tourism recovery rate\nWe first compute for the LISA for the recovery rate of the total number of tourists using local_moran() function in the code chunk below. The code below uses 100 simulations to produce the test results.\n\nlisa &lt;- wm_thai %&gt;%\n  mutate(local_moran = local_moran(\n    Revenue_total_recovery, nb, wt, nsim = 99),\n    .before = 1) %&gt;%\n  unnest(local_moran)\n\nWe can then visualize the test statistic and p-values for each province in a map using tmap package in the code chunk below.\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_shape(lisa) +\n  tm_fill(c(\"ii\", \"p_ii_sim\"), title = c(\"Local Moran's I\",\"P Value\")) +\n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(\n    main.title = \"LISA for Total Revenue Recovery Rate\",\n    legend.position = c(\"right\", \"bottom\"),\n    bg.color = \"grey90\")\n\nVariable(s) \"ii\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\n\n\n\n\nOutliers are generally provinces where the test statistic is negative, and clusters where it is positive– if they are significant. We see some potential outliers. We can produce a different set of plots to allow us to identify these types of provinces.\nUsing a LISA map, we can show graphically the location of clusters and outliers based on this (tourism revenue recovery rate)\n\nlisa_sig &lt;- lisa %&gt;%\n  filter(p_ii_sim &lt; 0.05)\n\ntm_shape(lisa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig) +\n  tm_fill(\"mean\", title = \"LISA Class α=5% \") +\n  tm_borders(alpha = 0.4) +\n  tm_text(\"Province\", size = 0.5) +\ntm_layout(\n    main.title = \"Revenue Recovery Rate\",\n    legend.position = c(\"right\", \"bottom\"),\n    bg.color = \"beige\",\n    asp = 0)\n\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them).\n\n\n\n\n\n\n\n\n\nWe can also observe the contents of the object lisa_sig to see the statistics for the identified significant provinces. The code chunk below shows each class separately for easier reference.\n\nlisa_sig[lisa_sig$mean == \"Low-Low\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 7 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97.63083 ymin: 6.422716 xmax: 100.3366 ymax: 10.12626\nGeodetic CRS:  WGS 84\n# A tibble: 7 × 3\n  Province            mean                                              geometry\n  &lt;chr&gt;               &lt;fct&gt;                                   &lt;MULTIPOLYGON [°]&gt;\n1 Nakhon Si Thammarat Low-Low (((99.77467 9.313729, 99.77478 9.313647, 99.77488…\n2 Krabi               Low-Low (((99.11329 7.489274, 99.11337 7.489274, 99.11343…\n3 Phang Nga           Low-Low (((98.61471 7.74431, 98.61461 7.74431, 98.61437 7…\n4 Phuket              Low-Low (((98.31437 7.478515, 98.31425 7.478502, 98.31417…\n5 Surat Thani         Low-Low (((99.96396 9.309907, 99.96376 9.30955, 99.96353 …\n6 Satun               Low-Low (((100.0903 6.425736, 100.09 6.425543, 100.0896 6…\n7 Trang               Low-Low (((99.47579 6.97262, 99.47565 6.972616, 99.47537 …\n\nlisa_sig[lisa_sig$mean == \"High-High\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 2 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 98.984 ymin: 14.94191 xmax: 101.3582 ymax: 19.63808\nGeodetic CRS:  WGS 84\n# A tibble: 2 × 3\n  Province    mean                                                      geometry\n  &lt;chr&gt;       &lt;fct&gt;                                           &lt;MULTIPOLYGON [°]&gt;\n1 Nan         High-High (((100.8948 19.63432, 100.8952 19.63431, 100.8957 19.63…\n2 Uthai Thani High-High (((99.13905 15.79655, 99.13918 15.79652, 99.13965 15.79…\n\nlisa_sig[lisa_sig$mean == \"High-Low\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 0 features and 2 fields\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nGeodetic CRS:  WGS 84\n# A tibble: 0 × 3\n# ℹ 3 variables: Province &lt;chr&gt;, mean &lt;fct&gt;, geometry &lt;GEOMETRY [°]&gt;\n\nlisa_sig[lisa_sig$mean == \"Low-High\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 0 features and 2 fields\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nGeodetic CRS:  WGS 84\n# A tibble: 0 × 3\n# ℹ 3 variables: Province &lt;chr&gt;, mean &lt;fct&gt;, geometry &lt;GEOMETRY [°]&gt;\n\n\nWe can summarize the findings from this analysis as:\n\nThere is a cluster of 7 provinces at the south of Thailand that have slower recovery in terms of their average tourism revenue. This includes popular destinations like Phuket and Krabi and their neighboring provinces\nThere is a cluster of 3 provinces in the north that have faster recovery on the same metric. This includes Chiang Rai, Nan and Phayao\nThe analysis revealed two outliers. Chachoengsao has high recovery while neighboring provinces are low. Nakhon Ratchasima has low recovery while neighboring provinces are high\n\nreveal the following"
  },
  {
    "objectID": "posts/thai-tourism-covid/index.html#e.2-analysing-lisa-number-of-tourists-recovery-rate",
    "href": "posts/thai-tourism-covid/index.html#e.2-analysing-lisa-number-of-tourists-recovery-rate",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "E.2 Analysing LISA: Number of tourists recovery rate",
    "text": "E.2 Analysing LISA: Number of tourists recovery rate\nWe compute for the LISA using local_moran() function for the number of tourist recovery rate in the code chunk below.\n\nlisa &lt;- wm_thai %&gt;%\n  mutate(local_moran = local_moran(\n    Tourists_total_recovery, nb, wt, nsim = 99),\n    .before = 1) %&gt;%\n  unnest(local_moran)\n\nWe will go straight to producing the LISA map based on the lisa object. Again, we use 0.05 to determine statistical significance.\n\nlisa_sig &lt;- lisa %&gt;%\n  filter(p_ii_sim &lt; 0.05)\n\ntm_shape(lisa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig) +\n  tm_fill(\"mean\", title = \"LISA Class α=5% \") +\n  tm_borders(alpha = 0.4) +\n  tm_text(\"Province\", size = 0.5) +\ntm_layout(\n    main.title = \"No of Tourist Recovery Rate\",\n    legend.position = c(\"right\", \"bottom\"),\n    bg.color = \"beige\",\n    asp = 0)\n\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them).\n\n\n\n\n\n\n\n\n\nTo aid interpretation, we display results tabularly as before using the code chunk below.\n\nlisa_sig[lisa_sig$mean == \"Low-Low\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 5 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97.63083 ymin: 7.467277 xmax: 100.3366 ymax: 10.12626\nGeodetic CRS:  WGS 84\n# A tibble: 5 × 3\n  Province            mean                                              geometry\n  &lt;chr&gt;               &lt;fct&gt;                                   &lt;MULTIPOLYGON [°]&gt;\n1 Nakhon Si Thammarat Low-Low (((99.77467 9.313729, 99.77478 9.313647, 99.77488…\n2 Krabi               Low-Low (((99.11329 7.489274, 99.11337 7.489274, 99.11343…\n3 Phang Nga           Low-Low (((98.61471 7.74431, 98.61461 7.74431, 98.61437 7…\n4 Phuket              Low-Low (((98.31437 7.478515, 98.31425 7.478502, 98.31417…\n5 Surat Thani         Low-Low (((99.96396 9.309907, 99.96376 9.30955, 99.96353 …\n\nlisa_sig[lisa_sig$mean == \"High-High\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 0 features and 2 fields\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nGeodetic CRS:  WGS 84\n# A tibble: 0 × 3\n# ℹ 3 variables: Province &lt;chr&gt;, mean &lt;fct&gt;, geometry &lt;GEOMETRY [°]&gt;\n\nlisa_sig[lisa_sig$mean == \"High-Low\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 2 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 99.15364 ymin: 6.422716 xmax: 101.9901 ymax: 13.9767\nGeodetic CRS:  WGS 84\n# A tibble: 2 × 3\n  Province     mean                                                     geometry\n  &lt;chr&gt;        &lt;fct&gt;                                          &lt;MULTIPOLYGON [°]&gt;\n1 Chachoengsao High-Low (((101.0612 13.97613, 101.0625 13.976, 101.0629 13.9760…\n2 Satun        High-Low (((100.0903 6.425736, 100.09 6.425543, 100.0896 6.42572…\n\nlisa_sig[lisa_sig$mean == \"Low-High\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 0 features and 2 fields\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nGeodetic CRS:  WGS 84\n# A tibble: 0 × 3\n# ℹ 3 variables: Province &lt;chr&gt;, mean &lt;fct&gt;, geometry &lt;GEOMETRY [°]&gt;\n\n\nThe findings can be summarized as:\n\nThere is a cluster at the south of five provinces with slower recovery with regards to the number of tourists– including Phuket and Krabi (similar to previous metric)\nSatun at the southern part has high recovery rate while its neighboring provinces are lower"
  },
  {
    "objectID": "posts/thai-tourism-covid/index.html#e.3-analysing-lisa-pre-covid-average-spend",
    "href": "posts/thai-tourism-covid/index.html#e.3-analysing-lisa-pre-covid-average-spend",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "E.3 Analysing LISA: Pre-Covid Average Spend",
    "text": "E.3 Analysing LISA: Pre-Covid Average Spend\nWe compute for the LISA using local_moran() function for the average tourist spending post-Covid in the code chunk below.\n\nlisa &lt;- wm_thai %&gt;%\n  mutate(local_moran = local_moran(\n    PreCovidSpend_total, nb, wt, nsim = 99),\n    .before = 1) %&gt;%\n  unnest(local_moran)\n\nWe will now produce the LISA map based on the generated lisa object. Again, we use 0.05 to determine statistical significance.\n\nlisa_sig &lt;- lisa %&gt;%\n  filter(p_ii_sim &lt; 0.05)\n\ntm_shape(lisa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig) +\n  tm_fill(\"mean\", title = \"LISA Class α=5% \") +\n  tm_borders(alpha = 0.4) +\n  tm_text(\"Province\", size = 0.5) +\ntm_layout(\n    main.title = \"Pre-Covid Average Tourist Spend\",\n    legend.position = c(\"right\", \"bottom\"),\n    bg.color = \"beige\",\n    asp = 0)\n\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them).\n\n\n\n\n\n\n\n\n\nWe display the results tabularly using the code chunk below to make it easier to interpret.\n\nlisa_sig[lisa_sig$mean == \"Low-Low\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 10 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 99.08612 ymin: 14.06424 xmax: 105.637 ymax: 18.22037\nGeodetic CRS:  WGS 84\n# A tibble: 10 × 3\n   Province         mean                                                geometry\n   &lt;chr&gt;            &lt;fct&gt;                                     &lt;MULTIPOLYGON [°]&gt;\n 1 Lopburi          Low-Low (((101.3453 15.75254, 101.3457 15.75224, 101.3466 1…\n 2 Sing Buri        Low-Low (((100.3691 15.0894, 100.3697 15.0891, 100.3708 15.…\n 3 Chainat          Low-Low (((100.1199 15.41243, 100.121 15.41234, 100.1229 15…\n 4 Ubon Ratchathani Low-Low (((105.0633 16.09675, 105.0634 16.09671, 105.0638 1…\n 5 Yasothon         Low-Low (((104.3952 16.34843, 104.3983 16.34707, 104.4 16.3…\n 6 Khon Kaen        Low-Low (((102.7072 17.08713, 102.708 17.087, 102.7096 17.0…\n 7 Loei             Low-Low (((102.095 18.21708, 102.0962 18.21675, 102.0971 18…\n 8 Roi Et           Low-Low (((104.314 16.43758, 104.3135 16.43452, 104.3137 16…\n 9 Nakhon Sawan     Low-Low (((100.0266 16.189, 100.0267 16.18889, 100.0268 16.…\n10 Suphan Buri      Low-Low (((99.37118 15.05073, 99.37454 15.0495, 99.3762 15.…\n\nlisa_sig[lisa_sig$mean == \"High-High\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 3 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97.63083 ymin: 7.467277 xmax: 99.41499 ymax: 9.478956\nGeodetic CRS:  WGS 84\n# A tibble: 3 × 3\n  Province  mean                                                        geometry\n  &lt;chr&gt;     &lt;fct&gt;                                             &lt;MULTIPOLYGON [°]&gt;\n1 Krabi     High-High (((99.11329 7.489274, 99.11337 7.489274, 99.11343 7.48929…\n2 Phang Nga High-High (((98.61471 7.74431, 98.61461 7.74431, 98.61437 7.744467,…\n3 Phuket    High-High (((98.31437 7.478515, 98.31425 7.478502, 98.31417 7.47851…\n\nlisa_sig[lisa_sig$mean == \"High-Low\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 0 features and 2 fields\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nGeodetic CRS:  WGS 84\n# A tibble: 0 × 3\n# ℹ 3 variables: Province &lt;chr&gt;, mean &lt;fct&gt;, geometry &lt;GEOMETRY [°]&gt;\n\nlisa_sig[lisa_sig$mean == \"Low-High\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 0 features and 2 fields\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nGeodetic CRS:  WGS 84\n# A tibble: 0 × 3\n# ℹ 3 variables: Province &lt;chr&gt;, mean &lt;fct&gt;, geometry &lt;GEOMETRY [°]&gt;\n\n\nThe findings can be summarized as:\n\nThere are multiple clusters, totaling 10 provinces, in the center of Thailand that have low average spending pre-Covid. These are composed of lesser known tourist destinations\nPhuket and Phang Nga make up a two-province cluster with high average tourist spending\nThere are no outliers identified in the analysis"
  },
  {
    "objectID": "posts/thai-tourism-covid/index.html#e.4-analysing-lisa-post-covid-average-spend",
    "href": "posts/thai-tourism-covid/index.html#e.4-analysing-lisa-post-covid-average-spend",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "E.4 Analysing LISA: Post-Covid Average Spend",
    "text": "E.4 Analysing LISA: Post-Covid Average Spend\nWe compute for the LISA using local_moran() function for the average tourist spending post-Covid in the code chunk below.\n\nlisa &lt;- wm_thai %&gt;%\n  mutate(local_moran = local_moran(\n    PostCovidSpend_total, nb, wt, nsim = 99),\n    .before = 1) %&gt;%\n  unnest(local_moran)\n\nWe will now produce the LISA map based on the generated lisa object. Again, we use 0.05 to determine statistical significance.\n\nlisa_sig &lt;- lisa %&gt;%\n  filter(p_ii_sim &lt; 0.05)\n\ntm_shape(lisa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig) +\n  tm_fill(\"mean\", title = \"LISA Class α=5% \") +\n  tm_borders(alpha = 0.4) +\n  tm_text(\"Province\", size = 0.5) +\ntm_layout(\n    main.title = \"Post-Covid Average Tourist Spend\",\n    legend.position = c(\"right\", \"bottom\"),\n    bg.color = \"beige\",\n    asp = 0)\n\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them).\n\n\n\n\n\n\n\n\n\nWe again display the results tabularly using the code chunk below to make it easier to interpret.\n\nlisa_sig[lisa_sig$mean == \"Low-Low\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 7 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 99.08612 ymin: 14.06424 xmax: 105.637 ymax: 18.22037\nGeodetic CRS:  WGS 84\n# A tibble: 7 × 3\n  Province         mean                                                 geometry\n  &lt;chr&gt;            &lt;fct&gt;                                      &lt;MULTIPOLYGON [°]&gt;\n1 Sing Buri        Low-Low (((100.3691 15.0894, 100.3697 15.0891, 100.3708 15.0…\n2 Ubon Ratchathani Low-Low (((105.0633 16.09675, 105.0634 16.09671, 105.0638 16…\n3 Loei             Low-Low (((102.095 18.21708, 102.0962 18.21675, 102.0971 18.…\n4 Roi Et           Low-Low (((104.314 16.43758, 104.3135 16.43452, 104.3137 16.…\n5 Mukdahan         Low-Low (((104.2527 16.89302, 104.2527 16.89274, 104.2527 16…\n6 Nakhon Sawan     Low-Low (((100.0266 16.189, 100.0267 16.18889, 100.0268 16.1…\n7 Suphan Buri      Low-Low (((99.37118 15.05073, 99.37454 15.0495, 99.3762 15.0…\n\nlisa_sig[lisa_sig$mean == \"High-High\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 3 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97.63083 ymin: 7.467277 xmax: 99.41499 ymax: 9.478956\nGeodetic CRS:  WGS 84\n# A tibble: 3 × 3\n  Province  mean                                                        geometry\n  &lt;chr&gt;     &lt;fct&gt;                                             &lt;MULTIPOLYGON [°]&gt;\n1 Krabi     High-High (((99.11329 7.489274, 99.11337 7.489274, 99.11343 7.48929…\n2 Phang Nga High-High (((98.61471 7.74431, 98.61461 7.74431, 98.61437 7.744467,…\n3 Phuket    High-High (((98.31437 7.478515, 98.31425 7.478502, 98.31417 7.47851…\n\nlisa_sig[lisa_sig$mean == \"High-Low\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 0 features and 2 fields\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nGeodetic CRS:  WGS 84\n# A tibble: 0 × 3\n# ℹ 3 variables: Province &lt;chr&gt;, mean &lt;fct&gt;, geometry &lt;GEOMETRY [°]&gt;\n\nlisa_sig[lisa_sig$mean == \"Low-High\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 0 features and 2 fields\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nGeodetic CRS:  WGS 84\n# A tibble: 0 × 3\n# ℹ 3 variables: Province &lt;chr&gt;, mean &lt;fct&gt;, geometry &lt;GEOMETRY [°]&gt;\n\n\nThe results very closely resember the ones for Pre-Covid average spending. One significant change is that the high spend cluster at the south now includes Krabi. (so it now consists of three provinces)"
  },
  {
    "objectID": "posts/thai-tourism-covid/index.html#e.5-analysing-lisa-average-spend-recovery-rate",
    "href": "posts/thai-tourism-covid/index.html#e.5-analysing-lisa-average-spend-recovery-rate",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "E.5 Analysing LISA: Average Spend Recovery Rate",
    "text": "E.5 Analysing LISA: Average Spend Recovery Rate\nWe compute for the LISA using local_moran() function for the average tourist spending recovery rate in the code chunk below.\n\nlisa &lt;- wm_thai %&gt;%\n  mutate(local_moran = local_moran(\n    Spend_total_recovery, nb, wt, nsim = 99),\n    .before = 1) %&gt;%\n  unnest(local_moran)\n\nWe will now produce the LISA map based on the generated lisa object. Again, we use 0.05 to determine statistical significance.\n\nlisa_sig &lt;- lisa %&gt;%\n  filter(p_ii_sim &lt; 0.05)\n\ntm_shape(lisa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig) +\n  tm_fill(\"mean\", title = \"LISA Class α=5% \") +\n  tm_borders(alpha = 0.4) +\n  tm_text(\"Province\", size = 0.5) +\ntm_layout(\n    main.title = \"Average Tourist Spend Recovery Rate\",\n    legend.position = c(\"right\", \"bottom\"),\n    bg.color = \"beige\",\n    asp = 0)\n\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them).\n\n\n\n\n\n\n\n\n\nWe again display the results tabularly using the code chunk below to make it easier to interpret.\n\nlisa_sig[lisa_sig$mean == \"Low-Low\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 6 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 98.32594 ymin: 5.613038 xmax: 101.7248 ymax: 10.78906\nGeodetic CRS:  WGS 84\n# A tibble: 6 × 3\n  Province            mean                                              geometry\n  &lt;chr&gt;               &lt;fct&gt;                                   &lt;MULTIPOLYGON [°]&gt;\n1 Nakhon Si Thammarat Low-Low (((99.77467 9.313729, 99.77478 9.313647, 99.77488…\n2 Surat Thani         Low-Low (((99.96396 9.309907, 99.96376 9.30955, 99.96353 …\n3 Ranong              Low-Low (((98.35294 9.440758, 98.35316 9.440558, 98.3533 …\n4 Trang               Low-Low (((99.47579 6.97262, 99.47565 6.972616, 99.47537 …\n5 Pattani             Low-Low (((101.2827 6.952051, 101.2839 6.95182, 101.2848 …\n6 Yala                Low-Low (((101.2927 6.681118, 101.2937 6.679529, 101.2939…\n\nlisa_sig[lisa_sig$mean == \"High-High\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 2 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 99.01629 ymin: 15.3183 xmax: 101.7972 ymax: 17.178\nGeodetic CRS:  WGS 84\n# A tibble: 2 × 3\n  Province       mean                                                   geometry\n  &lt;chr&gt;          &lt;fct&gt;                                        &lt;MULTIPOLYGON [°]&gt;\n1 Kamphaeng Phet High-High (((99.48875 16.91044, 99.48883 16.91016, 99.48884 16…\n2 Phetchabun     High-High (((101.3987 17.17792, 101.399 17.17781, 101.3993 17.…\n\nlisa_sig[lisa_sig$mean == \"High-Low\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 1 feature and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 98.25791 ymin: 7.478502 xmax: 98.48333 ymax: 8.200333\nGeodetic CRS:  WGS 84\n# A tibble: 1 × 3\n  Province mean                                                         geometry\n  &lt;chr&gt;    &lt;fct&gt;                                              &lt;MULTIPOLYGON [°]&gt;\n1 Phuket   High-Low (((98.31437 7.478515, 98.31425 7.478502, 98.31417 7.478513,…\n\nlisa_sig[lisa_sig$mean == \"Low-High\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 1 feature and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 100.42 ymin: 14.64684 xmax: 101.4044 ymax: 15.75613\nGeodetic CRS:  WGS 84\n# A tibble: 1 × 3\n  Province mean                                                         geometry\n  &lt;chr&gt;    &lt;fct&gt;                                              &lt;MULTIPOLYGON [°]&gt;\n1 Lopburi  Low-High (((101.3453 15.75254, 101.3457 15.75224, 101.3466 15.75236,…\n\n\nThe findings can be summarized as:\n\nThere are two clusters at the south, totaling 5 provinces, that have slow recovery. This includes the provinces Surat Thani, Nakhon Si Thammarat, Trang, Pattani and Yala\nThere is a cluster of three provinces at the center that have high recovery rate. This includes Kanpaeng Phet, Phichit and Phetchabun\nPhuket is appearing as an outlier with high tourist spend recovery rate relative to its neighbors."
  },
  {
    "objectID": "posts/thai-tourism-covid/index.html#f.1-ehsa---post-covid-tourism-revenue",
    "href": "posts/thai-tourism-covid/index.html#f.1-ehsa---post-covid-tourism-revenue",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "F.1 EHSA - Post-covid tourism revenue",
    "text": "F.1 EHSA - Post-covid tourism revenue\nWe perform EHSA on the post-covid (2022 onwards) values for the total tourism revenue.\nWe create post-Covid versions of our datasets using the code chunk below so it is easier to refer to them later. We also include a column for the year and month as integers as EHSA requires discrete numeric values as a time variable.\n\ntourism_postCov &lt;- subset(tourism, Date &gt; as.Date(\"2021-12-31\"))\ntourism_postCov$YYYYMM &lt;- as.integer(format(tourism_postCov$Date, \"%Y%m\"))\n\noccupancy_postCov &lt;- subset(occupancy_df, Date &gt; as.Date(\"2021-12-31\"))\noccupancy_postCov$YYYYMM &lt;- as.integer(format(occupancy_postCov$Date, \"%Y%m\"))\n\n\nF.1.1 Creating spacetime cube\nWe then create the space-time cube using spacetime() for just the post-covid tourism revenue. We use filter() to only select the rows for the measure of interest, and then select() to limit the original dataframe to only the date, province and variable.\n\nstcube &lt;- spacetime(select(filter(tourism_postCov, tourism_postCov$Indicator == \"revenue_all\"),\n                           YYYYMM, Province, Value),\n                    thai_sf,\n                    .loc_col = \"Province\",\n                    .time_col = \"YYYYMM\")\n\nTo check that the object is in the correct format, we can use is_spacetime_cube() and confirm that it returns TRUE\n\nis_spacetime_cube(stcube)\n\n[1] TRUE\n\n\n\n\nF.1.2 Calculating \\(G_i^*\\) statistics\nBefore computing for the \\(G_i^*\\) statistics, we need to derive the spatial weights. We use the code chunk below to identify the neighbors and to derive inverse distance weights. The alpha argument of st_inverse_distance() determines the level of distance decay.\n\nstcube_nb &lt;- stcube %&gt;%\n  activate(\"geometry\") %&gt;%\n  mutate(nb = include_self(\n    st_contiguity(geometry)),\n    wt=st_inverse_distance(nb,\n                  geometry,\n                  scale = 1,\n                  alpha = 1),\n    .before = 1) %&gt;%\n  set_nbs(\"nb\") %&gt;%\n  set_wts(\"wt\")\n\n! Polygon provided. Using point on surface.\n\n\nWarning: There was 1 warning in `stopifnot()`.\nℹ In argument: `wt = st_inverse_distance(nb, geometry, scale = 1, alpha = 1)`.\nCaused by warning in `st_point_on_surface.sfc()`:\n! st_point_on_surface may not give correct results for longitude/latitude data\n\n\nWe then use the following chunk to calculate the local \\(G_i^*\\) for each location. We do this using local_gstar_perm() of sfdep package. We then use unnest() to unnest the gi_star column of the newly created data frame.\n\ngi_stars &lt;- stcube_nb %&gt;%\n  group_by(YYYYMM) %&gt;%\n  mutate(gi_star = local_gstar_perm(\n    Value, nb, wt)) %&gt;%\n  tidyr::unnest(gi_star)\n\n\n\nF.1.3 Identifying Hot- and Cold-spots using Mann Kendall test\nThe Mann Kendall test checks for signs of monotonicity for a the local \\(G_i^*\\) statistic. Where the results are significant, the test infers that there are signs of monotonicity for that province / observation.\nThe code chunk below runs the Mann Kendall test (without permutations) on each province for the selected variable using MannKendall(). Provinces where the tau value is positive are showing an increasing trend, while negative tau values show decreasing trend.\nWe display the records with significant test results, and show negative and positive tau values as separate tables.\n\nehsa &lt;- gi_stars %&gt;%\n  group_by(Province) %&gt;%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %&gt;%\n  tidyr::unnest_wider(mk)\n\nfilter(ehsa, sl &lt; 0.05, tau &gt; 0)\n\n# A tibble: 33 × 6\n   Province         tau         sl     S     D  varS\n   &lt;chr&gt;          &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Amnat Charoen  0.956 0.00000250    87  91.0  334.\n 2 Bangkok        0.473 0.0215        43  91.0  334.\n 3 Bueng Kan      0.626 0.00217       57  91.0  334.\n 4 Buriram        0.758 0.000197      69  91.0  334.\n 5 Chaiyaphum     0.780 0.000127      71  91.0  334.\n 6 Chanthaburi    0.626 0.00217       57  91.0  334.\n 7 Kalasin        0.802 0.0000809     73  91.0  334.\n 8 Kamphaeng Phet 0.890 0.0000119     81  91.0  334.\n 9 Lampang        0.626 0.00217       57  91.0  334.\n10 Lamphun        0.956 0.00000250    87  91.0  334.\n# ℹ 23 more rows\n\nfilter(ehsa, sl &lt; 0.05, tau &lt; 0)\n\n# A tibble: 21 × 6\n   Province        tau        sl     S     D  varS\n   &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Ang Thong    -0.582 0.00442     -53  91.0  334.\n 2 Chachoengsao -0.802 0.0000809   -73  91.0  334.\n 3 Chainat      -0.626 0.00217     -57  91.0  334.\n 4 Chiang Rai   -0.429 0.0375      -39  91.0  334.\n 5 Chumphon     -0.890 0.0000119   -81  91.0  334.\n 6 Kanchanaburi -0.890 0.0000119   -81  91.0  334.\n 7 Mae Hong Son -0.429 0.0375      -39  91.0  334.\n 8 Mukdahan     -0.473 0.0215      -43  91.0  334.\n 9 Phetchabun   -0.824 0.0000510   -75  91.0  334.\n10 Phetchaburi  -0.802 0.0000809   -73  91.0  334.\n# ℹ 11 more rows\n\n\nThe results show that 33 of the 77 provinces are showing significant positive trend– which might be expected as provinces are recovering post-Covid. However, there are 21 provinces which are showing a significant negative trend which might be a concern if any of these are expected to be major tourist destinations.\n\n\nF.1.4 Emerging Hot Spot Analysis\nWe can use emerging_hot_spot_analysis() to perform EHSA, including the previous hypothesis testing, and classify provinces based on the test results.\nThe code chunk below uses emerging_hot_spot_analysis() directly on the spacetime cube to run the test with 100 permutations.\n\nehsa &lt;- emerging_hotspot_analysis(\n  x = stcube,\n  .var = \"Value\",\n  k = 1,\n  nsim = 99\n)\n\nWe can visualize the number of provinces per category using the code chunk below which utilizes ggplot() to build a bar chart.\n\nggplot(data = ehsa,\n       aes(x = classification)) +\n  geom_bar(fill = \"lightblue\") +\n  labs(title = \"EHSA Classification - PostCovid Tourism Revenue\",\n       y = \"Number of Provinces\", x=\"\") +\n  theme_minimal() +\n  geom_text(stat = 'count', aes(label = ..count..), vjust = 0) +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(count)` instead.\n\n\n\n\n\n\n\n\n\nWe can also visualize the hot- and coldspot classifications visually. First, we need to combine the ehsa object with the sf dataframe with the code chunk below.\n\nthai_ehsa &lt;- thai_sf %&gt;%\n  left_join(ehsa,\n            by = join_by(Province == location))\n\nNext, we use tmap package to create a choropleth map based on the EHSA classification. We only include provinces that have significant test results and a pattern detected.\n\nehsa_sig &lt;- filter(thai_ehsa, !(classification == \"no pattern detected\"), p_value &lt; 0.05)\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_shape(thai_ehsa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(ehsa_sig) +\n  tm_fill(\"classification\", title = \"EHSA Classification\") +\n  tm_text(\"Province\", size = 0.5) +\n  tm_borders(alpha = 0.4) +\ntm_layout(\n    main.title = \"Post Covid Tourism Revenue\",\n    legend.position = c(\"right\", \"bottom\"),\n    bg.color = \"beige\",\n    asp = 0)\n\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them).\n\n\n\n\n\n\n\n\n\nWe can display the same provinces tabularly using the code chunk below\n\nfor (i in unique(ehsa_sig$classification)){\nprint(as.matrix(ehsa_sig[ehsa_sig$classification == i, c(\"Province\", \"classification\")]))\n}\n\n  Province  classification         geometry                      \n1 \"Bangkok\" \"intensifying hotspot\" MULTIPOLYGON (((100.6139 13...\n   Province        classification        geometry                      \n2  \"Nonthaburi\"    \"consecutive hotspot\" MULTIPOLYGON (((100.3415 14...\n8  \"Khon Kaen\"     \"consecutive hotspot\" MULTIPOLYGON (((102.7072 17...\n10 \"Sakon Nakhon\"  \"consecutive hotspot\" MULTIPOLYGON (((103.5404 18...\n11 \"Nakhon Phanom\" \"consecutive hotspot\" MULTIPOLYGON (((104.192 18....\n15 \"Kanchanaburi\"  \"consecutive hotspot\" MULTIPOLYGON (((98.58631 15...\n   Province                   classification     \n3  \"Phra Nakhon Si Ayutthaya\" \"sporadic coldspot\"\n4  \"Sing Buri\"                \"sporadic coldspot\"\n5  \"Trat\"                     \"sporadic coldspot\"\n6  \"Chachoengsao\"             \"sporadic coldspot\"\n7  \"Buri Ram\"                 \"sporadic coldspot\"\n9  \"Nong Khai\"                \"sporadic coldspot\"\n12 \"Mukdahan\"                 \"sporadic coldspot\"\n13 \"Lamphun\"                  \"sporadic coldspot\"\n14 \"Lampang\"                  \"sporadic coldspot\"\n16 \"Prachuap Khiri Khan\"      \"sporadic coldspot\"\n17 \"Ranong\"                   \"sporadic coldspot\"\n18 \"Phatthalung\"              \"sporadic coldspot\"\n19 \"Pattani\"                  \"sporadic coldspot\"\n   geometry                      \n3  MULTIPOLYGON (((100.5131 14...\n4  MULTIPOLYGON (((100.3691 15...\n5  MULTIPOLYGON (((102.5216 11...\n6  MULTIPOLYGON (((101.0612 13...\n7  MULTIPOLYGON (((102.9303 15...\n9  MULTIPOLYGON (((103.2985 18...\n12 MULTIPOLYGON (((104.2527 16...\n13 MULTIPOLYGON (((99.18821 18...\n14 MULTIPOLYGON (((99.58445 19...\n16 MULTIPOLYGON (((99.56326 11...\n17 MULTIPOLYGON (((98.35294 9....\n18 MULTIPOLYGON (((99.96416 7....\n19 MULTIPOLYGON (((101.2827 6....\n\n\nThe results identifies Bangkok as an intensifying hotspot, and six others as consecutive hotspots.\nThere are 13 provinces identified as sporadic coldspots which are locations that are cold spots for less than 90% of the time, but never identified as significant hotspots."
  },
  {
    "objectID": "posts/thai-tourism-covid/index.html#f.2-ehsa---post-covid-number-of-tourists",
    "href": "posts/thai-tourism-covid/index.html#f.2-ehsa---post-covid-number-of-tourists",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "F.2 EHSA - Post-covid number of tourists",
    "text": "F.2 EHSA - Post-covid number of tourists\nWe perform EHSA on the post-covid (2022 onwards) values for the total number of tourists.\n\nF.2.1 Creating spacetime cube\nWe then create the space-time cube using spacetime() for just the post-covid number of tourists. We use filter() to only select the rows for the measure of interest, and then select() to limit the original dataframe to only the date, province and variable.\n\nstcube &lt;- spacetime(select(filter(tourism_postCov, tourism_postCov$Indicator == \"no_tourist_all\"),\n                           YYYYMM, Province, Value),\n                    thai_sf,\n                    .loc_col = \"Province\",\n                    .time_col = \"YYYYMM\")\n\nTo check that the object is in the correct format, we can use is_spacetime_cube() and confirm that it returns TRUE\n\nis_spacetime_cube(stcube)\n\n[1] TRUE\n\n\n\n\nF.2.2 Calculating \\(G_i^*\\) statistics\nBefore computing for the \\(G_i^*\\) statistics, we need to derive the spatial weights. We use the code chunk below to identify the neighbors and to derive inverse distance weights. The alpha argument of st_inverse_distance() determines the level of distance decay.\n\nstcube_nb &lt;- stcube %&gt;%\n  activate(\"geometry\") %&gt;%\n  mutate(nb = include_self(\n    st_contiguity(geometry)),\n    wt=st_inverse_distance(nb,\n                  geometry,\n                  scale = 1,\n                  alpha = 1),\n    .before = 1) %&gt;%\n  set_nbs(\"nb\") %&gt;%\n  set_wts(\"wt\")\n\nWe then use the following chunk to calculate the local \\(G_i^*\\) for each location. We do this using local_gstar_perm() of sfdep package. We then use unnest() to unnest the gi_star column of the newly created data frame.\n\ngi_stars &lt;- stcube_nb %&gt;%\n  group_by(YYYYMM) %&gt;%\n  mutate(gi_star = local_gstar_perm(\n    Value, nb, wt)) %&gt;%\n  tidyr::unnest(gi_star)\n\n\n\nF.2.3 Identifying Hot- and Cold-spots using Mann Kendall test\nThe code chunk below runs the Mann Kendall test (without permutations) on each province using MannKendall(). Provinces where the tau value is positive are showing an increasing trend, while negative tau values show decreasing trend.\nWe display the records with significant test results, and show negative and positive tau values as separate tables.\n\nehsa &lt;- gi_stars %&gt;%\n  group_by(Province) %&gt;%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %&gt;%\n  tidyr::unnest_wider(mk)\n\nfilter(ehsa, sl &lt; 0.05, tau &gt; 0)\n\n# A tibble: 14 × 6\n   Province                   tau        sl     S     D  varS\n   &lt;chr&gt;                    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Bangkok                  0.648 0.00150      59  91.0  334.\n 2 Krabi                    0.714 0.000459     65  91.0  334.\n 3 Nakhon Si Thammarat      0.890 0.0000119    81  91.0  334.\n 4 Nonthaburi               0.626 0.00217      57  91.0  334.\n 5 Pattani                  0.648 0.00150      59  91.0  334.\n 6 Phatthalung              0.780 0.000127     71  91.0  334.\n 7 Phra Nakhon Si Ayutthaya 0.604 0.00311      55  91.0  334.\n 8 Phuket                   0.670 0.00102      61  91.0  334.\n 9 Prachuap Khiri Khan      0.648 0.00150      59  91.0  334.\n10 Ratchaburi               0.429 0.0375       39  91.0  334.\n11 Samut Prakan             0.473 0.0215       43  91.0  334.\n12 Surat Thani              0.648 0.00150      59  91.0  334.\n13 Tak                      0.692 0.000688     63  91.0  334.\n14 Trang                    0.451 0.0285       41  91.0  334.\n\nfilter(ehsa, sl &lt; 0.05, tau &lt; 0)\n\n# A tibble: 18 × 6\n   Province            tau        sl     S     D  varS\n   &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Chachoengsao     -0.758 0.000197    -69  91.0  334.\n 2 Kanchanaburi     -0.692 0.000688    -63  91.0  334.\n 3 Khon Kaen        -0.604 0.00311     -55  91.0  334.\n 4 Loei             -0.516 0.0118      -47  91.0  334.\n 5 Mukdahan         -0.648 0.00150     -59  91.0  334.\n 6 Nakhon Nayok     -0.824 0.0000510   -75  91.0  334.\n 7 Nakhon Pathom    -0.495 0.0160      -45  91.0  334.\n 8 Nong Khai        -0.648 0.00150     -59  91.0  334.\n 9 Phetchabun       -0.560 0.00620     -51  91.0  334.\n10 Phetchaburi      -0.780 0.000127    -71  91.0  334.\n11 Samut Sakhon     -0.802 0.0000809   -73  91.0  334.\n12 Samut Songkhram  -0.780 0.000127    -71  91.0  334.\n13 Sing Buri        -0.626 0.00217     -57  91.0  334.\n14 Surin            -0.560 0.00620     -51  91.0  334.\n15 Trat             -0.780 0.000127    -71  91.0  334.\n16 Ubon Ratchathani -0.495 0.0160      -45  91.0  334.\n17 Udon Thani       -0.407 0.0487      -37  91.0  334.\n18 Yasothon         -0.758 0.000197    -69  91.0  334.\n\n\nThe results show that 14 of the 77 provinces are showing significant positive trend. However, there are 18 provinces (more) which are showing a significant negative trend.\n\n\nF.2.4 Emerging Hot Spot Analysis\nWe can use emerging_hot_spot_analysis() to perform EHSA, including the previous hypothesis testing, and classify provinces based on the test results.\nThe code chunk below uses emerging_hot_spot_analysis() directly on the spacetime cube to run the test with 100 permutations.\n\nehsa &lt;- emerging_hotspot_analysis(\n  x = stcube,\n  .var = \"Value\",\n  k = 1,\n  nsim = 99\n)\n\nWe can visualize the number of provinces per category using the code chunk below which utilizes ggplot() to build a bar chart.\n\nggplot(data = ehsa,\n       aes(x = classification)) +\n  geom_bar(fill = \"lightblue\") +\n  labs(title = \"EHSA Classification - PostCovid Tourism Revenue\",\n       y = \"Number of Provinces\", x=\"\") +\n  theme_minimal() +\n  geom_text(stat = 'count', aes(label = ..count..), vjust = 0) +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))\n\n\n\n\n\n\n\n\nWe can also visualize the hot- and coldspot classifications visually. First, we need to combine the ehsa object with the sf dataframe with the code chunk below.\n\nthai_ehsa &lt;- thai_sf %&gt;%\n  left_join(ehsa,\n            by = join_by(Province == location))\n\nNext, we use tmap package to create a choropleth map based on the EHSA classification. We only include provinces that have significant test results and a pattern detected.\n\nehsa_sig &lt;- filter(thai_ehsa, !(classification == \"no pattern detected\"), p_value &lt; 0.05)\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_shape(thai_ehsa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(ehsa_sig) +\n  tm_fill(\"classification\", title = \"EHSA Classification\") +\n  tm_text(\"Province\", size = 0.5) +\n  tm_borders(alpha = 0.4) +\ntm_layout(\n    main.title = \"Post Covid Number of Tourists\",\n    legend.position = c(\"right\", \"bottom\"),\n    bg.color = \"beige\",\n    asp = 0)\n\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them).\n\n\n\n\n\n\n\n\n\nWe can display the same provinces tabularly using the code chunk below\n\nfor (i in unique(ehsa_sig$classification)){\nprint(as.matrix(ehsa_sig[ehsa_sig$classification == i, c(\"Province\", \"classification\")]))\n}\n\n  Province       classification     geometry                      \n1 \"Nonthaburi\"   \"sporadic hotspot\" MULTIPOLYGON (((100.3415 14...\n2 \"Sakon Nakhon\" \"sporadic hotspot\" MULTIPOLYGON (((103.5404 18...\n  Province  classification      geometry                      \n3 \"Lamphun\" \"sporadic coldspot\" MULTIPOLYGON (((99.18821 18...\n6 \"Ranong\"  \"sporadic coldspot\" MULTIPOLYGON (((98.35294 9....\n7 \"Yala\"    \"sporadic coldspot\" MULTIPOLYGON (((101.2927 6....\n  Province       classification        geometry                      \n4 \"Nakhon Sawan\" \"consecutive hotspot\" MULTIPOLYGON (((100.0266 16...\n5 \"Phuket\"       \"consecutive hotspot\" MULTIPOLYGON (((98.31437 7....\n\n\nThe results identifies Phuket and Nakhon Sawan as consecutive hotspots which means they had a single uninterrupted run of being significant hotspots, but have been significant hotspot for less than 90% of the time.\nNonthaburi and Sakon Nakhon are sporadic hotspots, while Lamphun, Ranong and Yala are sporadic coldspots which mean that they have been on-and-off as hot and coldspots for the number of tourists."
  },
  {
    "objectID": "posts/thai-tourism-covid/index.html#f.3-ehsa---occupancy-rate",
    "href": "posts/thai-tourism-covid/index.html#f.3-ehsa---occupancy-rate",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "F.3 EHSA - Occupancy Rate",
    "text": "F.3 EHSA - Occupancy Rate\nWe perform EHSA on the post-covid (2022 onwards) values for the tourist occupancy rates.\n\nF.3.1 Creating spacetime cube\nWe then create the space-time cube using spacetime() for just the occupancy rates. We use select() to limit the original dataframe to only the date, province and variable.\n\nstcube &lt;- spacetime(select(occupancy_postCov,\n                           YYYYMM, Province, occupancy),\n                    thai_sf,\n                    .loc_col = \"Province\",\n                    .time_col = \"YYYYMM\")\n\nTo check that the object is in the correct format, we can use is_spacetime_cube() and confirm that it returns TRUE\n\nis_spacetime_cube(stcube)\n\n[1] TRUE\n\n\n\n\nF.3.2 Calculating \\(G_i^*\\) statistics\nBefore computing for the \\(G_i^*\\) statistics, we need to derive the spatial weights. We use the code chunk below to identify the neighbors and to derive inverse distance weights. The alpha argument of st_inverse_distance() determines the level of distance decay.\n\nstcube_nb &lt;- stcube %&gt;%\n  activate(\"geometry\") %&gt;%\n  mutate(nb = include_self(\n    st_contiguity(geometry)),\n    wt=st_inverse_distance(nb,\n                  geometry,\n                  scale = 1,\n                  alpha = 1),\n    .before = 1) %&gt;%\n  set_nbs(\"nb\") %&gt;%\n  set_wts(\"wt\")\n\nWe then use the following chunk to calculate the local \\(G_i^*\\) for each location. We do this using local_gstar_perm() of sfdep package. We then use unnest() to unnest the gi_star column of the newly created data frame.\n\ngi_stars &lt;- stcube_nb %&gt;%\n  group_by(YYYYMM) %&gt;%\n  mutate(gi_star = local_gstar_perm(\n    occupancy, nb, wt)) %&gt;%\n  tidyr::unnest(gi_star)\n\n\n\nF.3.3 Identifying Hot- and Cold-spots using Mann Kendall test\nThe code chunk below runs the Mann Kendall test (without permutations) on each province using MannKendall(). Provinces where the tau value is positive are showing an increasing trend, while negative tau values show decreasing trend.\nWe display the records with significant test results, and show negative and positive tau values as separate tables.\n\nehsa &lt;- gi_stars %&gt;%\n  group_by(Province) %&gt;%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %&gt;%\n  tidyr::unnest_wider(mk)\n\nfilter(ehsa, sl &lt; 0.05, tau &gt; 0)\n\n# A tibble: 17 × 6\n   Province              tau         sl     S     D  varS\n   &lt;chr&gt;               &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Bangkok             0.934 0.00000429    85  91.0  334.\n 2 Buriram             0.604 0.00311       55  91.0  334.\n 3 Chumphon            0.670 0.00102       61  91.0  334.\n 4 Krabi               0.714 0.000459      65  91.0  334.\n 5 Nakhon Si Thammarat 0.824 0.0000510     75  91.0  334.\n 6 Narathiwat          0.626 0.00217       57  91.0  334.\n 7 Nonthaburi          0.714 0.000459      65  91.0  334.\n 8 Phatthalung         0.692 0.000688      63  91.0  334.\n 9 Phuket              0.890 0.0000119     81  91.0  334.\n10 Prachuap Khiri Khan 0.934 0.00000429    85  91.0  334.\n11 Ranong              0.560 0.00620       51  91.0  334.\n12 Ratchaburi          0.604 0.00311       55  91.0  334.\n13 Rayong              0.626 0.00217       57  91.0  334.\n14 Samut Prakan        0.736 0.000302      67  91.0  334.\n15 Songkhla            0.648 0.00150       59  91.0  334.\n16 Surat Thani         0.912 0.00000715    83  91.0  334.\n17 Yala                0.495 0.0160        45  91.0  334.\n\nfilter(ehsa, sl &lt; 0.05, tau &lt; 0)\n\n# A tibble: 23 × 6\n   Province         tau       sl     S     D  varS\n   &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Amnat Charoen -0.780 0.000127   -71  91.0  334.\n 2 Chachoengsao  -0.582 0.00442    -53  91.0  334.\n 3 Chainat       -0.473 0.0215     -43  91.0  334.\n 4 Chaiyaphum    -0.736 0.000302   -67  91.0  334.\n 5 Chanthaburi   -0.429 0.0375     -39  91.0  334.\n 6 Kanchanaburi  -0.582 0.00442    -53  91.0  334.\n 7 Lopburi       -0.451 0.0285     -41  91.0  334.\n 8 Nakhon Nayok  -0.604 0.00311    -55  91.0  334.\n 9 Nakhon Pathom -0.648 0.00150    -59  91.0  334.\n10 Nakhon Sawan  -0.451 0.0285     -41  91.0  334.\n# ℹ 13 more rows\n\n\nThe results show that 17 of the 77 provinces are showing significant positive trend. However, there are 23 provinces (more) which are showing a significant negative trend.\n\n\nF.3.4 Emerging Hot Spot Analysis\nWe can use emerging_hot_spot_analysis() to perform EHSA, including the previous hypothesis testing, and classify provinces based on the test results.\nThe code chunk below uses emerging_hot_spot_analysis() directly on the spacetime cube to run the test with 100 permutations.\n\nehsa &lt;- emerging_hotspot_analysis(\n  x = stcube,\n  .var = \"occupancy\",\n  k = 1,\n  nsim = 99\n)\n\nWe can visualize the number of provinces per category using the code chunk below which utilizes ggplot() to build a bar chart.\n\nggplot(data = ehsa,\n       aes(x = classification)) +\n  geom_bar(fill = \"lightblue\") +\n  labs(title = \"EHSA Classification - PostCovid Tourism Revenue\",\n       y = \"Number of Provinces\", x=\"\") +\n  theme_minimal() +\n  geom_text(stat = 'count', aes(label = ..count..), vjust = 0) +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))\n\n\n\n\n\n\n\n\nWe can also visualize the hot- and coldspot classifications visually. First, we need to combine the ehsa object with the sf dataframe with the code chunk below.\n\nthai_ehsa &lt;- thai_sf %&gt;%\n  left_join(ehsa,\n            by = join_by(Province == location))\n\nNext, we use tmap package to create a choropleth map based on the EHSA classification. We only include provinces that have significant test results and a pattern detected.\n\nehsa_sig &lt;- filter(thai_ehsa, !(classification == \"no pattern detected\"), p_value &lt; 0.05)\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_shape(thai_ehsa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(ehsa_sig) +\n  tm_fill(\"classification\", title = \"EHSA Classification\") +\n  tm_text(\"Province\", size = 0.5) +\n  tm_borders(alpha = 0.4) +\ntm_layout(\n    main.title = \"Post Covid Tourist Occupancy Rate\",\n    legend.position = c(\"right\", \"bottom\"),\n    bg.color = \"beige\",\n    asp = 0)\n\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them).\n\n\n\n\n\n\n\n\n\nWe can display the same provinces tabularly using the code chunk below\n\nfor (i in unique(ehsa_sig$classification)){\nprint(as.matrix(ehsa_sig[ehsa_sig$classification == i, c(\"Province\", \"classification\")]))\n}\n\n   Province                   classification      \n1  \"Nonthaburi\"               \"oscilating hotspot\"\n2  \"Phra Nakhon Si Ayutthaya\" \"oscilating hotspot\"\n5  \"Chanthaburi\"              \"oscilating hotspot\"\n7  \"Sa Kaeo\"                  \"oscilating hotspot\"\n8  \"Loei\"                     \"oscilating hotspot\"\n9  \"Maha Sarakham\"            \"oscilating hotspot\"\n11 \"Sakon Nakhon\"             \"oscilating hotspot\"\n13 \"Lamphun\"                  \"oscilating hotspot\"\n16 \"Nakhon Sawan\"             \"oscilating hotspot\"\n18 \"Kamphaeng Phet\"           \"oscilating hotspot\"\n19 \"Tak\"                      \"oscilating hotspot\"\n20 \"Sukhothai\"                \"oscilating hotspot\"\n21 \"Kanchanaburi\"             \"oscilating hotspot\"\n25 \"Nakhon Si Thammarat\"      \"oscilating hotspot\"\n26 \"Phuket\"                   \"oscilating hotspot\"\n   geometry                      \n1  MULTIPOLYGON (((100.3415 14...\n2  MULTIPOLYGON (((100.5131 14...\n5  MULTIPOLYGON (((102.2517 12...\n7  MULTIPOLYGON (((102.1877 14...\n8  MULTIPOLYGON (((102.095 18....\n9  MULTIPOLYGON (((103.1562 16...\n11 MULTIPOLYGON (((103.5404 18...\n13 MULTIPOLYGON (((99.18821 18...\n16 MULTIPOLYGON (((100.0266 16...\n18 MULTIPOLYGON (((99.48875 16...\n19 MULTIPOLYGON (((97.97318 17...\n20 MULTIPOLYGON (((99.60051 17...\n21 MULTIPOLYGON (((98.58631 15...\n25 MULTIPOLYGON (((99.77467 9....\n26 MULTIPOLYGON (((98.31437 7....\n   Province          classification      geometry                      \n3  \"Sing Buri\"       \"sporadic coldspot\" MULTIPOLYGON (((100.3691 15...\n4  \"Chai Nat\"        \"sporadic coldspot\" MULTIPOLYGON (((100.1199 15...\n10 \"Roi Et\"          \"sporadic coldspot\" MULTIPOLYGON (((104.314 16....\n12 \"Chiang Mai\"      \"sporadic coldspot\" MULTIPOLYGON (((99.52512 20...\n15 \"Phrae\"           \"sporadic coldspot\" MULTIPOLYGON (((100.1597 18...\n17 \"Uthai Thani\"     \"sporadic coldspot\" MULTIPOLYGON (((99.13905 15...\n23 \"Samut Songkhram\" \"sporadic coldspot\" MULTIPOLYGON (((100.0116 13...\n27 \"Surat Thani\"     \"sporadic coldspot\" MULTIPOLYGON (((99.96396 9....\n28 \"Songkhla\"        \"sporadic coldspot\" MULTIPOLYGON (((100.5973 7....\n29 \"Satun\"           \"sporadic coldspot\" MULTIPOLYGON (((100.0903 6....\n31 \"Narathiwat\"      \"sporadic coldspot\" MULTIPOLYGON (((101.6323 6....\n   Province       classification         geometry                      \n6  \"Prachin Buri\" \"consecutive coldspot\" MULTIPOLYGON (((101.4881 14...\n24 \"Phetchaburi\"  \"consecutive coldspot\" MULTIPOLYGON (((99.75869 13...\n   Province        classification        geometry                      \n14 \"Uttaradit\"     \"consecutive hotspot\" MULTIPOLYGON (((101.0924 18...\n22 \"Nakhon Pathom\" \"consecutive hotspot\" MULTIPOLYGON (((100.2231 14...\n30 \"Yala\"          \"consecutive hotspot\" MULTIPOLYGON (((101.2927 6....\n\n\nThe results identify a number of cold and hotspots. If we focus on the coldspots:\n\nwe see that Prachin Buri and Phetchaburi are consecutive coldspots\nA list of 11 provinces, including Chiang Mai, are sporadic coldspots"
  },
  {
    "objectID": "posts/tz-fin-2023/index.html",
    "href": "posts/tz-fin-2023/index.html",
    "title": "Geographically Weighted Modeling of Financial Inclusion in Tanzania",
    "section": "",
    "text": "This was my final project for the course Geospatial Analysis. Here I looked at the information from FinscopeTanzania 2023 to model measures of financial inclusion both globally and geographically. In this project, I used geographically weighted logistics regression to see how the factors linked to financial inclusion vary across districts."
  },
  {
    "objectID": "posts/tz-fin-2023/index.html#a.1-background",
    "href": "posts/tz-fin-2023/index.html#a.1-background",
    "title": "Geographically Weighted Modeling of Financial Inclusion in Tanzania",
    "section": "A.1 Background",
    "text": "A.1 Background\nThe World Bank defines financial inclusion as the state of having access to useful and affordable financial products to meet one’s needs. Financial inclusion is an enabler to 7 of the Sustainable Development Goals, and is seen as the key enabler to reduce extreme poverty.\nOne key dimension of financial inclusion that the World Bank looked at in their latest Global Findex Database 2021 is the ownership of bank accounts for adults. In this report, 76% of the global adult population have their own accounts, but only 71% of the developing nations’ do. In some countries like Tanzania this number is even lower at 52%. Banking is just one traditional dimension. Other vehicles like mobile payments can bridge the gap in access to services for some of these nations.\nTanzania recognizes the importance of financial inclusion in promoting economic growth and with the Bank of Tanzania, the country’s central bank, the Microfinance Policy of 2000 was developed and focused on expanding financial services for low-income individuals.\nThe program behind financial inclusion has been structured from 2014 with the first National Financial Inclusion Framework for 2014-2016, with the latest version being the third framework for 2023-2028. While there has been significant progress, (e.g., access to financial services has risen from 42% in 2013 to 89% in 2023) the country continues to aim for inclusion for the whole population by increasing access, encouraging usage and enhancing the quality of financial services."
  },
  {
    "objectID": "posts/tz-fin-2023/index.html#a.2-objectives",
    "href": "posts/tz-fin-2023/index.html#a.2-objectives",
    "title": "Geographically Weighted Modeling of Financial Inclusion in Tanzania",
    "section": "A.2 Objectives",
    "text": "A.2 Objectives\nFinscope Tanzania 2023 is a public-private sector collaboration and aimed, among others, to understand and describe the financial behavior of individuals in the country and to establish an updated view of the level of financial inclusion across various measures. A large part of the findings is showing the change (improvements) of the overall measures against the previous 2017 report.\nThe objective of this study is to build on the Finscope Tanzania 2023 by identifying influential variables and identifying if geospatial factors influence the effect of those variables.\nIn order to satisfy this, the specific deliverables for the study will be:\n\nto build a global or non-spatial explanatory model for the level of financial inclusion across Tanzania;\nto build a geographically weighted explanatory model for the same response variables; and,\nto assess the advantage of the geographically weighted model and to analyse the geographically weighted model"
  },
  {
    "objectID": "posts/tz-fin-2023/index.html#a.3-data-sources",
    "href": "posts/tz-fin-2023/index.html#a.3-data-sources",
    "title": "Geographically Weighted Modeling of Financial Inclusion in Tanzania",
    "section": "A.3 Data Sources",
    "text": "A.3 Data Sources\nThe following data sources are used for this analysis:\n\nFinscope Tanzania 2023 individual survey data from Finscope Tanzania\n\nThe dataset is contained in a csv and translates the responses from 9,915 individuals who answered the survey\nThe respondents are all adults aged 16 years and above take across Tanzania\nThe dataset also includes derived fields which include different indicators for financial inclusion based on different criteria\n\nDistrict-level boundaries in Tanzania as a shapefile from geoBoundaries.org portal"
  },
  {
    "objectID": "posts/tz-fin-2023/index.html#a.4-importing-and-launching-r-packages",
    "href": "posts/tz-fin-2023/index.html#a.4-importing-and-launching-r-packages",
    "title": "Geographically Weighted Modeling of Financial Inclusion in Tanzania",
    "section": "A.4 Importing and Launching R Packages",
    "text": "A.4 Importing and Launching R Packages\nFor this study, the following R packages will be used. A description of the packages and the code, using p_load() of the pacman package, to import them is given below.\n\nPackage DescriptionImport Code\n\n\nThe loaded packages include:\n\nolsrr - for building OLS (ordinary least squares) regression models and performing diagnostic tests\nGWmodel - for calibrating geographically weighted family of models\ntmap - for plotting cartographic quality maps\nggstatsplot - for multivariate data visualization and analysis\nsf - spatial data handling\ntidyverse - attribute data handling\n\n\n\n\npacman::p_load(olsrr, sf, GWmodel, tmap, tidyverse, ggstatsplot, sfdep)"
  },
  {
    "objectID": "posts/tz-fin-2023/index.html#b.1-loading-tanzania-district-boundaries",
    "href": "posts/tz-fin-2023/index.html#b.1-loading-tanzania-district-boundaries",
    "title": "Geographically Weighted Modeling of Financial Inclusion in Tanzania",
    "section": "B.1 Loading Tanzania District boundaries",
    "text": "B.1 Loading Tanzania District boundaries\nWe load the district level boundaries in the following code chunk using st_read() and indicating the appropriate layer name. (i.e., the level 2 map) We also use rename() to already change the shapeName field to district to make it more understandable. We also project the map onto EPSG 32737 using st_transform() in order to be able to reference distances in terms of metres.\n\ntz_dist &lt;- st_read(dsn=\"data/geospatial\", \n                   layer=\"geoBoundaries-TZA-ADM2\") %&gt;%\n  rename(district = shapeName) %&gt;%\n  st_transform(32737)\n\nReading layer `geoBoundaries-TZA-ADM2' from data source \n  `C:\\drkrodriguez\\datawithderek\\posts\\tz-fin-2023\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 170 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 29.58953 ymin: -11.76235 xmax: 40.44473 ymax: -0.983143\nGeodetic CRS:  WGS 84\n\n\n\ntz_dist\n\nSimple feature collection with 170 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -548018.9 ymin: 8698528 xmax: 658181.6 ymax: 9890194\nProjected CRS: WGS 84 / UTM zone 37S\nFirst 10 features:\n       district shapeISO                 shapeID shapeGroup shapeType\n1        Arusha     &lt;NA&gt; 72390352B32479700182608        TZA      ADM2\n2  Arusha Urban     &lt;NA&gt; 72390352B90906351205470        TZA      ADM2\n3        Karatu     &lt;NA&gt; 72390352B22674606658861        TZA      ADM2\n4       Longido     &lt;NA&gt; 72390352B95731720096997        TZA      ADM2\n5          Meru     &lt;NA&gt; 72390352B99598192663387        TZA      ADM2\n6       Monduli     &lt;NA&gt; 72390352B11439822404473        TZA      ADM2\n7    Ngorongoro     &lt;NA&gt; 72390352B42279830137418        TZA      ADM2\n8         Ilala     &lt;NA&gt; 72390352B40584164885098        TZA      ADM2\n9     Kinondoni     &lt;NA&gt; 72390352B66429416458525        TZA      ADM2\n10       Temeke     &lt;NA&gt; 72390352B94835751472469        TZA      ADM2\n                         geometry\n1  MULTIPOLYGON (((262372 9603...\n2  MULTIPOLYGON (((251788.2 96...\n3  MULTIPOLYGON (((148006.1 96...\n4  MULTIPOLYGON (((206258.1 96...\n5  MULTIPOLYGON (((262372 9603...\n6  MULTIPOLYGON (((226729.3 96...\n7  MULTIPOLYGON (((160641.8 96...\n8  MULTIPOLYGON (((530993 9249...\n9  MULTIPOLYGON (((529848.2 92...\n10 MULTIPOLYGON (((531400.6 92...\n\n\nThe output shows that there are 170 objects loaded which corresponds to individual districts. The object is also of multipolygon class which could indicate that there are districts with discontinuous land areas, like islands.\nWe can create a simple map to visualize the boundaries using qtm() from tmap.\n\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\nqtm(tz_dist, text = \"district\", text.size = 0.4)\n\n\n\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting"
  },
  {
    "objectID": "posts/tz-fin-2023/index.html#b.2-deriving-district-centroids",
    "href": "posts/tz-fin-2023/index.html#b.2-deriving-district-centroids",
    "title": "Geographically Weighted Modeling of Financial Inclusion in Tanzania",
    "section": "B.2 Deriving District centroids",
    "text": "B.2 Deriving District centroids\nBefore we load the aspatial data, we will process the district boundary map to be able to use it for future operations with the other dataset. One step that needs to be done is to define representative points, which can be the centroids for the boundary map. The primary purpose of this is to be able to map the aspatial data for a district into a single location. In order to do this, the first step is to convert the multipolygon layer object into a polygon object which will allow for proper centroid calculations for each district.\nWe use the code chunk below to convert the sf object into polygons using st_cast() and then create a column for each individual polygon’s area using mutate() with st_area(). We then use groupby() to reduce back the object to one row per district and then filter() to keep only the largest polygon for each district.\n\ntz_dist_poly &lt;- tz_dist %&gt;%\n  st_cast(\"POLYGON\") %&gt;%\n  mutate(area = st_area(.)) %&gt;%\n  group_by(district) %&gt;%\n  filter(area == max(area)) %&gt;%\n  ungroup() %&gt;%\n  select(-area) %&gt;%\n  select(district)\n\nWarning in st_cast.sf(., \"POLYGON\"): repeating attributes for all\nsub-geometries for which they may not be constant\n\n\nWe can produce a map with tmap package to see if the operation had any irregular effects on the geography. In order to see the difference between the original map and the polygon map, we add the original map as the first layer in red, and then overlay the polygon map. The areas which now appear red are the polygons or areas that have been excluded from the original map to the polygon map.\n\ntm_shape(tz_dist) +\n  tm_polygons(\"red\") +\ntm_shape(tz_dist_poly) +\n  tm_polygons(\"grey\") +\n  tm_layout(title = \"Full vs Poly Map\",\n            title.position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\nFrom the output, we see that in addition to a few small islands, there are three inland areas that have been excluded from the original map. While this produces holes in the new map, this might not be a big concern right now as long as the centroids we get from the remaining geometries is meaningful.\nIn order to generate the centroids, we can now use st_centroid() to compute them across each district’s largest polygon.\n\ntz_dist_centroids &lt;- st_centroid(tz_dist_poly)\n\nWarning: st_centroid assumes attributes are constant over geometries\n\n\nWe can check the location of the centroids by plotting them as a layer on top of the original district boundary layer using tmap package in the code below.\n\ntm_shape(tz_dist) +\n  tm_polygons(\"grey\") +\ntm_shape(tz_dist_centroids) +\n  tm_dots(\"green\", size = 0.2) +\n  tm_layout(title = \"District Centroids\",\n            title.position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\nThe centroid locations look mostly acceptable, with a few exceptions where they might be lying somewhere away from the district boundaries given some of the districts have odd, nonconvex shapes."
  },
  {
    "objectID": "posts/tz-fin-2023/index.html#b.2-loading-finscope-tanzania-2023-respondent-data",
    "href": "posts/tz-fin-2023/index.html#b.2-loading-finscope-tanzania-2023-respondent-data",
    "title": "Geographically Weighted Modeling of Financial Inclusion in Tanzania",
    "section": "B.2 Loading Finscope Tanzania 2023 Respondent Data",
    "text": "B.2 Loading Finscope Tanzania 2023 Respondent Data\nWe can use read_csv() in the code chunk below to load the raw respondent data into an R object.\n\nfstz23 &lt;- read_csv(\"data/aspatial/FinScope Tanzania 2023_Individual Main Data_FINAL.csv\", show_col_types = FALSE)\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nThere are 9915 rows or records, and 721 columns or fields. Most of these columns should not be relevant in meeting our objective, so it is advised to limit the data we work with to those meaningful variable. These variables should be our variable(s) of concern, or the dependent variable(s), and the variables that may contribute to it, or the independent variables."
  },
  {
    "objectID": "posts/tz-fin-2023/index.html#b.4-preparing-finscope-tanzania-2023-respondent-data",
    "href": "posts/tz-fin-2023/index.html#b.4-preparing-finscope-tanzania-2023-respondent-data",
    "title": "Geographically Weighted Modeling of Financial Inclusion in Tanzania",
    "section": "B.4 Preparing Finscope Tanzania 2023 Respondent Data",
    "text": "B.4 Preparing Finscope Tanzania 2023 Respondent Data\n\nB.4.1 Selecting potential variables\nThe first step in preparing the dataset is to reduce the data by keeping only the potentially relevant fields. This means identifying fields that can be used as is or to derive both the response variable(s) and the explanatory variables.\nThis is performed by scanning the datamap file (i.e., data dictionary) that accompanies the dataset. From there, we decide to keep only the following variables. We also change the variable names to a shorter and more recognizable one.\n\nFinancial Inclusion (3)Geographic (4)Demographic (11)Economic (8)Technographic (3)\n\n\n\n\n\n\n\n\n\n\nVariable Name\nDescription\nNew Variable Name\n\n\n\n\nBANKED\nClassified as: Banked; Not Banked\nfi_banked\n\n\nOVERALL_FORMAL\nClassified as using formal instruments: Yes, No\nfi_formal\n\n\nINFORMAL\nClassified as using informal instruments: Yes, No\nfi_informal\n\n\n\n\n\n\n\n\nVariable\nDescription\nNew Variable Name\n\n\n\n\nreg_name\nRegion name\nregion\n\n\ndist_name\nDistrict name\ndistrict\n\n\nward_name\nWard name\nward\n\n\nclustertype\nIndicates if in rural or urban\nurban\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable Name\nDescription\nNew Variable Name\n\n\n\n\nc8c\nAge\nage\n\n\nc9\nGender: male. or female\nfemale\n\n\nc10\nMarital status: married, divorced, widowed, single (4 levels)\nmaritalstatus\n\n\nc11\nHighest level education (10 levels of values)\neducation\n\n\nc2\nHead of household: respondent, not the respondent\nhead_hh\n\n\nc8n_a1\nVisually impaired: Yes, No\nvisual_impaired\n\n\nc8n_b1\nHearing impaired: Yes, No\nhearing_impaired\n\n\nc8n_c1\nCommunication impaired: Yes, No\ncomm_impaired\n\n\nc8n_d1\nMovement impaired: Yes, No\nmove_impaired\n\n\nc8n_e1\nDifficulty with daily activities: Yes, No\ndaily_impaired\n\n\nc8n_f1\nDifficulty remembering and concentrating: Yes, No\ncogn_impaired\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable Name\nDescription\nNew Variable Name\n\n\n\n\nc12_1\nLand ownership (6 levels)\nland_own\n\n\nc14\nFamily involved in agriculture, fishing or aquaculture: Yes, No\nagricultural\n\n\nc18_2\nPrimary source of funds (12 levels)\nsource_of_funds\n\n\nc27__17\nHas some form of ID: Yes, No\nhas_id\n\n\nD2_1__1\nReceives salary from regular job: Yes, No\nreg_job\n\n\nD2_1__2\nReceives money from selling goods produced: Yes, No\nproduction\n\n\nD2_1__11\nDoes not receive income: Yes, No\nno_income\n\n\nIncomeMain\nDerived variable for main source of income (14 levels)\nincome_source\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable Name\nDescription\nNew Variable Name\n\n\n\n\nc23__1\nAccess to mobile phone: No, Yes\nmobile\n\n\nc23__2\nAccess to internet: No, Yes\ninternet\n\n\nc24_1\nPersonally own mobile phone: No, Yes\nown_mobile\n\n\n\n\n\n\nThe code block below compiles the 29 variables and their new names in two separate objects. These will be used in the succeeding steps for data preparation.\n\ncolstokeep &lt;- c(\"reg_name\", \"dist_name\", \"ward_name\", \"clustertype\",\n                \"c8c\", \"c9\", \"c10\", \"c11\", \"c2\",\n                \"c8n_a1\", \"c8n_b1\", \"c8n_c1\", \"c8n_d1\", \"c8n_e1\", \"c8n_f1\",\n                \"c12_1\", \"c14\", \"c18_2\",\n                \"c23__1\", \"c23__2\", \"c24_1\", \"c27__17\",\n                \"D2_1__1\", \"D2_1__2\", \"D2_1__11\",\n                \"BANKED\", \"OVERALL_FORMAL\", \"INFORMAL\", \"IncomeMain\")\nnewnames &lt;- c(\"region\", \"district\", \"ward\", \"urban\",\n              \"age\", \"female\", \"maritalstatus\", \"education\", \"head_hh\",\n              \"visual_impaired\", \"hearing_impaired\", \"comm_impaired\",\n              \"move_impaired\", \"daily_impaired\", \"cogn_impaired\",\n              \"land_own\", \"agricultural\", \"source_of_funds\",\n              \"mobile\", \"internet\", \"own_mobile\", \"has_id\",\n              \"reg_job\", \"production\", \"no_income\",\n              \"fi_banked\", \"fi_formal\", \"fi_informal\", \"income_source\")\nlength(colstokeep)\n\n[1] 29\n\nlength(newnames)\n\n[1] 29\n\n\nWe then use the code chunk below to keep the selected variables using select() and then to rename the variable or column names using colnames()\n\nfstz23_sf &lt;- fstz23 %&gt;%\n  select(all_of(colstokeep))\n\ncolnames(fstz23_sf) &lt;- newnames\n\n\n\nB.4.2 Recoding of variables\nRecoding of variables is a data preparation step where variable values are replaced by another. This may be done for reasons like cleaning the data or standardising the data. This is generally performed in R using the recode() function.\n\nB.4.2.1 Recoding of district names\nWe have district names in the map and in the survey data. As these are two different data sources, there is a chance that they mismatch. We need to ensure that they use the same names as we will use these to add the geographic information to the survey data.\nWe first check which names in each set do not have a corresponding match in the other. We can do this by performing a left join using left_join() and checking which elements do not have matches. We can the use filter() to find the records that did not return a valid value from the other dataset. The code chunk performs this left join approach twice as it needs to be checked for direction for each data source, and then we display the district names that are unmatched for each dataset.\n\nmismatched_values &lt;- tz_dist %&gt;%\n  left_join(fstz23_sf, by = \"district\") %&gt;%\n  filter(is.na(region)) %&gt;%\n  select(district)\nmismatched_tzmap &lt;- mismatched_values$district\n\nmismatched_values &lt;- fstz23_sf %&gt;%\n  left_join(tz_dist, by = \"district\") %&gt;%\n  filter(is.na(shapeType)) %&gt;%\n  select(district)\nmismatched_fstz23 &lt;- mismatched_values$district\n\nlist(\n  mismatched_in_map = mismatched_tzmap,\n  number_mm1 = length(c(mismatched_tzmap)),\n  mismatched_in_survey = unique(mismatched_fstz23),\n  number_mm2 = length(c(unique(mismatched_fstz23)))\n)\n\n$mismatched_in_map\n [1] \"Arusha Urban\"                 \"Meru\"                        \n [3] \"Dodoma Urban\"                 \"Iringa Urban\"                \n [5] \"Mafinga Township Authority\"   \"Bukoba Urban\"                \n [7] \"Mpanda Urban\"                 \"Kasulu Township Authority\"   \n [9] \"Kigoma  Urban\"                \"Moshi Urban\"                 \n[11] \"Lindi Urban\"                  \"Babati UrbanBabati Urban\"    \n[13] \"Butiam\"                       \"Musoma Urban\"                \n[15] \"Mbeya Urban\"                  \"Magharibi\"                   \n[17] \"Morogoro Urban\"               \"Masasi  Township Authority\"  \n[19] \"Mtwara Urban\"                 \"Makambako Township Authority\"\n[21] \"Njombe Urban\"                 \"Kibaha Urban\"                \n[23] \"Mafia\"                        \"Sumbawanga Urban\"            \n[25] \"Songea Urban\"                 \"Kahama Township Authority\"   \n[27] \"Shinyanga Urban\"              \"Singida Urban\"               \n[29] \"Tunduma\"                      \"Tabora Urban\"                \n[31] \"Handeni Mji\"                  \"Korogwe\"                     \n[33] \"Korogwe Township Authority\"   \"Tanga Urban\"                 \n\n$number_mm1\n[1] 34\n\n$mismatched_in_survey\n [1] \"Tanganyika\"  \"Kigamboni\"   \"Arumeru\"     \"Butiama\"     \"Dodoma\"     \n [6] \"Tanga\"       \"Malinyi\"     \"Kibiti\"      \"Magharibi B\" \"Magharibi A\"\n[11] \"Ubungo\"      \"Tabora\"     \n\n$number_mm2\n[1] 12\n\n\nThe output reveals that there are 34 district names in the boundary map that are not matched, while there are 12 in the survey data that are not matched. We do not need to ensure all 34 in the first dataset is matched as there might really be areas where there are no respondents or residents. We do, however, want almost all, if not all, of the records in the second dataset to be matched as this is where our modeling data sits. It is also worth noting that all the recoding for districts will be done on fstz23_sf.\nWe will perform checks and data cleaning for the 12 unmatched values in fstz23_sf, and we will also explore some of the remaining unmatched variables in tz_dist. We see that there are values which have “Urban” at the end so there might also be an opportunity to create matches for them.\n\nTanganyika and TangaMagharibiArumeruButiamaDodomaKibitiKigamboniMalinyiTaboraUbungoSelect Urban Areas\n\n\nFor unmatched values, there might be differences in spellings between the two sources. While this doesn’t guarantee catching misspellings, we can at least check if there are other variables that share the first few letters with the unmatched value.\nTo find district values which contains “Tang” we can use str_detect() on both dataset’s columns.\n\nto_find &lt;- \"Tanga\"\nunique(tz_dist_poly[str_detect(tz_dist_poly$district,to_find),]$district)\n\n[1] \"Tanga Urban\"\n\nunique(fstz23_sf[str_detect(fstz23_sf$district,to_find),]$district)\n\n[1] \"Tanganyika\" \"Tanga\"     \n\n\nThere is only one value in tz_dist that contains “Tang” but two in fstz_23. It should be safe to assume that Tanga and Tanga Urban are one and the same. We can perform the recoding using recode() within mutate() for the district column of fstz_23.\n\nfstz23_sf &lt;- mutate(fstz23_sf, district = recode(district,\n                            \"Tanga\" = \"Tanga Urban\"))\n\nFor Tanganyika, we can use the following code chunk to check how many survey records are affected using length(), and then show the wards and regions for the records that do have a district name of Tanganyika.\n\n# Count number of records which has district name Tanganyika\ncount(fstz23_sf[str_detect(fstz23_sf$district,\"Tanganyika\"),])\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1    89\n\n# Show regions and wards for records with district name Tanganyika\nunique(select(fstz23_sf[str_detect(fstz23_sf$district,\"Tanganyika\"),], \n       c(region, district, ward)))\n\n# A tibble: 6 × 3\n  region district   ward    \n  &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;   \n1 Katavi Tanganyika Mnyagala\n2 Katavi Tanganyika Ikola   \n3 Katavi Tanganyika Bulamata\n4 Katavi Tanganyika Mishamo \n5 Katavi Tanganyika Sibwesa \n6 Katavi Tanganyika Isengule\n\n\nThere are six wards that all reflect the same region name of “Katavi”. Upon research, it appears that the Tanganyika district was recently formed and was part of the rural area of the district Mpanda. As such, we will recode Tanganyika to Mpanda using the same approach with\n\nfstz23_sf &lt;- mutate(fstz23_sf, district = recode(district,\n                            \"Tanganyika\" = \"Mpanda\"))\n\n\n\nWe saw that there are two districts with the word Magharibi in the fstz23, and one in tz_dist. We can confirm this by using str_detect() to check for all district names containing “Mag” (so we also check some variation in spelling) in both datasets.\n\nto_find &lt;- \"Magha\"\nunique(tz_dist_poly[str_detect(tz_dist_poly$district,to_find),]$district)\n\n[1] \"Magharibi\"\n\nunique(fstz23_sf[str_detect(fstz23_sf$district,to_find),]$district)\n\n[1] \"Magharibi B\" \"Magharibi A\"\n\n\nFor this case, we drop the B and A by using recode() on the district column of ftsz23_sf.\n\nfstz23_sf &lt;- mutate(fstz23_sf, district = recode(district,\n                            \"Magharibi B\" = \"Magharibi\",\n                            \"Magharibi A\" = \"Magharibi\"))\n\n\n\nWe next look into the unmatched district “Arumeru” in fstz23. We will use the code chunk below for this district and most of the succeeding ones to check district names that match in both data sets, show the regions and wards in fstz23 for the district, and the number of records which reflect that district. For these, we continue using str_detect() which is included in tidyverse under the stringr package in order to find matches of a substring.\n\nto_find &lt;- \"Arumeru\"\nprint(\"Matches in tz_dist\")\n\n[1] \"Matches in tz_dist\"\n\nunique(tz_dist_poly[str_detect(tz_dist_poly$district,to_find),]$district)\n\ncharacter(0)\n\nprint(\"Matches in ftsz23\")\n\n[1] \"Matches in ftsz23\"\n\nunique(fstz23_sf[str_detect(fstz23_sf$district,to_find),]$district)\n\n[1] \"Arumeru\"\n\ncount(fstz23_sf[str_detect(fstz23_sf$district,to_find),])\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1   105\n\nunique(select(fstz23_sf[str_detect(fstz23_sf$district,to_find),], \n       c(region, district, ward, urban)))\n\n# A tibble: 7 × 4\n  region district ward         urban\n  &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;        &lt;chr&gt;\n1 Arusha Arumeru  Poli         Urban\n2 Arusha Arumeru  Maroroni     Rural\n3 Arusha Arumeru  Olmotonyi    Rural\n4 Arusha Arumeru  Oloirien     Urban\n5 Arusha Arumeru  Maji ya Chai Rural\n6 Arusha Arumeru  Kisongo      Rural\n7 Arusha Arumeru  Nkoaranga    Rural\n\n\nThe output shows that there are no districts in tz_dist that have a name of Arumeru, but there are 105 in fstz23. Upon research, we see that the wards of Arumeru was split between Arusha and Meru. Among the wards in the dataset, the following are now part of Meru: Maroroni, Poli, Maji ya Chai, Nkoaranga. The balance 3 are Arusha: Olmotonyi, Oloirien, Kisongo.\nWe first update the records for the last three wards to reflect a district name of Arusha using the following code chunk.\n\nfstz23_sf[(fstz23_sf$ward == \"Olmotonyi\" | fstz23_sf$ward == \"Oloirien\" | fstz23_sf$ward == \"Kisongo\"),]$district = \"Arusha\"\n\nWe can then use recode() to change the remaining records that are still reflecting “Arumeru” and change them to “Meru”\n\nfstz23_sf &lt;- mutate(fstz23_sf, district = recode(district,\n                            \"Arumeru\" = \"Meru\"))\n\n\n\nWe execute the same code chunk that makes use of recode() to find records where the district name contains “Butiam”\n\nto_find &lt;- \"Butiam\"\nprint(\"Matches in tz_dist\")\n\n[1] \"Matches in tz_dist\"\n\nunique(tz_dist_poly[str_detect(tz_dist_poly$district,to_find),]$district)\n\n[1] \"Butiam\"\n\nprint(\"Matches in ftsz23\")\n\n[1] \"Matches in ftsz23\"\n\nunique(fstz23_sf[str_detect(fstz23_sf$district,to_find),]$district)\n\n[1] \"Butiama\"\n\ncount(fstz23_sf[str_detect(fstz23_sf$district,to_find),])\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1    45\n\nunique(select(fstz23_sf[str_detect(fstz23_sf$district,to_find),], \n       c(region, district, ward, urban)))\n\n# A tibble: 3 × 4\n  region district ward    urban\n  &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;\n1 Mara   Butiama  Butiama Urban\n2 Mara   Butiama  Bukabwa Rural\n3 Mara   Butiama  Mirwa   Rural\n\n\nWe see that there is only one district from each dataset and it appears that they can only refer to the same district. We then use recode() to update “Butiama” to “Butiam”\n\nfstz23_sf &lt;- mutate(fstz23_sf, district = recode(district,\n                            \"Butiama\" = \"Butiam\"))\n\n\n\nWe execute the same code chunk that makes use of recode() to find records where the district name contains “Dodom”\n\nto_find &lt;- \"Dodom\"\nprint(\"Matches in tz_dist\")\n\n[1] \"Matches in tz_dist\"\n\nunique(tz_dist_poly[str_detect(tz_dist_poly$district,to_find),]$district)\n\n[1] \"Dodoma Urban\"\n\nprint(\"Matches in ftsz23\")\n\n[1] \"Matches in ftsz23\"\n\nunique(fstz23_sf[str_detect(fstz23_sf$district,to_find),]$district)\n\n[1] \"Dodoma\"\n\ncount(fstz23_sf[str_detect(fstz23_sf$district,to_find),])\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1   142\n\nunique(select(fstz23_sf[str_detect(fstz23_sf$district,to_find),], \n       c(region, district, ward, urban)))\n\n# A tibble: 10 × 4\n   region district ward            urban\n   &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;           &lt;chr&gt;\n 1 Dodoma Dodoma   Msalato         Urban\n 2 Dodoma Dodoma   Mnadani         Urban\n 3 Dodoma Dodoma   Ntyuka          Urban\n 4 Dodoma Dodoma   Mbabala         Urban\n 5 Dodoma Dodoma   Nkuhungu        Urban\n 6 Dodoma Dodoma   Nzuguni         Urban\n 7 Dodoma Dodoma   Hombolo Bwawani Urban\n 8 Dodoma Dodoma   Kikuyu Kusini   Urban\n 9 Dodoma Dodoma   Mkonze          Urban\n10 Dodoma Dodoma   Uhuru           Urban\n\n\nWe again see that there is a one-to-one matching for the sole district on both dataset. We will then update “Dodoma” to “Dodoma Urban” using recode() in the chunk below\n\nfstz23_sf &lt;- mutate(fstz23_sf, district = recode(district,\n                            \"Dodoma\" = \"Dodoma Urban\"))\n\n\n\nWe execute the same code chunk that makes use of recode() to find records where the district name contains “Kibit”\n\nto_find &lt;- \"Kibit\"\nprint(\"Matches in tz_dist\")\n\n[1] \"Matches in tz_dist\"\n\nunique(tz_dist_poly[str_detect(tz_dist_poly$district,to_find),]$district)\n\ncharacter(0)\n\nprint(\"Matches in ftsz23\")\n\n[1] \"Matches in ftsz23\"\n\nunique(fstz23_sf[str_detect(fstz23_sf$district,to_find),]$district)\n\n[1] \"Kibiti\"\n\ncount(fstz23_sf[str_detect(fstz23_sf$district,to_find),])\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1    29\n\nunique(select(fstz23_sf[str_detect(fstz23_sf$district,to_find),], \n       c(region, district, ward, urban)))\n\n# A tibble: 2 × 4\n  region district ward   urban\n  &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt;\n1 Pwani  Kibiti   Mbuchi Rural\n2 Pwani  Kibiti   Bungu  Rural\n\n\nUpon research, it appears that Kibiti is a relatively new district. The two wards that appear under it were actually part of Rufiji district, so we can change “Kibiti” to “Rufiji” using encode()\n\nfstz23_sf &lt;- mutate(fstz23_sf, district = recode(district,\n                            \"Kibiti\" = \"Rufiji\"))\n\n\n\nWe execute the same code chunk that makes use of recode() to find records where the district name contains “Kigamb”\n\nto_find &lt;- \"Kigamb\"\nprint(\"Matches in tz_dist\")\n\n[1] \"Matches in tz_dist\"\n\nunique(tz_dist_poly[str_detect(tz_dist_poly$district,to_find),]$district)\n\ncharacter(0)\n\nprint(\"Matches in ftsz23\")\n\n[1] \"Matches in ftsz23\"\n\nunique(fstz23_sf[str_detect(fstz23_sf$district,to_find),]$district)\n\n[1] \"Kigamboni\"\n\ncount(fstz23_sf[str_detect(fstz23_sf$district,to_find),])\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1    45\n\nunique(select(fstz23_sf[str_detect(fstz23_sf$district,to_find),], \n       c(region, district, ward, urban)))\n\n# A tibble: 3 × 4\n  region        district  ward      urban\n  &lt;chr&gt;         &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;\n1 Dar es Salaam Kigamboni Kibada    Urban\n2 Dar es Salaam Kigamboni Kigamboni Urban\n3 Dar es Salaam Kigamboni Somangila Urban\n\n\nThere is no indication of the district merging or splitting recently from another, but if we check our map, the region should occupy part of the space which appears as “Temeke”. We then recode Kigamboni as Temeke using the following code chunk.\n\nfstz23_sf &lt;- mutate(fstz23_sf, district = recode(district,\n                            \"Kigamboni\" = \"Temeke\"))\n\n\n\nWe execute the same code chunk that makes use of recode() to find records where the district name contains “Malin”\n\nto_find &lt;- \"Malin\"\nprint(\"Matches in tz_dist\")\n\n[1] \"Matches in tz_dist\"\n\nunique(tz_dist_poly[str_detect(tz_dist_poly$district,to_find),]$district)\n\ncharacter(0)\n\nprint(\"Matches in ftsz23\")\n\n[1] \"Matches in ftsz23\"\n\nunique(fstz23_sf[str_detect(fstz23_sf$district,to_find),]$district)\n\n[1] \"Malinyi\"\n\ncount(fstz23_sf[str_detect(fstz23_sf$district,to_find),])\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1    15\n\nunique(select(fstz23_sf[str_detect(fstz23_sf$district,to_find),], \n       c(region, district, ward, urban)))\n\n# A tibble: 1 × 4\n  region   district ward     urban\n  &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;\n1 Morogoro Malinyi  Usangule Rural\n\n\nIf we check the map, the location of Malinyi falls in the region of Ulanga in our map. We again use recode() to change the district names to Ulanga.\n\nfstz23_sf &lt;- mutate(fstz23_sf, district = recode(district,\n                            \"Malinyi\" = \"Ulanga\"))\n\n\n\nWe execute the same code chunk that makes use of recode() to find records where the district name contains “Tabor”\n\nto_find &lt;- \"Tabor\"\nprint(\"Matches in tz_dist\")\n\n[1] \"Matches in tz_dist\"\n\nunique(tz_dist_poly[str_detect(tz_dist_poly$district,to_find),]$district)\n\n[1] \"Tabora Urban\"\n\nprint(\"Matches in ftsz23\")\n\n[1] \"Matches in ftsz23\"\n\nunique(fstz23_sf[str_detect(fstz23_sf$district,to_find),]$district)\n\n[1] \"Tabora\"\n\ncount(fstz23_sf[str_detect(fstz23_sf$district,to_find),])\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1    45\n\nunique(select(fstz23_sf[str_detect(fstz23_sf$district,to_find),], \n       c(region, district, ward, urban)))\n\n# A tibble: 3 × 4\n  region district ward     urban\n  &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;\n1 Tabora Tabora   Mbugani  Urban\n2 Tabora Tabora   Kiloleni Urban\n3 Tabora Tabora   Mwinyi   Urban\n\n\nGiven that there is only “Tabora Urban” in tz_dist, we should be able to update the district names in fstz23 using this.\n\nfstz23_sf &lt;- mutate(fstz23_sf, district = recode(district,\n                            \"Tabora\" = \"Tabora Urban\"))\n\n\n\nWe execute the same code chunk that makes use of recode() to find records where the district name contains “Ubung”\n\nto_find &lt;- \"Ubung\"\nprint(\"Matches in tz_dist\")\n\n[1] \"Matches in tz_dist\"\n\nunique(tz_dist_poly[str_detect(tz_dist_poly$district,to_find),]$district)\n\ncharacter(0)\n\nprint(\"Matches in ftsz23\")\n\n[1] \"Matches in ftsz23\"\n\nunique(fstz23_sf[str_detect(fstz23_sf$district,to_find),]$district)\n\n[1] \"Ubungo\"\n\ncount(fstz23_sf[str_detect(fstz23_sf$district,to_find),])\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1    88\n\nunique(select(fstz23_sf[str_detect(fstz23_sf$district,to_find),], \n       c(region, district, ward, urban)))\n\n# A tibble: 6 × 4\n  region        district ward    urban\n  &lt;chr&gt;         &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;\n1 Dar es Salaam Ubungo   Mabibo  Urban\n2 Dar es Salaam Ubungo   Mbezi   Urban\n3 Dar es Salaam Ubungo   Kimara  Urban\n4 Dar es Salaam Ubungo   Msigani Urban\n5 Dar es Salaam Ubungo   Goba    Urban\n6 Dar es Salaam Ubungo   Manzese Urban\n\n\nUpon checking, the location of Ubungo appears to be within the boundaries of the district Kinondoni in our map. We recode it as such with the following code chunk.\n\nfstz23_sf &lt;- mutate(fstz23_sf, district = recode(district,\n                            \"Ubungo\" = \"Kinondoni\"))\n\n\n\nWe have removed all mismatches from fstz23 but have 28 unmatched district names in tz_dist. Among these are a few districts that are suffixed by “Urban”\n\nmismatched_values &lt;- tz_dist %&gt;%\n  left_join(fstz23_sf, by = \"district\") %&gt;%\n  filter(is.na(region)) %&gt;%\n  select(district)\nmismatch_urban &lt;- mismatched_values[str_detect(mismatched_values$district,\"Urban\"),]$district\nmismatch_urban\n\n [1] \"Arusha Urban\"             \"Iringa Urban\"            \n [3] \"Bukoba Urban\"             \"Mpanda Urban\"            \n [5] \"Kigoma  Urban\"            \"Moshi Urban\"             \n [7] \"Lindi Urban\"              \"Babati UrbanBabati Urban\"\n [9] \"Musoma Urban\"             \"Mbeya Urban\"             \n[11] \"Morogoro Urban\"           \"Mtwara Urban\"            \n[13] \"Njombe Urban\"             \"Kibaha Urban\"            \n[15] \"Sumbawanga Urban\"         \"Songea Urban\"            \n[17] \"Shinyanga Urban\"          \"Singida Urban\"           \n\n\nFor these, we will assume they refer to the urban region of a given district. For example, Arusha Urban will cover the urban area of the Arusha district. Based on this assumption, we can go through ftsz23 to check for records for that district and are in the urban area and then map them to the values above. We use a for loop in the code chunk below to find records in fstz23 that match these conditions and then apply the suffixed district names as replacements. We use the word() function from stringr package in order to pick the first word and treat it as the “plain” district name.\n\nfor (i in mismatch_urban)\n  {\n    dist = word(i, 1)\n    fstz23_sf[str_detect(fstz23_sf$district,dist) & fstz23_sf$urban == \"Urban\",]$district = i\n  }\n\n\n\n\nWe can check for the remaining mismatches by rerunning the earlier code.\n\nmismatched_values &lt;- tz_dist %&gt;%\n  left_join(fstz23_sf, by = \"district\") %&gt;%\n  filter(is.na(region)) %&gt;%\n  select(district)\nmismatched_tzmap &lt;- mismatched_values$district\n\nmismatched_values &lt;- fstz23_sf %&gt;%\n  left_join(tz_dist, by = \"district\") %&gt;%\n  filter(is.na(shapeType)) %&gt;%\n  select(district)\nmismatched_fstz23 &lt;- mismatched_values$district\n\nlist(\n  mismatched_in_map = mismatched_tzmap,\n  number_mm1 = length(c(mismatched_tzmap)),\n  mismatched_in_survey = unique(mismatched_fstz23),\n  number_mm2 = length(c(unique(mismatched_fstz23)))\n)\n\n$mismatched_in_map\n [1] \"Iringa Urban\"                 \"Mafinga Township Authority\"  \n [3] \"Kasulu Township Authority\"    \"Masasi  Township Authority\"  \n [5] \"Makambako Township Authority\" \"Mafia\"                       \n [7] \"Kahama Township Authority\"    \"Tunduma\"                     \n [9] \"Handeni Mji\"                  \"Korogwe\"                     \n[11] \"Korogwe Township Authority\"  \n\n$number_mm1\n[1] 11\n\n$mismatched_in_survey\ncharacter(0)\n\n$number_mm2\n[1] 0\n\n\nWe were able to remove all mismatches from fstz23 and then reduce the mismatches from tz_dist from 34 to 11. We can visualize the distribution of the (recoded) records in our map by first adding the number of records into the sf object. We use count() in the code chunk below to compute for the number of records per district. We then use left_join() to merge it with the district map, and replace any zero values (from mismatches) with zero.\n\ntz_dist_stat &lt;- tz_dist %&gt;%\n  left_join(count(fstz23_sf, district), by= \"district\") %&gt;%\n  rename(\"orig_records\" = \"n\") %&gt;%\n  replace(is.na(.),0)\n\nWe can then use tmap package to plot the distribution of respondents, We first add a layer in grey for the full map and then add a choropleth map for districts with nonzero number of respondents. This will show districts with no respondents as grey.\n\ntm_shape(tz_dist)+\n  tm_polygons(\"grey\") +\ntm_shape(tz_dist_stat[tz_dist_stat$orig_records &gt; 0,]) +\n  tm_polygons(\"orig_records\", title = \"Original Records\")\n\n\n\n\n\n\n\n\nThe output shows that there are only a few districts that do not have any respondents. Most of the districts have between 0 to 50 respondents. The highest number of respondents in a single district is between 150 and 200.\n\n\nB.4.2.2 Recoding of Modelling Variables\nIf we look at the data, the modelling variables that we have in fstz23 are mostly categorical. Most of these are binary, but some have more than two values. While we can use categorical variables for EDA, these do not work well once we start modelling. For our case, we will go ahead and convert most of these variables upfront.\nWe can use the following code chunk which uses the unique() function to display the distinct values. We use lapply() to run the function on each variable in the dataframe but we exclude the region, district, ward and age as these will either not be part of the modelling, or it is already in numeric form.\n\nlapply(select(fstz23_sf,-c(region, district, ward, age)), unique)\n\n$urban\n[1] \"Rural\" \"Urban\"\n\n$female\n[1] \"Female\" \"Male\"  \n\n$maritalstatus\n[1] \"Married/living together\" \"Widowed\"                \n[3] \"Divorced/separated\"      \"Single/never married\"   \n\n$education\n [1] \"Some primary\"                             \n [2] \"No formal education\"                      \n [3] \"Primary completed\"                        \n [4] \"Some secondary\"                           \n [5] \"Some University or other higher education\"\n [6] \"University or higher education completed\" \n [7] \"Secondary competed-O level\"               \n [8] \"Post primary technical training\"          \n [9] \"Secondary completed-A level\"              \n[10] \"Don’t know\"                               \n\n$head_hh\n[1] \"Respondent is hhh\"  \"Respondent not hhh\"\n\n$visual_impaired\n[1] \"Yes\" \"No\" \n\n$hearing_impaired\n[1] \"No\"  \"Yes\"\n\n$comm_impaired\n[1] \"No\"  \"Yes\"\n\n$move_impaired\n[1] \"No\"  \"Yes\"\n\n$daily_impaired\n[1] \"No\"  \"Yes\"\n\n$cogn_impaired\n[1] \"No\"  \"Yes\"\n\n$land_own\n[1] \"You personally own the land/plot where you live\" \n[2] \"The land/plot is rented\"                         \n[3] \"You own the land/plot together with someone else\"\n[4] \"You don’t own or rent the land\"                  \n[5] \"Other A household members owns the land/plot\"    \n[6] \"Don’t know (Don’t read out)\"                     \n\n$agricultural\n[1] \"Yes\" \"No\" \n\n$source_of_funds\n [1] \"Your household sells some of its crops and uses the money\"                                              \n [2] NA                                                                                                       \n [3] \"Your household has to borrow money\"                                                                     \n [4] \"Your household has money to buy it, it uses money from wages / other regular job /  sources of income\"  \n [5] \"Help from friends/relatives/neighbors/community/Government\"                                             \n [6] \"Use savings the household has\"                                                                          \n [7] \"Your household sells non-agricultural things to get money\"                                              \n [8] \"Your household gets it from a buyer to whom it has to sell its crop, livestock or fish when it is ready\"\n [9] \"Your household does piece work/casual jobs to get money to buy it\"                                      \n[10] \"Your household sells some of its livestock and uses the money\"                                          \n[11] \"Your household gets it in exchange for work it does\"                                                    \n[12] \"Your household doesn’t have to buy because it manage with what it has\"                                  \n[13] \"Your household sells products like milk, eggs that it get from its livestock to get money to buy it\"    \n\n$mobile\n[1] \"Yes\" \"No\" \n\n$internet\n[1] \"Yes\" \"No\"  NA   \n\n$own_mobile\n[1] \"Yes\" \"2\"  \n\n$has_id\n[1] \"No\"  \"Yes\"\n\n$reg_job\n[1] \"No\"  \"Yes\"\n\n$production\n[1] \"Yes\" \"No\" \n\n$no_income\n[1] \"No\"  \"Yes\"\n\n$fi_banked\n[1] \"Not Banked\" \"Banked\"    \n\n$fi_formal\n[1] \"OVERALL_FORMAL\"     \"Not OVERALL_FORMAL\"\n\n$fi_informal\n[1] \"INFORMAL incl SACCO AND CMG RISK CONTRIBUTIONS\"\n[2] \"Not INFORMAL\"                                  \n\n$income_source\n [1] \"Farmers and fishers\"                                         \n [2] \"Piece work/casual labor\"                                     \n [3] \"Traders - non-agricultural\"                                  \n [4] \"Dependents\"                                                  \n [5] \"Service providers\"                                           \n [6] \"Traders - agricultural products\"                             \n [7] \"Welfare\"                                                     \n [8] \"Formal sector salaried\"                                      \n [9] \"Pension\"                                                     \n[10] \"Informal sector salaried\"                                    \n[11] \"Other\"                                                       \n[12] \"Rental income\"                                               \n[13] \"Gambling\"                                                    \n[14] \"Interest from savings, investments, stocks, unit trusts etc.\"\n\n\nThe output reveals that 5 of the variables have more than 2 values or levels, while the balance 20 are binary. We also see that for the internet variable there are some records that show NA. We use the code below to check if each of the records have NA for internet by using is.na(), and then counting the number of records by just adding up the TRUE values using sum()\n\nsum(is.na(fstz23_sf$internet))\n\n[1] 7\n\n\nAs there are only seven with NA values, and there is no sure way of replacing them with the right value, removing them from the dataset should not produce any big issues. We use the code chunk below to remove the na’s from the internet variable. There are a number of different ways to remove na’s, here we just use a mask based on the complement of the results of the is.na() function which returns TRUE for any invalid values. We include the count of rows before and after running the code to check that there is a difference of seven rows.\n\nnrow(fstz23_sf)\n\n[1] 9915\n\nfstz23_sf &lt;- fstz23_sf[!is.na(fstz23_sf$internet),]\nnrow(fstz23_sf)\n\n[1] 9908\n\n\nFor binary variables, we will use the code below to replace the ‘positive’ value with 1 and the negative with 0. The values for each variable may vary so we need to apply the recoding individually for these variables.\n\nfstz23_sf &lt;- mutate(fstz23_sf,\n                    urban = recode(urban, \"Rural\" = 0, \"Urban\" = 1),\n                    female = recode(female, \"Male\" = 0, \"Female\" = 1),\n                    head_hh = recode(head_hh, \"Respondent not hhh\" = 0, \"Respondent is hhh\" = 1),\n                    visual_impaired = recode(visual_impaired, \"No\" = 0, \"Yes\" = 1),\n                    hearing_impaired = recode(hearing_impaired, \"No\" = 0, \"Yes\" = 1),\n                    comm_impaired = recode(comm_impaired, \"No\" = 0, \"Yes\" = 1),\n                    move_impaired = recode(move_impaired, \"No\" = 0, \"Yes\" = 1),\n                    daily_impaired = recode(daily_impaired, \"No\" = 0, \"Yes\" = 1),\n                    cogn_impaired = recode(cogn_impaired, \"No\" = 0, \"Yes\" = 1),\n                    agricultural = recode(agricultural, \"No\" = 0, \"Yes\" = 1),\n                    mobile = recode(mobile, \"No\" = 0, \"Yes\" = 1),\n                    internet = recode(internet, \"No\" = 0, \"Yes\" = 1),\n                    own_mobile = recode(own_mobile, \"2\" = 0, \"Yes\" = 1),\n                    has_id = recode(has_id, \"No\" = 0, \"Yes\" = 1),\n                    reg_job = recode(reg_job, \"No\" = 0, \"Yes\" = 1),\n                    production = recode(production, \"No\" = 0, \"Yes\" = 1),\n                    no_income = recode(no_income, \"No\" = 0, \"Yes\" = 1),\n                    fi_banked = recode(fi_banked, \"Not Banked\" = 0, \"Banked\" = 1),\n                    fi_formal = recode(fi_formal, \"Not OVERALL_FORMAL\" = 0, \"OVERALL_FORMAL\" = 1),\n                    fi_informal = recode(fi_informal, \"Not INFORMAL\" = 0,\n                                         \"INFORMAL incl SACCO AND CMG RISK CONTRIBUTIONS\" = 1))\n\nFor the non-binary variables, we start by checking the distribution of the data across their different levels. Where there is a very small amount of data in one category, we will not be too worried merging them with another (logical) one. We use the code chunk below which uses count() to give the number of records or rows for each of the values of the indicated column. We then wrap the output in arrange() to sort the values in ascending number of records.\n\nMarital StatusEducationLand OwnershipSource of FundsPrimary Source of Income\n\n\n\narrange(count(fstz23_sf, maritalstatus),n)\n\n# A tibble: 4 × 2\n  maritalstatus               n\n  &lt;chr&gt;                   &lt;int&gt;\n1 Widowed                   949\n2 Divorced/separated        956\n3 Single/never married     1934\n4 Married/living together  6069\n\n\n\n\n\narrange(count(fstz23_sf, education),n)\n\n# A tibble: 10 × 2\n   education                                     n\n   &lt;chr&gt;                                     &lt;int&gt;\n 1 Don’t know                                    4\n 2 Secondary completed-A level                  40\n 3 Post primary technical training              55\n 4 Some University or other higher education   125\n 5 University or higher education completed    292\n 6 Some secondary                              830\n 7 Secondary competed-O level                 1273\n 8 Some primary                               1354\n 9 No formal education                        1592\n10 Primary completed                          4343\n\n\n\n\n\narrange(count(fstz23_sf, land_own),n)\n\n# A tibble: 6 × 2\n  land_own                                             n\n  &lt;chr&gt;                                            &lt;int&gt;\n1 Don’t know (Don’t read out)                         14\n2 The land/plot is rented                           1022\n3 You own the land/plot together with someone else  1544\n4 You don’t own or rent the land                    1820\n5 Other A household members owns the land/plot      1974\n6 You personally own the land/plot where you live   3534\n\n\n\n\n\narrange(count(fstz23_sf, source_of_funds),n)\n\n# A tibble: 13 × 2\n   source_of_funds                                                             n\n   &lt;chr&gt;                                                                   &lt;int&gt;\n 1 Your household gets it in exchange for work it does                        30\n 2 Your household sells products like milk, eggs that it get from its liv…    32\n 3 Your household doesn’t have to buy because it manage with what it has      43\n 4 Help from friends/relatives/neighbors/community/Government                 67\n 5 Your household has to borrow money                                        101\n 6 Your household gets it from a buyer to whom it has to sell its crop, l…   146\n 7 Your household sells non-agricultural things to get money                 184\n 8 Your household sells some of its livestock and uses the money             304\n 9 Your household has money to buy it, it uses money from wages / other r…   448\n10 Use savings the household has                                             930\n11 Your household does piece work/casual jobs to get money to buy it        1164\n12 Your household sells some of its crops and uses the money                2325\n13 &lt;NA&gt;                                                                     4134\n\n\n\n\n\narrange(count(fstz23_sf, income_source),n)\n\n# A tibble: 14 × 2\n   income_source                                                    n\n   &lt;chr&gt;                                                        &lt;int&gt;\n 1 Interest from savings, investments, stocks, unit trusts etc.     2\n 2 Gambling                                                         6\n 3 Rental income                                                   46\n 4 Other                                                           67\n 5 Pension                                                         67\n 6 Welfare                                                         87\n 7 Informal sector salaried                                       209\n 8 Traders - agricultural products                                218\n 9 Service providers                                              386\n10 Formal sector salaried                                         426\n11 Traders - non-agricultural                                     643\n12 Dependents                                                    1960\n13 Piece work/casual labor                                       2559\n14 Farmers and fishers                                           3232\n\n\n\n\n\nThe output shows that there is a very large number of NAs in the source of funds. We highlight this using the is.na() function in the first code chunk below. As there are more than 40% missing values, and the income_source variable may already be holding the similar, but more complete, information, we will go ahead and drop the variable by using the select() function in the second code chunk.\n\nsum(is.na(fstz23_sf$source_of_funds))\n\n[1] 4134\n\n\n\nfstz23_sf &lt;- select(fstz23_sf, -c(source_of_funds))\n\nWhile we will be preforming a separate EDA for the variables, we will already create binary variables for certain levels of the remaining categorical variables. Common practice is to produce n-1 dummy variable for a variable with n-levels, however, we will opt to consolidate some of the variables together where it makes sense. For martial status, we can keep three levels. The first variable denotes whether the respondent is currently married, the second variable will be whether the respondent is widowed, separated or divorced. Single respondents should reflect a value of zero for both of the variables. For the code chunks, we use as.integer() to convert the logical outputs into zeros and ones.\n\nfstz23_sf$is_married &lt;- as.integer(fstz23_sf$maritalstatus == \"Married/living together\")\nfstz23_sf$was_married &lt;- as.integer(fstz23_sf$maritalstatus == \"Widowed\" | fstz23_sf$maritalstatus == \"Divorced/separated\")\n\nFor education, we will keep four levels for primary, secondary and tertiary or higher education. (with the fourth level denoting that the respondent has not completed primary)\n\nfstz23_sf$educ_primary &lt;- as.integer(fstz23_sf$education == \"Primary completed\" | fstz23_sf$education == \"Some secondary\" | fstz23_sf$education == \"Post primary technical training\")\nfstz23_sf$educ_secondary &lt;- as.integer(fstz23_sf$education == \"Secondary competed-O level\" | fstz23_sf$education == \"Secondary completed-A level\" | fstz23_sf$education == \"Some University or other higher education\")\nfstz23_sf$educ_tertiary &lt;- as.integer(fstz23_sf$education == \"University or higher education completed\")\n\nFor land ownership, we define start with four levels to denote whether the respondent personally owns the land, the land is owned by family or shared with someone, the land is rented, or the land is neither owned nor rented.\n\nfstz23_sf$land_self_own &lt;- as.integer(fstz23_sf$land_own == \"You personally own the land/plot where you live\")\nfstz23_sf$land_hh_or_shared &lt;- as.integer(fstz23_sf$land_own == \"You own the land/plot together with someone else\" | fstz23_sf$land_own == \"Other A household members owns the land/plot\")\nfstz23_sf$land_rented &lt;- as.integer(fstz23_sf$land_own == \"The land/plot is rented\")\n\nFor sources of income, we see that the largest group is “Farmers and Fishers” (3232), “Piece-work or Casual Labor” (2559) and “Dependents” (1960). We will keep these three as distinct levels. We can then define traders (861), salaried (635) and all other sources excuding welfare, gambling, pension, and service providers. We will take service providers to be very similar to casual labor as it also counts as an irregular source of funds.\n\nfstz23_sf$income_farm_and_fish &lt;- as.integer(fstz23_sf$income_source == \"Farmers and fishers\")\nfstz23_sf$income_piecework &lt;- as.integer(fstz23_sf$income_source == \"Piece work/casual labor\" | fstz23_sf$income_source == \"Service providers\")\nfstz23_sf$income_dependent &lt;- as.integer(fstz23_sf$income_source == \"Dependents\")\nfstz23_sf$income_trader &lt;- as.integer(fstz23_sf$income_source == \"Traders - non-agricultural\" | fstz23_sf$income_source == \"Traders - agricultural products\")\nfstz23_sf$income_salaried &lt;- as.integer(fstz23_sf$income_source == \"Formal sector salaried\" | fstz23_sf$income_source == \"Informal sector salaried\")\nfstz23_sf$income_other &lt;- as.integer(fstz23_sf$income_source == \"Other\" | fstz23_sf$income_source == \"Rental income\" | fstz23_sf$income_source == \"Interest from savings, investments, stocks, unit trusts etc.\")\n\nThese recoded variables will now be used for model calibrations instead of the original variables.\n\n\n\nB.4.3 Creation of overall measures\nThere are currently three different variables for financial inclusion looking at three different dimensions of financial inclusion. We can create an overall financial inclusion variable to indicate if the respondent is included in any of the three different dimensions of FI.\nWe use the following code to create a new variable which returns 1 if any of the three variables is 1, otherwise it returns zero. As the three original variables are in zero and one, we can use the logical or operator (|) to implement this operation. We then use as.integer() to convert the result from logical to a zero-one integer.\n\nfstz23_sf$fi_overall &lt;- as.integer(fstz23_sf$fi_banked | fstz23_sf$fi_formal | fstz23_sf$fi_informal)\n\nWe can also do the same for the different categories of physical impairment and create a single variable that combines it all.\n\nfstz23_sf$any_impaired &lt;- as.integer(fstz23_sf$visual_impaired | fstz23_sf$hearing_impaired | fstz23_sf$comm_impaired |\n                                       fstz23_sf$move_impaired | fstz23_sf$daily_impaired)\n\n\n\nB.4.4 Converting fstz23_sf into an sf dataframe\nThe object fstz23_sf still does not include any geospatial information and cannot be used later for geographically weighted modelling. To solve this, we can use the district centroids as the point location of the respondents. We first use left_join() to import the point geometry of the centroids, and then we use st_as_sf() in order to make sure that the new object is recognized as an sf dataframe.\n\nfstz23_sf &lt;- left_join(fstz23_sf, tz_dist_centroids, by = \"district\") %&gt;%\n  st_as_sf()\n\nWe can check if the geometries are properly mapped by plotting the respondents onto the boundary map using tmap package.\n\ntm_shape(tz_dist) +\n  tm_polygons(\"grey\") +\ntm_shape(fstz23_sf) +\n  tm_dots(\"blue\", size = 0.2) +\n  tm_layout(title = \"Respondents\",\n            title.position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\nThe respondents appear to be properly mapped to the district centroids, however, duplicate locations will be unacceptable for the methods we will use for geographically weighted modelling later. To solver this, we can slightly shift the points by introducing st_jitter(). For the amount argument, we use a value of 1000 which means that points will be shifted by up to 1km from their original point. This 1km should not be an issue and will not cause points to go beyond the district boundary.\n\nfstz23_sf &lt;- st_jitter(fstz23_sf, 1000)\n\nWe can doublecheck that the operation is successful and there are no duplicated locations by using duplicated() to check if a value is duplicated and then using any() to check if the function returned true for any value.\n\nany(duplicated(fstz23_sf$geometry))\n\n[1] FALSE\n\n\nThe code hase returned FALSE so we are assured that there are no duplicated point locations."
  },
  {
    "objectID": "posts/tz-fin-2023/index.html#c.1-respondent-age",
    "href": "posts/tz-fin-2023/index.html#c.1-respondent-age",
    "title": "Geographically Weighted Modeling of Financial Inclusion in Tanzania",
    "section": "C.1 Respondent Age",
    "text": "C.1 Respondent Age\nThe age variable in the survey data is the only numeric variable retained. We can use the following code to produce a histogram to show the distribution of values of this variable. We use ggplot package to produce the chart, but we include the central measures– the mean, median, mode, as captions for additional insights.\n\nggplot(fstz23_sf, aes(x = age)) +\n  geom_histogram(binwidth = 5, fill = \"#4A90E2\", color = \"black\") +\n  labs(title = \"Age Distribution of Respondents\",\n       x = \"Respondent Age\",\n       y = \"Number of Respondents\",\n       caption = paste(\"Mean =\", round(mean(fstz23_sf$age, na.rm = TRUE), 1), \n                       \", Median =\", round(median(fstz23_sf$age, na.rm = TRUE), 1), \n                       \", Mode =\", as.numeric(names(sort(table(fstz23_sf$age), decreasing = TRUE)[1])))) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    axis.title.x = element_text(size = 12),\n    axis.title.y = element_text(size = 12),\n    axis.text = element_text(size = 10)\n  )\n\n\n\n\n\n\n\n\nWe can also display the summary statistics using the summary() function.\n\nsummary(fstz23_sf$age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  16.00   27.00   37.00   39.68   50.00  100.00 \n\n\nThe outputs show that the ages range from 16 to 100 and is right skewed. The distribution has a mean of ~40yrs and a mode of 30 yrs. Given the shape of the distribution, we can consider scaled versions of the variable when we calibrate the model later."
  },
  {
    "objectID": "posts/tz-fin-2023/index.html#c.2-financial-inclusion-measures",
    "href": "posts/tz-fin-2023/index.html#c.2-financial-inclusion-measures",
    "title": "Geographically Weighted Modeling of Financial Inclusion in Tanzania",
    "section": "C.2 Financial Inclusion Measures",
    "text": "C.2 Financial Inclusion Measures\nWe currently have four variables that give an indication of whether the respondent is financially included. For this study, we want to limit to one, or at most two variables. We expect that some of the variables are highly correlated, while some will perform much better in a model than others.\nWe first check the overall distribution or proportion of respondents across these four measures. We use ggplot package to create a bar chart to show the proportion of respondents achieving financial inclusion based on each dimension. The first part of the code computes for the proportion numbers as plotting the data directly will result in counts rather than percentages.\n\n# Calculate the proportions for each variable\nproportions &lt;-  st_drop_geometry(fstz23_sf) %&gt;%\n  summarise(\n    fi_banked = mean(fi_banked, na.rm = TRUE) * 100,\n    fi_formal = mean(fi_formal, na.rm = TRUE) * 100,\n    fi_informal = mean(fi_informal, na.rm = TRUE) * 100,\n    fi_overall = mean(fi_overall, na.rm = TRUE) * 100\n  )\n\n# Convert the proportions to a long format for ggplot2\nproportions_long &lt;- proportions %&gt;%\n  pivot_longer(cols = everything(), names_to = \"variable\", values_to = \"value\")\n\n# Create the bar chart\nggplot(proportions_long, aes(x = variable, y = value, fill = variable)) +\n  geom_bar(stat = \"identity\", color = \"black\") +\n  geom_text(aes(label = round(value, 1)), vjust = -0.5, size = 4) +\n  scale_y_continuous(labels = scales::percent_format(scale = 1)) +\n  labs(title = \"Proportion of Respondents for Financial Inclusion Variables\",\n       x = \"\",\n       y = \"Percentage\",\n       fill = \"Variable\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    axis.title.y = element_text(size = 12),\n    axis.text = element_text(size = 10),\n    legend.position = \"none\"\n  )\n\n\n\n\n\n\n\n\nThe output shows that only 20.5% of the respondents are banked, the lowest across the three main dimensions. It also shows that the difference between formal and overall financial inclusion is a difference of 7%. This means that only 7% of respondents are banked or using informal FI instruments, but are not using formal instruments.\nWe can visualize the correlation based on overlapping values across the three dimensions. Visually, overlaps can be visualized using Venn diagrams, but we can also use an upset chart from UpSetR package. This visualization is more scalable than venn diagrams, which is not really an issue since we only have three categories. This chart makes it much easier to compare intersections and non-intersections against each other.\nIn the code chunk below, we load the UpSetR package, and then prepare the data so that we only have the three variables. The preparation ensures that the variables are in 0-1 integers which is the required format for the function. We then use upset() from UpSetR package to produce the chart by passing the data and defining the variables to be plotted.\n\nlibrary(UpSetR)\n\n# Create a binary dataframe\noverlap_data &lt;- as.data.frame(st_drop_geometry(fstz23_sf)) %&gt;%\n  mutate(\n    Banked = as.integer(fi_banked == 1),\n    Formal = as.integer(fi_formal == 1),\n    Informal = as.integer(fi_informal == 1)\n  ) %&gt;%\n  select(Banked, Formal, Informal)\n\n# Create the UpSet plot\nupset(overlap_data, sets = c(\"Banked\", \"Formal\", \"Informal\"),\n      keep.order = TRUE, order.by = \"freq\", number.angles = 45,\n      main.bar.color = \"blue\", sets.bar.color = \"red\",\n      text.scale = c(1.5, 1.5, 1.5, 1, 1.5, 1.5),\n      mainbar.y.label = \"Intersection Size\", sets.x.label = \"Set Size\")\n\n\n\n\n\n\n\n\nThe resulting chart shows that:\n\nAll banked respondents are also financial included based on formal instruments. (bank is a subset or category under formal instruments)\nThere are 663 respondents (~7%) that are financially included based on informal instruments, but not based on formal instruments\nThere are 2812 respondents (~28%) that are financially included based on formal instruments, but not based on informal instruments\nThere are 3836 respondents (~38%) that are financially included based on both formal and informal instruments\n\nWe do not have enough to already exclude any of these variables, but we expect that the overall measure covers too much of the sample to be predictable. We also expect that the informal FI measure has too much overlap with formal so we would likely choose one but not the other."
  },
  {
    "objectID": "posts/tz-fin-2023/index.html#c.3-correlation-of-predictors",
    "href": "posts/tz-fin-2023/index.html#c.3-correlation-of-predictors",
    "title": "Geographically Weighted Modeling of Financial Inclusion in Tanzania",
    "section": "C.3 Correlation of predictors",
    "text": "C.3 Correlation of predictors\nWe can check if any of the predictors are highly correlated as the precence of autocorrelation affects the performance and interpretation of the model.\nWe can do this by producing the correlation plot of the potential predictor variables. We use ggcorrmat() of ggstatsplot package to produce this. We pass a dataframe with just the predictor variables and the code will output a diagonal matrix with the correlation coeficients between each pair of variables.\n\nggcorrmat(select(st_drop_geometry(fstz23_sf),\n                 -c(region, district, ward, maritalstatus, education, land_own, income_source,\n                    fi_banked, fi_formal, fi_informal, fi_overall)))\n\n\n\n\n\n\n\n\nThe code outputs a large matrix, but we only need to focus on pairs where the correlation coefficient is high. (i.e., &gt; 0.8) Those are:\n\nage and age_standardized = 1 - This is expected as one is just a transformation. We will keep them both for now as we want to see which one will result to a better model.\nany_impaired and visual_impaired = 0.81 - This is also expected as any_impaired is a derived variable. It looks like most of the impairment reported is visual in nature. We will then drop the derived variable\nincome_salaried and reg_job = 0.95 - This appears to be redundant variables and likely refer to the same condition. We should be able to drop the latter\nproduction and income_farm_and_fish = 0.71 - while not as high as the last pair, this pair most likely also refers to the same type of work. We will follow the same approach so we only keep the variables prefixed by income in the calibration\n\nBased on those, we can clean up our dataset by dropping the three variables mentioned above. We can also drop the variables we don’t need which include the categorical variables that we already have created new variables for. We perform this with the use of the select() function and using a “-” to exclude rather than select columns.\n\nfstz23_sf &lt;- select(fstz23_sf,-c(any_impaired, reg_job, production,\n                                 maritalstatus, education, land_own, income_source))"
  },
  {
    "objectID": "posts/tz-fin-2023/index.html#d.1-global-models-without-variable-selection",
    "href": "posts/tz-fin-2023/index.html#d.1-global-models-without-variable-selection",
    "title": "Geographically Weighted Modeling of Financial Inclusion in Tanzania",
    "section": "D.1 Global Models without Variable Selection",
    "text": "D.1 Global Models without Variable Selection\nTo calibrate the model, we use glm() which calibrates generalised linear models. As the dependent variable is binary, we need to make sure that the model used is a logistic regression model. This is done by passing the value “binomial” to the family argument.\nWe will run this with all variables for all four FI measures to see if any of them are performing very well or very poorly against the others. We will then focus on finetuning and then preparing the geographically weighted models on those variables that can be best explained with this approach.\n\nD.1.1 Global Model for Formal FI (no variable selection)\nThe code below calibrates a model with all variables with fi_formal as the dependent variable. We then use summary() to output the results. While the results display the AIC as a measure, we also compute for the reduction in variance due to the predictors. This is done by comparing the deviance and the null deviance. A higher number means that the variables had a larger contribution in predicting or explaining the value of the dependent variable.\n\nfi_formal.lr &lt;- glm(fi_formal ~ urban + age + female + head_hh + visual_impaired + hearing_impaired + comm_impaired +\n                      move_impaired + daily_impaired + cogn_impaired + agricultural + mobile + internet +\n                      own_mobile + has_id + no_income + is_married + was_married+\n                      educ_primary + educ_secondary + educ_tertiary +\n                      land_self_own + land_hh_or_shared + land_rented +\n                      income_farm_and_fish + income_piecework + income_dependent + income_trader + income_salaried +\n                      income_other,\n                    family = \"binomial\",\n                    data = fstz23_sf)\nsummary(fi_formal.lr)\n\n\nCall:\nglm(formula = fi_formal ~ urban + age + female + head_hh + visual_impaired + \n    hearing_impaired + comm_impaired + move_impaired + daily_impaired + \n    cogn_impaired + agricultural + mobile + internet + own_mobile + \n    has_id + no_income + is_married + was_married + educ_primary + \n    educ_secondary + educ_tertiary + land_self_own + land_hh_or_shared + \n    land_rented + income_farm_and_fish + income_piecework + income_dependent + \n    income_trader + income_salaried + income_other, family = \"binomial\", \n    data = fstz23_sf)\n\nCoefficients:\n                      Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)          -0.909394   0.313346  -2.902 0.003705 ** \nurban                 0.632501   0.080982   7.810 5.70e-15 ***\nage                   0.005744   0.002538   2.263 0.023640 *  \nfemale                0.046569   0.074759   0.623 0.533335    \nhead_hh               0.178742   0.087275   2.048 0.040557 *  \nvisual_impaired      -0.024436   0.095515  -0.256 0.798080    \nhearing_impaired     -0.291409   0.158199  -1.842 0.065469 .  \ncomm_impaired         0.275147   0.337988   0.814 0.415603    \nmove_impaired         0.072813   0.122261   0.596 0.551477    \ndaily_impaired        0.111153   0.201332   0.552 0.580887    \ncogn_impaired         0.144262   0.159558   0.904 0.365924    \nagricultural          0.030786   0.084129   0.366 0.714411    \nmobile                0.549352   0.082138   6.688 2.26e-11 ***\ninternet              0.579452   0.081901   7.075 1.49e-12 ***\nown_mobile            1.902444   0.070026  27.168  &lt; 2e-16 ***\nhas_id               -0.675854   0.089887  -7.519 5.52e-14 ***\nno_income            -0.583239   0.122630  -4.756 1.97e-06 ***\nis_married           -0.034435   0.098156  -0.351 0.725726    \nwas_married          -0.194584   0.134307  -1.449 0.147392    \neduc_primary          0.663236   0.064346  10.307  &lt; 2e-16 ***\neduc_secondary        1.492458   0.128363  11.627  &lt; 2e-16 ***\neduc_tertiary         2.714657   0.723016   3.755 0.000174 ***\nland_self_own         0.170538   0.095351   1.789 0.073690 .  \nland_hh_or_shared     0.081153   0.085445   0.950 0.342232    \nland_rented           0.375301   0.137679   2.726 0.006412 ** \nincome_farm_and_fish -0.692323   0.268336  -2.580 0.009878 ** \nincome_piecework     -0.791856   0.269060  -2.943 0.003250 ** \nincome_dependent     -0.874045   0.275270  -3.175 0.001497 ** \nincome_trader        -0.026406   0.300939  -0.088 0.930079    \nincome_salaried      -0.157785   0.344056  -0.459 0.646518    \nincome_other         -0.660914   0.403607  -1.638 0.101522    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 11074.6  on 9907  degrees of freedom\nResidual deviance:  7563.1  on 9877  degrees of freedom\nAIC: 7625.1\n\nNumber of Fisher Scoring iterations: 7\n\n1 - summary(fi_formal.lr)$deviance / summary(fi_formal.lr)$null.deviance \n\n[1] 0.3170735\n\n\n\n\nD.1.2 Global Model for Banked FI (no variable selection)\nThe code below calibrates a model with all variables with fi_banked as the dependent variable.\n\nfi_banked.lr &lt;- glm(fi_banked ~ urban + age + female + head_hh + visual_impaired + hearing_impaired + comm_impaired +\n                      move_impaired + daily_impaired + cogn_impaired + agricultural + mobile + internet +\n                      own_mobile + has_id + no_income + is_married + was_married+\n                      educ_primary + educ_secondary + educ_tertiary +\n                      land_self_own + land_hh_or_shared + land_rented +\n                      income_farm_and_fish + income_piecework + income_dependent + income_trader + income_salaried +\n                      income_other,\n                    family = \"binomial\",\n                    data = fstz23_sf)\nsummary(fi_banked.lr)\n\n\nCall:\nglm(formula = fi_banked ~ urban + age + female + head_hh + visual_impaired + \n    hearing_impaired + comm_impaired + move_impaired + daily_impaired + \n    cogn_impaired + agricultural + mobile + internet + own_mobile + \n    has_id + no_income + is_married + was_married + educ_primary + \n    educ_secondary + educ_tertiary + land_self_own + land_hh_or_shared + \n    land_rented + income_farm_and_fish + income_piecework + income_dependent + \n    income_trader + income_salaried + income_other, family = \"binomial\", \n    data = fstz23_sf)\n\nCoefficients:\n                      Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)          -2.037948   0.289783  -7.033 2.03e-12 ***\nurban                 0.487211   0.070234   6.937 4.01e-12 ***\nage                   0.008988   0.002742   3.278 0.001046 ** \nfemale               -0.244823   0.073536  -3.329 0.000871 ***\nhead_hh               0.429123   0.084337   5.088 3.62e-07 ***\nvisual_impaired       0.220218   0.092799   2.373 0.017641 *  \nhearing_impaired     -0.545221   0.215985  -2.524 0.011592 *  \ncomm_impaired         0.168385   0.482018   0.349 0.726839    \nmove_impaired        -0.322852   0.143402  -2.251 0.024361 *  \ndaily_impaired        0.103399   0.261231   0.396 0.692242    \ncogn_impaired        -0.163345   0.189727  -0.861 0.389267    \nagricultural         -0.177993   0.076526  -2.326 0.020024 *  \nmobile               -0.054909   0.136147  -0.403 0.686723    \ninternet              0.628532   0.063778   9.855  &lt; 2e-16 ***\nown_mobile            0.856049   0.122809   6.971 3.16e-12 ***\nhas_id               -1.180586   0.146446  -8.062 7.53e-16 ***\nno_income            -0.091093   0.158109  -0.576 0.564522    \nis_married           -0.082966   0.090487  -0.917 0.359204    \nwas_married          -0.397742   0.126719  -3.139 0.001697 ** \neduc_primary          0.796899   0.086567   9.206  &lt; 2e-16 ***\neduc_secondary        1.700194   0.107407  15.829  &lt; 2e-16 ***\neduc_tertiary         3.121148   0.208886  14.942  &lt; 2e-16 ***\nland_self_own         0.272392   0.094898   2.870 0.004100 ** \nland_hh_or_shared     0.162165   0.093024   1.743 0.081288 .  \nland_rented           0.190437   0.108746   1.751 0.079909 .  \nincome_farm_and_fish -1.599462   0.207346  -7.714 1.22e-14 ***\nincome_piecework     -1.950615   0.209724  -9.301  &lt; 2e-16 ***\nincome_dependent     -1.893223   0.225727  -8.387  &lt; 2e-16 ***\nincome_trader        -1.546829   0.219310  -7.053 1.75e-12 ***\nincome_salaried      -0.644514   0.226550  -2.845 0.004442 ** \nincome_other         -1.345573   0.301468  -4.463 8.07e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 10056.8  on 9907  degrees of freedom\nResidual deviance:  7550.3  on 9877  degrees of freedom\nAIC: 7612.3\n\nNumber of Fisher Scoring iterations: 6\n\n1 - summary(fi_banked.lr)$deviance / summary(fi_banked.lr)$null.deviance \n\n[1] 0.2492412\n\n\n\n\nD.1.3 Global Model for Informal FI (no variable selection)\nThe code below calibrates a model with all variables with fi_informal as the dependent variable.\n\nfi_informal.lr &lt;- glm(fi_informal ~ urban + age + female + head_hh + visual_impaired + hearing_impaired + comm_impaired +\n                      move_impaired + daily_impaired + cogn_impaired + agricultural + mobile + internet +\n                      own_mobile + has_id + no_income + is_married + was_married+\n                      educ_primary + educ_secondary + educ_tertiary +\n                      land_self_own + land_hh_or_shared + land_rented +\n                      income_farm_and_fish + income_piecework + income_dependent + income_trader + income_salaried +\n                      income_other,\n                    family = \"binomial\",\n                    data = fstz23_sf)\nsummary(fi_informal.lr)\n\n\nCall:\nglm(formula = fi_informal ~ urban + age + female + head_hh + \n    visual_impaired + hearing_impaired + comm_impaired + move_impaired + \n    daily_impaired + cogn_impaired + agricultural + mobile + \n    internet + own_mobile + has_id + no_income + is_married + \n    was_married + educ_primary + educ_secondary + educ_tertiary + \n    land_self_own + land_hh_or_shared + land_rented + income_farm_and_fish + \n    income_piecework + income_dependent + income_trader + income_salaried + \n    income_other, family = \"binomial\", data = fstz23_sf)\n\nCoefficients:\n                     Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)          -0.85492    0.22360  -3.823 0.000132 ***\nurban                 0.06146    0.05511   1.115 0.264748    \nage                  -0.01826    0.00197  -9.270  &lt; 2e-16 ***\nfemale                0.09939    0.05470   1.817 0.069206 .  \nhead_hh               0.50994    0.06308   8.084 6.29e-16 ***\nvisual_impaired       0.17875    0.07094   2.520 0.011748 *  \nhearing_impaired      0.03574    0.13339   0.268 0.788769    \ncomm_impaired        -0.33005    0.30810  -1.071 0.284058    \nmove_impaired         0.05833    0.09647   0.605 0.545371    \ndaily_impaired       -0.19622    0.17527  -1.120 0.262915    \ncogn_impaired         0.09270    0.12797   0.724 0.468830    \nagricultural          0.10476    0.05926   1.768 0.077070 .  \nmobile                0.43162    0.07583   5.691 1.26e-08 ***\ninternet              0.40287    0.05307   7.591 3.19e-14 ***\nown_mobile            0.31775    0.06393   4.970 6.68e-07 ***\nhas_id               -0.46332    0.07398  -6.263 3.77e-10 ***\nno_income            -0.52889    0.10286  -5.142 2.72e-07 ***\nis_married            0.52809    0.06802   7.764 8.24e-15 ***\nwas_married           0.21085    0.09467   2.227 0.025933 *  \neduc_primary          0.20200    0.05155   3.919 8.90e-05 ***\neduc_secondary        0.39907    0.07977   5.003 5.64e-07 ***\neduc_tertiary         0.63482    0.16032   3.960 7.50e-05 ***\nland_self_own        -0.18105    0.06966  -2.599 0.009349 ** \nland_hh_or_shared     0.13379    0.06473   2.067 0.038751 *  \nland_rented           0.02514    0.08747   0.287 0.773773    \nincome_farm_and_fish  0.20099    0.17616   1.141 0.253888    \nincome_piecework      0.13475    0.17689   0.762 0.446201    \nincome_dependent     -0.20958    0.18466  -1.135 0.256397    \nincome_trader         0.45674    0.18825   2.426 0.015254 *  \nincome_salaried       0.42832    0.19652   2.180 0.029291 *  \nincome_other          0.32673    0.26233   1.246 0.212942    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 13683  on 9907  degrees of freedom\nResidual deviance: 12417  on 9877  degrees of freedom\nAIC: 12479\n\nNumber of Fisher Scoring iterations: 4\n\n1 - summary(fi_informal.lr)$deviance / summary(fi_informal.lr)$null.deviance \n\n[1] 0.09256485\n\n\n\n\nD.1.4 Global Model for Overall FI (no variable selection)\nThe code below calibrates a model with all variables with fi_overall as the dependent variable.\n\nfi_overall.lr &lt;- glm(fi_overall ~ urban + age + female + head_hh + visual_impaired + hearing_impaired + comm_impaired +\n                      move_impaired + daily_impaired + cogn_impaired + agricultural + mobile + internet +\n                      own_mobile + has_id + no_income + is_married + was_married+\n                      educ_primary + educ_secondary + educ_tertiary +\n                      land_self_own + land_hh_or_shared + land_rented +\n                      income_farm_and_fish + income_piecework + income_dependent + income_trader + income_salaried +\n                      income_other,\n                    family = \"binomial\",\n                    data = fstz23_sf)\nsummary(fi_overall.lr)\n\n\nCall:\nglm(formula = fi_overall ~ urban + age + female + head_hh + visual_impaired + \n    hearing_impaired + comm_impaired + move_impaired + daily_impaired + \n    cogn_impaired + agricultural + mobile + internet + own_mobile + \n    has_id + no_income + is_married + was_married + educ_primary + \n    educ_secondary + educ_tertiary + land_self_own + land_hh_or_shared + \n    land_rented + income_farm_and_fish + income_piecework + income_dependent + \n    income_trader + income_salaried + income_other, family = \"binomial\", \n    data = fstz23_sf)\n\nCoefficients:\n                      Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)          -0.055692   0.333888  -0.167 0.867527    \nurban                 0.516681   0.086899   5.946 2.75e-09 ***\nage                  -0.001405   0.002650  -0.530 0.595996    \nfemale                0.024066   0.078557   0.306 0.759332    \nhead_hh               0.235402   0.093558   2.516 0.011866 *  \nvisual_impaired       0.150577   0.103060   1.461 0.144000    \nhearing_impaired     -0.114633   0.163713  -0.700 0.483798    \ncomm_impaired         0.033323   0.328379   0.101 0.919171    \nmove_impaired         0.007001   0.126653   0.055 0.955920    \ndaily_impaired       -0.002848   0.200755  -0.014 0.988681    \ncogn_impaired         0.204732   0.165965   1.234 0.217357    \nagricultural          0.043097   0.087983   0.490 0.624249    \nmobile                0.517871   0.081228   6.376 1.82e-10 ***\ninternet              0.554273   0.091495   6.058 1.38e-09 ***\nown_mobile            1.524108   0.074932  20.340  &lt; 2e-16 ***\nhas_id               -0.592184   0.090867  -6.517 7.17e-11 ***\nno_income            -0.620211   0.119034  -5.210 1.88e-07 ***\nis_married            0.254598   0.101106   2.518 0.011798 *  \nwas_married           0.096682   0.140398   0.689 0.491059    \neduc_primary          0.563723   0.068504   8.229  &lt; 2e-16 ***\neduc_secondary        1.393584   0.141706   9.834  &lt; 2e-16 ***\neduc_tertiary         2.338111   0.722148   3.238 0.001205 ** \nland_self_own         0.108214   0.101402   1.067 0.285892    \nland_hh_or_shared     0.210526   0.088977   2.366 0.017979 *  \nland_rented           0.319084   0.146909   2.172 0.029857 *  \nincome_farm_and_fish -0.616538   0.290245  -2.124 0.033654 *  \nincome_piecework     -0.684558   0.290886  -2.353 0.018605 *  \nincome_dependent     -1.042797   0.295604  -3.528 0.000419 ***\nincome_trader        -0.036161   0.329672  -0.110 0.912657    \nincome_salaried      -0.187959   0.374037  -0.503 0.615305    \nincome_other         -0.262658   0.473406  -0.555 0.579014    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 9339.8  on 9907  degrees of freedom\nResidual deviance: 6817.2  on 9877  degrees of freedom\nAIC: 6879.2\n\nNumber of Fisher Scoring iterations: 7\n\n1 - summary(fi_overall.lr)$deviance / summary(fi_overall.lr)$null.deviance\n\n[1] 0.2700872\n\n\n\n\nD.1.5 Choosing a dependent variable based on global model calibration\nThe different models showed that the one using fi_formal and fi_overall as the dependent variable performed better than the other two. Since fi_formal gave the best reduction in deviance, and we have seen earlier that the other fi measures overlap with fi_formal anyway, we will focus on fi_formal as the main indicator for which we will finetune the model."
  },
  {
    "objectID": "posts/tz-fin-2023/index.html#d.2-variable-selection-for-the-global-model",
    "href": "posts/tz-fin-2023/index.html#d.2-variable-selection-for-the-global-model",
    "title": "Geographically Weighted Modeling of Financial Inclusion in Tanzania",
    "section": "D.2 Variable selection for the global model",
    "text": "D.2 Variable selection for the global model\nFrom the model results, we see that not all the variables contribute in the same degree as the others. We can use the output to pick the significant variables or perform a technique like forward, backward or stepwise regression to select variables by introducing or removing them one at a time.\nForward regression can be done using ols_step_forward_p() of the olsrr package. The function takes in the full model and starts from an empty model and adds variables with the highest significance one at a time. This continues doing this as long as variables with significance less than the specified p-value can be added.\n\nfi_formal_fw_mlr &lt;- ols_step_forward_p(fi_formal.lr, p_val = 0.05, details = FALSE)\n\nWe can also display the results by calling the resulting object.\n\nfi_formal_fw_mlr\n\n\n                                      Stepwise Summary                                      \n------------------------------------------------------------------------------------------\nStep    Variable             AIC          SBC             SBIC            R2       Adj. R2 \n------------------------------------------------------------------------------------------\n 0      Base Model        11452.392    11466.795     -5643875908.953    0.00000    0.00000 \n 1      own_mobile         8001.162     8022.765    -11332019050.848    0.29427    0.29420 \n 2      has_id             7826.777     7855.582    -11742615711.514    0.30673    0.30659 \n 3      urban              7661.916     7697.921    -12144725901.394    0.31830    0.31810 \n 4      mobile             7569.414     7612.621    -12378471273.642    0.32478    0.32450 \n 5      internet           7492.544     7542.952    -12576969271.789    0.33013    0.32979 \n 6      no_income          7432.878     7490.487    -12734351633.418    0.33429    0.33388 \n 7      educ_secondary     7382.226     7447.036    -12870263096.019    0.33781    0.33735 \n 8      educ_primary       7285.271     7357.282    -13129780806.723    0.34439    0.34386 \n 9      educ_tertiary      7255.572     7334.784    -13213906836.776    0.34649    0.34589 \n 10     age                7243.236     7329.649    -13252042393.875    0.34743    0.34677 \n 11     income_trader      7234.121     7327.736    -13281646286.889    0.34817    0.34744 \n 12     head_hh            7230.229     7331.044    -13297292381.943    0.34855    0.34776 \n------------------------------------------------------------------------------------------\n\nFinal Model Output \n------------------\n\n                         Model Summary                           \n----------------------------------------------------------------\nR                       0.590       RMSE                  0.348 \nR-Squared               0.349       MSE                   0.121 \nAdj. R-Squared          0.348       Coef. Var            46.241 \nPred R-Squared          0.347       AIC                7230.229 \nMAE                     0.249       SBC                7331.044 \n----------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                                 ANOVA                                  \n-----------------------------------------------------------------------\n                Sum of                                                 \n               Squares          DF    Mean Square       F         Sig. \n-----------------------------------------------------------------------\nRegression     642.088          12         53.507    441.188    0.0000 \nResidual      1200.065        9895          0.121                      \nTotal         1842.153        9907                                     \n-----------------------------------------------------------------------\n\n                                    Parameter Estimates                                     \n-------------------------------------------------------------------------------------------\n         model      Beta    Std. Error    Std. Beta      t        Sig      lower     upper \n-------------------------------------------------------------------------------------------\n   (Intercept)     0.236         0.017                 14.240    0.000     0.204     0.269 \n    own_mobile     0.388         0.011        0.388    36.683    0.000     0.367     0.408 \n        has_id    -0.114         0.011       -0.094    -9.947    0.000    -0.136    -0.091 \n         urban     0.061         0.008        0.067     7.537    0.000     0.045     0.077 \n        mobile     0.108         0.012        0.088     8.912    0.000     0.084     0.132 \n      internet     0.053         0.009        0.055     6.158    0.000     0.036     0.069 \n     no_income    -0.098         0.013       -0.066    -7.601    0.000    -0.123    -0.072 \neduc_secondary     0.162         0.013        0.133    12.745    0.000     0.137     0.187 \n  educ_primary     0.099         0.009        0.115    11.673    0.000     0.083     0.116 \n educ_tertiary     0.134         0.023        0.053     5.924    0.000     0.090     0.179 \n           age     0.001         0.000        0.028     2.850    0.004     0.000     0.001 \n income_trader     0.042         0.013        0.028     3.321    0.001     0.017     0.067 \n       head_hh     0.019         0.008        0.022     2.426    0.015     0.004     0.035 \n-------------------------------------------------------------------------------------------\n\n\nThe results show that the global model for fi_formal includes 12 explanatory variables which consist of variables for:\n\nMobile phone usage and ownership and internet access (3 variables) - has the highest combined weight\nEducation (3 variables)\nOther positive coefficient: urban, age, trader, head_hh\nNegative coefficients: no source of income, no id\n\nThe last variable is surprising as it implies that having an id is linked to a lower probability of being financially included. The model did not pick up gender which means that it doesn’t see a clear distinction between males and females for this dimension of financial inclusion."
  },
  {
    "objectID": "posts/tz-fin-2023/index.html#d.3-testing-for-spatial-autocorrelation",
    "href": "posts/tz-fin-2023/index.html#d.3-testing-for-spatial-autocorrelation",
    "title": "Geographically Weighted Modeling of Financial Inclusion in Tanzania",
    "section": "D.3 Testing for spatial autocorrelation",
    "text": "D.3 Testing for spatial autocorrelation\nBefore calibrating the geographically weighted model, we can check if there is a pattern linking global model performance and the respondent’s location. One way to do this is to plot the residuals or error and check for any patterns.\nThe first step is to export relevant output from the model as a dataframe. We just extract the residuals by referencing the model object in the results.\n\nmlr_output &lt;- as.data.frame(fi_formal_fw_mlr$model$residuals) %&gt;%\n  rename('FW_MLR_RES' = 'fi_formal_fw_mlr$model$residuals')\n\nWe then import these into fstz23_sf as a new variable MLR_RES using cbind(). This function adds the dataframe as new columns in their current order.\n\nfstz23_sf &lt;- cbind(fstz23_sf,\n                         mlr_output$FW_MLR_RES) %&gt;%\n  rename('MLR_RES' = 'mlr_output.FW_MLR_RES')\n\nWe remember that our respondent data only have the district s their location so dissplaying them as points might not be very meaningful or accurate. We can instead plot the residuals at a district level to see if there are any patterns arising from there.\nTo do this, we first compute for the average residual at a district level by using group_by() to summarise the object by district and then define the aggregate function using summarise(). We use st_drop_geometry() so that the geometry column is dropped and we only have the district name and the average residual value.\n\navg_res_df &lt;- st_drop_geometry(fstz23_sf) %&gt;%\n  group_by(district) %&gt;%\n  summarise(avg_res = mean(MLR_RES, na.rm = TRUE))\n\nhead(avg_res_df)\n\n# A tibble: 6 × 2\n  district                 avg_res\n  &lt;chr&gt;                      &lt;dbl&gt;\n1 Arusha                    0.105 \n2 Arusha Urban              0.0168\n3 Babati                   -0.0522\n4 Babati UrbanBabati Urban -0.0185\n5 Bagamoyo                  0.0395\n6 Bahi                     -0.179 \n\n\nWe then export these average residual values into the TZ boundary map by using left_join() on the district name.\n\ntz_dist_stat &lt;- tz_dist_stat %&gt;%\n  left_join(avg_res_df, by= \"district\")\n\nWe can now use tmap package to visually display the average residual value per district. We again use two layers, and then exclude any districts where there is no residuals by using a mask with !is.na()\n\ntm_shape(tz_dist)+\n  tm_polygons(\"grey\") +\ntm_shape(tz_dist_stat[!is.na(tz_dist_stat$avg_res),]) +\n  tm_polygons(\"avg_res\", title = \"Average Residuals\")\n\nVariable(s) \"avg_res\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\n\n\n\n\nThe map shows some apparent clusters with positive (average) residuals and some with negative residuals. A large number of districts have relatively low average residuals between -0.1 and 0.1.\nWe can confirm this observation by running global Moran’s I test on the average residual value. In order to do this, we first need to compute for the neighbors and the weights for each of the district. We use st_knn() to derive neighbors using knn method with a parameter of 6 neighbors, and then use equal weights for neighbors using the st_weights() function. We perform these functions on the centroids object as these methods work on points rather than shapes.\n\ntz_dist_centroids &lt;- tz_dist_centroids %&gt;%\n  mutate(nb = st_knn(geometry, k = 6,\n                     longlat = FALSE),\n         wt = st_weights(nb,\n                         style = \"W\"),\n         .before = 1)\n\nWe then run the Moran’s I test with permutations using global_moran_perm() from sfdep package. Note that we use the avg_res from the map object but have kept the neighbor list and weights in the centrodis object. The nsim argument indicates that we are running 10 simulations for this test. We also replace any na values with zero as the code will not work with any na values.\n\nset.seed(1234)\nglobal_moran_perm(replace_na(tz_dist_stat$avg_res,0),\n                  tz_dist_centroids$nb,\n                  tz_dist_centroids$wt,\n                  alternative = \"two.sided\",\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.19584, observed rank = 100, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\nAs the results are significant, the test confirms that there is spatial autocorrelation for the average residual values across districts. The positive test statistic confirms our observation that the pattern is that of clustering. We should then build geographically weighted models as they are likely to produce better results as we account for the respondents’ locations."
  },
  {
    "objectID": "posts/tz-fin-2023/index.html#e.1-computing-a-bandwidth",
    "href": "posts/tz-fin-2023/index.html#e.1-computing-a-bandwidth",
    "title": "Geographically Weighted Modeling of Financial Inclusion in Tanzania",
    "section": "E.1 Computing a bandwidth",
    "text": "E.1 Computing a bandwidth\nThe first step in calibrating a geographically weighted model is determining the bandwidth to use. The choice can either be a fixed bandwidth which is based on distance, or an adaptive bandwidth which is based on the number of neighbors. For our case, we will opt for a fixed bandwidth since we have not precisely mapped the locations of the respondents, and there is a wide range of values for the number of respondents per district. A fixed bandwidth is likely to ensure that it captures most, if not all, of the points in the same district and also some in neighboring districts.\nTo compute for the optimum fixed bandwidth, we use bw.ggwr() of GWModel package. The approach argument defines the stopping rule to be used, which is cross validation in this case. Setting the adaptive argument to FALSE indicates that we are computing for the fixed bandwidth. Like the global model, we indicate binomial for the family argument to specify we are calibrating a logistic regression model.\nThe first part of the function is the formula for the model. To be sure of the details to put in here, one may use the formula() function on the model object of the forward regression output to display the final formula.\n\nbw_fixed &lt;- bw.ggwr(formula = fi_formal ~ own_mobile + has_id + urban + mobile + internet +\n                     no_income + educ_secondary + educ_primary + educ_tertiary +\n                     age + income_trader + head_hh,\n                  data = fstz23_sf,\n                  family = \"binomial\",\n                  approach = \"CV\",\n                  kernel = \"gaussian\",\n                  adaptive = FALSE,\n                  longlat = FALSE)\n\n# To display the global model's formula, you may use\n# formula(fi_formal_fw_mlr$model)\n\nThe output shows a recommended bandwidth of ~99.5km should be used. We save the output as an rds object to save our results and prevent the need to rerun the code again.\n\nwrite_rds(bw_fixed, \"data/rds/bw_fixed.rds\")"
  },
  {
    "objectID": "posts/tz-fin-2023/index.html#e.2-deriving-the-distance-matrix",
    "href": "posts/tz-fin-2023/index.html#e.2-deriving-the-distance-matrix",
    "title": "Geographically Weighted Modeling of Financial Inclusion in Tanzania",
    "section": "E.2 Deriving the distance matrix",
    "text": "E.2 Deriving the distance matrix\nCalibration of the logistics regression model requires a properly set up distance matrix. We use the code below which computes for this using gw.dist() on the coordinates of the data points. The coordinates function does not work on an sf dataframe so we convert it into Spatial format first.\n\ndistMAT &lt;- gw.dist(dp.locat=\n                     coordinates(as_Spatial(fstz23_sf)))"
  },
  {
    "objectID": "posts/tz-fin-2023/index.html#e.3-calibrating-the-fixed-bandwidth-model",
    "href": "posts/tz-fin-2023/index.html#e.3-calibrating-the-fixed-bandwidth-model",
    "title": "Geographically Weighted Modeling of Financial Inclusion in Tanzania",
    "section": "E.3 Calibrating the fixed bandwidth model",
    "text": "E.3 Calibrating the fixed bandwidth model\nWe can now calibrate the geographically weighted model using the computed bandwidthby using ggwr.basic() from GWModel. The function uses mostly the same arguments as the previous one, with the exception that the bandwidth now becomes an input here.\n\ngwr_fixed &lt;- ggwr.basic(formula = fi_formal ~ own_mobile + has_id + urban + mobile + internet +\n                     no_income + educ_secondary + educ_primary + educ_tertiary +\n                     age + income_trader + head_hh,\n                     data = fstz23_sf,\n                     family = \"binomial\",\n                     kernel = \"gaussian\",\n                     bw = bw_fixed,\n                     adaptive = FALSE,\n                     longlat = FALSE,\n                     dMat = distMAT)\n\nWe again save this object into an rds file to save our work for future runs.\n\nwrite_rds(gwr_fixed, \"data/rds/gwr_fixed.rds\")\n\nWe can show the results by calling the object as in the code chunk below.\n\ngwr_fixed\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2024-11-09 01:56:00.625357 \n   Call:\n   ggwr.basic(formula = fi_formal ~ own_mobile + has_id + urban + \n    mobile + internet + no_income + educ_secondary + educ_primary + \n    educ_tertiary + age + income_trader + head_hh, data = fstz23_sf, \n    bw = bw_fixed, family = \"binomial\", kernel = \"gaussian\", \n    adaptive = FALSE, longlat = FALSE, dMat = distMAT)\n\n   Dependent (y) variable:  fi_formal\n   Independent variables:  own_mobile has_id urban mobile internet no_income educ_secondary educ_primary educ_tertiary age income_trader head_hh\n   Number of data points: 9908\n   Used family: binomial\n   ***********************************************************************\n   *              Results of Generalized linear Regression               *\n   ***********************************************************************\n\nCall:\nNULL\n\nCoefficients:\n                Estimate Std. Error z value Pr(&gt;|z|)    \nIntercept      -1.524234   0.125637 -12.132  &lt; 2e-16 ***\nown_mobile      1.909171   0.069174  27.600  &lt; 2e-16 ***\nhas_id         -0.713992   0.083010  -8.601  &lt; 2e-16 ***\nurban           0.634941   0.074585   8.513  &lt; 2e-16 ***\nmobile          0.544885   0.081262   6.705 2.01e-11 ***\ninternet        0.599629   0.081309   7.375 1.65e-13 ***\nno_income      -0.713969   0.097385  -7.331 2.28e-13 ***\neduc_secondary  1.504240   0.125972  11.941  &lt; 2e-16 ***\neduc_primary    0.668065   0.063948  10.447  &lt; 2e-16 ***\neduc_tertiary   2.949785   0.719341   4.101 4.12e-05 ***\nage             0.005471   0.002060   2.655  0.00792 ** \nincome_trader   0.708805   0.148055   4.787 1.69e-06 ***\nhead_hh         0.192332   0.067661   2.843  0.00447 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 11074.6  on 9907  degrees of freedom\nResidual deviance:  7601.7  on 9895  degrees of freedom\nAIC: 7627.7\n\nNumber of Fisher Scoring iterations: 7\n\n\n AICc:  7627.753\n Pseudo R-square value:  0.31359\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Fixed bandwidth: 99550.75 \n   Regression points: the same locations as observations are used.\n   Distance metric: A distance matrix is specified for this model calibration.\n\n   ************Summary of Generalized GWR coefficient estimates:**********\n                        Min.    1st Qu.     Median    3rd Qu.   Max.\n   Intercept      -2.2553256 -1.7682889 -1.5998528 -1.3662353 0.2864\n   own_mobile      1.0280291  1.8126102  1.9268403  2.0854432 2.7884\n   has_id         -1.5130176 -0.9107411 -0.7352893 -0.6028037 0.4318\n   urban          -0.3774224  0.3679404  0.6370600  0.8034331 1.5909\n   mobile         -0.6754378  0.4583577  0.7072317  0.8739171 1.1907\n   internet       -0.4507688  0.3207454  0.6938241  0.8874637 1.2307\n   no_income      -1.6221173 -0.9028388 -0.7060301 -0.5112796 0.3790\n   educ_secondary  0.5492826  1.2938089  1.4288504  1.7448408 2.3738\n   educ_primary    0.1792122  0.4986158  0.6758896  0.8737680 1.2324\n   educ_tertiary   0.7005712  2.4681136  4.0382606  5.3483262 6.5277\n   age            -0.0341697  0.0025534  0.0057795  0.0108019 0.0220\n   income_trader   0.0135346  0.3952052  0.6642425  1.0400905 3.1913\n   head_hh        -0.3123380  0.0940548  0.1752028  0.2485868 0.4916\n   ************************Diagnostic information*************************\n   Number of data points: 9908 \n   GW Deviance: 7106.258 \n   AIC : 7548.682 \n   AICc : 7558.832 \n   Pseudo R-square value:  0.3583282 \n\n   ***********************************************************************\n   Program stops at: 2024-11-09 02:01:47.510664 \n\n\nThe output shows an improvement with the gleographically weighted model across the different performance measures. AICc improved from 7627.7 to 7558.832. The pseudo R-squared value improved from 0.31359 to 0.3583282.\nThe output also shows that the coefficient values vary widely across the local models. For some variables, there is also a change in sign across those value ranges– which implies that for some variables like mobile and internet access, they are detrimental to the respondent’s level of financial inclusion depending on their location.\nWe can analyze the gwr model further by accessing the details in the output."
  },
  {
    "objectID": "posts/tz-fin-2023/index.html#e.4-importing-gwr-model-results-into-an-sf-dataframe",
    "href": "posts/tz-fin-2023/index.html#e.4-importing-gwr-model-results-into-an-sf-dataframe",
    "title": "Geographically Weighted Modeling of Financial Inclusion in Tanzania",
    "section": "E.4 Importing gwr model results into an sf dataframe",
    "text": "E.4 Importing gwr model results into an sf dataframe\nThe gwr output includes details for every local model in an object called SDF. We can combine this with the geospatial information in order to be able to visualize the values of coefficients, residuals and fit measures at an individual location level. We use the following code chunk to extract SDF as a dataframe and then combine it with with fstz23_sf using cbind(). The code only retains the district names and the gwr model output by using select() to specify those columns.\n\ngwr_fixed_output &lt;- as.data.frame(gwr_fixed$SDF) %&gt;%\n  select(-c(geometry))\n\ngwr_sf_fixed &lt;- cbind(fstz23_sf, gwr_fixed_output) %&gt;%\n  select(2, (ncol(fstz23_sf)):(ncol(fstz23_sf)+ncol(gwr_fixed_output))) %&gt;%\n  st_drop_geometry()\n\nAs the local models might not be relevant on their own, we can summarize all of the model values by district and then visualize these variables by district rather than by individual data point. We use the following code chunk to compute for the average of each of the variables by district name using the following code chunk. The code uses group_by() to aggregate by district. Columns with the mean for each variable are then added by using across() and everything() to compute the mean for each variable.\n\ngwr_sf_fixed_by_dist &lt;- gwr_sf_fixed %&gt;%\n  group_by(district) %&gt;%\n  summarise(across(everything(), ~ mean(.x, na.rm = TRUE)))\n\nWe then import to the district boundary map using left_join() on district.\n\ntz_dist_stat &lt;- tz_dist_stat %&gt;%\n  left_join(gwr_sf_fixed_by_dist, by= \"district\")"
  },
  {
    "objectID": "posts/tz-fin-2023/index.html#e.5-visualizing-model-coefficients-and-metrics",
    "href": "posts/tz-fin-2023/index.html#e.5-visualizing-model-coefficients-and-metrics",
    "title": "Geographically Weighted Modeling of Financial Inclusion in Tanzania",
    "section": "E.5 Visualizing model coefficients and metrics",
    "text": "E.5 Visualizing model coefficients and metrics\nWe can visualize the patterns in the model coefficients or of other metrics with the updated sf dataframe.\n\nE.5.1 Visualizing model coefficients\nFor model coefficients, we focus on the following that have a wide range of values: has_id, mobile, internet, no_income, education (all levels), head_hh.\n\nWith IDMobile AccessInternet AccessLack of income sourcesEducationHeads of household\n\n\nThe code chunk below produces a choropleth map for the average coefficient of has_id in each district.\n\ntm_shape(tz_dist)+\n  tm_polygons(\"grey\") +\ntm_shape(tz_dist_stat[!is.na(tz_dist_stat$has_id.1),]) +\n  tm_polygons(\"has_id.1\", title = \"ß has_id\")\n\nVariable(s) \"has_id.1\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\n\n\n\n\nThe chart shows that the coefficient of has_id is negative across most of Tanzania. There are only a few districts where it is positive. This result seems surprising as ID’s may be treated as a way of getting access to financial services. There is likely more to this and the absence of an ID for some of the respondents might be link to another condition which is not appearing here. (e.g., those on welfare that do not have access to banking might all have IDs)\n\n\nThe code chunk below produces a choropleth map for the average coefficient of mobile in each district. We highlight districts with negative (average) coefficients with a red border.\n\ntm_shape(tz_dist)+\n  tm_polygons(\"grey\") +\ntm_shape(tz_dist_stat[!is.na(tz_dist_stat$mobile.1),]) +\n  tm_polygons(\"mobile.1\", title = \"ß mobile\") +\ntm_shape(tz_dist_stat[(!is.na(tz_dist_stat$mobile.1) & (tz_dist_stat$mobile.1) &lt; 0),]) +\n  tm_borders(\"red\")\n\nVariable(s) \"mobile.1\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\n\n\n\n\nThe output shows increased incidence of financial inclusion for respondents with access to a mobile phone. However, there is a cluster of districts on the northeast and another on the west that shows a negative correlation between mobile phone access and financial inclusion. This needs to be investigated as access is linked to access to mobile banking. These clusters might have low adoption, or limited access to such services which might be causing the results to appear as such.\n\n\nThe code chunk below produces a choropleth map for the average coefficient of internet in each district. We highlight districts with negative (average) coefficients with a red border.\n\ntm_shape(tz_dist)+\n  tm_polygons(\"grey\") +\ntm_shape(tz_dist_stat[!is.na(tz_dist_stat$internet.1),]) +\n  tm_polygons(\"internet.1\", title = \"ß internet\") +\ntm_shape(tz_dist_stat[(!is.na(tz_dist_stat$internet.1) & (tz_dist_stat$internet.1) &lt; 0),]) +\n  tm_borders(\"red\")\n\nVariable(s) \"internet.1\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\n\n\n\n\nWe see a similar state as mobile access. In general, probability of being financially included with internet access, but for some regions, this is not the case. There is a cluster in the north and in the west-southwest that are showing a negative correlation between financial inclusion and internet access. These also appear to be different regions districts compared to the ones for mobiel access. This needs to be investigated as to why internet access is not bringing higher levels of financial inclusion to these districts.\n\n\nThe code chunk below produces a choropleth map for the average coefficient of no_income in each district.\n\ntm_shape(tz_dist)+\n  tm_polygons(\"grey\") +\ntm_shape(tz_dist_stat[!is.na(tz_dist_stat$no_income.1),]) +\n  tm_polygons(\"no_income.1\", title = \"ß no_income\") +\ntm_shape(tz_dist_stat[(!is.na(tz_dist_stat$no_income.1) & (tz_dist_stat$no_income.1) &gt; 0),]) +\n  tm_borders(\"darkgreen\")\n\nVariable(s) \"no_income.1\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\n\n\n\n\nWith the exception of a cluster of five districts in the centre of Tanzania, the lack of a source of income, unsurprisingly, relates to higher probability of not being financially included. What we want to focus on are districts where the impact is higher. There is a cluster in the southwest, another in the north that appear to be more impacted. These might indicate more vulnerable populations– either a high degree of unemployment, or a much lower level (quality) of welfare or government support compared to the rest of the country.\n\n\nThe code chunk below produces a row of three choropleth maps for the average coefficient of each of the three education variables in each district. Each variable’s map is created using tmap and then stored in an object which are then displayed in a row using tmap_arrange().\n\nprim &lt;- tm_shape(tz_dist)+\n  tm_polygons(\"grey\") +\ntm_shape(tz_dist_stat[!is.na(tz_dist_stat$educ_primary.1),]) +\n  tm_polygons(\"educ_primary.1\", title = \"ß educ_primary\", palette = \"Greens\")\n\nsec &lt;- tm_shape(tz_dist)+\n  tm_polygons(\"grey\") +\ntm_shape(tz_dist_stat[!is.na(tz_dist_stat$educ_secondary.1),]) +\n  tm_polygons(\"educ_secondary.1\", title = \"ß educ_secondary\", palette = \"Greens\")\n\nter &lt;- tm_shape(tz_dist)+\n  tm_polygons(\"grey\") +\ntm_shape(tz_dist_stat[!is.na(tz_dist_stat$educ_tertiary.1),]) +\n  tm_polygons(\"educ_tertiary.1\", title = \"ß educ_tertiary\", palette = \"Greens\")\n\ntmap_arrange(prim, sec, ter, ncol = 3)\n\n\n\n\n\n\n\n\nThe output shows that education increases the chances of being financially included. The degree of impact also increases, generally, by the level of education.\n\n\nThe code chunk below produces a choropleth map for the average coefficient of head_hh in each district. We highlight districts with negative (average) coefficients with a red border.\n\ntm_shape(tz_dist)+\n  tm_polygons(\"grey\") +\ntm_shape(tz_dist_stat[!is.na(tz_dist_stat$head_hh.1),]) +\n  tm_polygons(\"head_hh.1\", title = \"ß head_hh\") +\ntm_shape(tz_dist_stat[(!is.na(tz_dist_stat$head_hh.1) & (tz_dist_stat$head_hh.1) &lt; 0),]) +\n  tm_borders(\"red\")\n\nVariable(s) \"head_hh.1\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\n\n\n\n\nFor most of the districts, it appears that the head of the household is more likely to be financially included (compared to non-heads of household) However, there are districts which are showing a negative coefficient. While there may be valid reasons for this (e.g., the head of household just stays home and might not be active in managing the funds) the aim is to make sure that no resident is excluded. Further investigation is needed if this can and needs to be addressed.\n\n\n\n\n\nE.5.2 Visualizing model residuals\nWe can also visualize the model residuals using the same approach. We can use the following code chunk to display the avrage residuals beside the level of financial inclusion in the district (which is simply the proportion of the indivudals that are financially included) to see if there is any pattern in the residuals and if they are related to the level of FI in a district.\n\ny &lt;- tm_shape(tz_dist)+\n  tm_polygons(\"grey\") +\ntm_shape(tz_dist_stat[!is.na(tz_dist_stat$y),]) +\n  tm_polygons(\"y\", title = \"Formal FI\", palette=\"-viridis\") +\ntm_shape(tz_dist_stat[!is.na(tz_dist_stat$y) & tz_dist_stat$y &lt; 0.4,]) +\n  tm_text(\"district\")\n\nres &lt;- tm_shape(tz_dist)+\n  tm_polygons(\"grey\") +\ntm_shape(tz_dist_stat[!is.na(tz_dist_stat$residual),]) +\n  tm_polygons(\"residual\", title = \"Residual\")\n\ntmap_arrange(y, res, ncol = 2)\n\nVariable(s) \"residual\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\n\n\n\n\nThe output shows that most of the districts that have the most negative residuals also are the ones that have low financial inclusion. (50% and below) This may indicate that the calibrated model, which was based on the global calibration, is not as appropriate for them. There may be other factors that explain the level of financial inclusion in these districts."
  }
]