[
  {
    "objectID": "posts/tz-fin-2023/index.html",
    "href": "posts/tz-fin-2023/index.html",
    "title": "Geographically Weighted Modeling of Financial Inclusion in Tanzania",
    "section": "",
    "text": "This was my final project for the course Geospatial Analysis. Here I looked at the information from FinscopeTanzania 2023 to model measures of financial inclusion both globally and geographically. In this project, I used geographically weighted logistics regression to see how the factors linked to financial inclusion vary across districts."
  },
  {
    "objectID": "posts/tz-fin-2023/index.html#a.1-background",
    "href": "posts/tz-fin-2023/index.html#a.1-background",
    "title": "Geographically Weighted Modeling of Financial Inclusion in Tanzania",
    "section": "A.1 Background",
    "text": "A.1 Background\nThe World Bank defines financial inclusion as the state of having access to useful and affordable financial products to meet one’s needs. Financial inclusion is an enabler to 7 of the Sustainable Development Goals, and is seen as the key enabler to reduce extreme poverty.\nOne key dimension of financial inclusion that the World Bank looked at in their latest Global Findex Database 2021 is the ownership of bank accounts for adults. In this report, 76% of the global adult population have their own accounts, but only 71% of the developing nations’ do. In some countries like Tanzania this number is even lower at 52%. Banking is just one traditional dimension. Other vehicles like mobile payments can bridge the gap in access to services for some of these nations.\nTanzania recognizes the importance of financial inclusion in promoting economic growth and with the Bank of Tanzania, the country’s central bank, the Microfinance Policy of 2000 was developed and focused on expanding financial services for low-income individuals.\nThe program behind financial inclusion has been structured from 2014 with the first National Financial Inclusion Framework for 2014-2016, with the latest version being the third framework for 2023-2028. While there has been significant progress, (e.g., access to financial services has risen from 42% in 2013 to 89% in 2023) the country continues to aim for inclusion for the whole population by increasing access, encouraging usage and enhancing the quality of financial services."
  },
  {
    "objectID": "posts/tz-fin-2023/index.html#a.2-objectives",
    "href": "posts/tz-fin-2023/index.html#a.2-objectives",
    "title": "Geographically Weighted Modeling of Financial Inclusion in Tanzania",
    "section": "A.2 Objectives",
    "text": "A.2 Objectives\nFinscope Tanzania 2023 is a public-private sector collaboration and aimed, among others, to understand and describe the financial behavior of individuals in the country and to establish an updated view of the level of financial inclusion across various measures. A large part of the findings is showing the change (improvements) of the overall measures against the previous 2017 report.\nThe objective of this study is to build on the Finscope Tanzania 2023 by identifying influential variables and identifying if geospatial factors influence the effect of those variables.\nIn order to satisfy this, the specific deliverables for the study will be:\n\nto build a global or non-spatial explanatory model for the level of financial inclusion across Tanzania;\nto build a geographically weighted explanatory model for the same response variables; and,\nto assess the advantage of the geographically weighted model and to analyse the geographically weighted model"
  },
  {
    "objectID": "posts/tz-fin-2023/index.html#a.3-data-sources",
    "href": "posts/tz-fin-2023/index.html#a.3-data-sources",
    "title": "Geographically Weighted Modeling of Financial Inclusion in Tanzania",
    "section": "A.3 Data Sources",
    "text": "A.3 Data Sources\nThe following data sources are used for this analysis:\n\nFinscope Tanzania 2023 individual survey data from Finscope Tanzania\n\nThe dataset is contained in a csv and translates the responses from 9,915 individuals who answered the survey\nThe respondents are all adults aged 16 years and above take across Tanzania\nThe dataset also includes derived fields which include different indicators for financial inclusion based on different criteria\n\nDistrict-level boundaries in Tanzania as a shapefile from geoBoundaries.org portal"
  },
  {
    "objectID": "posts/tz-fin-2023/index.html#a.4-importing-and-launching-r-packages",
    "href": "posts/tz-fin-2023/index.html#a.4-importing-and-launching-r-packages",
    "title": "Geographically Weighted Modeling of Financial Inclusion in Tanzania",
    "section": "A.4 Importing and Launching R Packages",
    "text": "A.4 Importing and Launching R Packages\nFor this study, the following R packages will be used. A description of the packages and the code, using p_load() of the pacman package, to import them is given below.\n\nPackage DescriptionImport Code\n\n\nThe loaded packages include:\n\nolsrr - for building OLS (ordinary least squares) regression models and performing diagnostic tests\nGWmodel - for calibrating geographically weighted family of models\ntmap - for plotting cartographic quality maps\nggstatsplot - for multivariate data visualization and analysis\nsf - spatial data handling\ntidyverse - attribute data handling\n\n\n\n\npacman::p_load(olsrr, sf, GWmodel, tmap, tidyverse, ggstatsplot, sfdep)"
  },
  {
    "objectID": "posts/tz-fin-2023/index.html#b.1-loading-tanzania-district-boundaries",
    "href": "posts/tz-fin-2023/index.html#b.1-loading-tanzania-district-boundaries",
    "title": "Geographically Weighted Modeling of Financial Inclusion in Tanzania",
    "section": "B.1 Loading Tanzania District boundaries",
    "text": "B.1 Loading Tanzania District boundaries\nWe load the district level boundaries in the following code chunk using st_read() and indicating the appropriate layer name. (i.e., the level 2 map) We also use rename() to already change the shapeName field to district to make it more understandable. We also project the map onto EPSG 32737 using st_transform() in order to be able to reference distances in terms of metres.\n\ntz_dist &lt;- st_read(dsn=\"data/geospatial\", \n                   layer=\"geoBoundaries-TZA-ADM2\") %&gt;%\n  rename(district = shapeName) %&gt;%\n  st_transform(32737)\n\nReading layer `geoBoundaries-TZA-ADM2' from data source \n  `C:\\drkrodriguez\\datawithderek\\posts\\tz-fin-2023\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 170 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 29.58953 ymin: -11.76235 xmax: 40.44473 ymax: -0.983143\nGeodetic CRS:  WGS 84\n\n\n\ntz_dist\n\nSimple feature collection with 170 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -548018.9 ymin: 8698528 xmax: 658181.6 ymax: 9890194\nProjected CRS: WGS 84 / UTM zone 37S\nFirst 10 features:\n       district shapeISO                 shapeID shapeGroup shapeType\n1        Arusha     &lt;NA&gt; 72390352B32479700182608        TZA      ADM2\n2  Arusha Urban     &lt;NA&gt; 72390352B90906351205470        TZA      ADM2\n3        Karatu     &lt;NA&gt; 72390352B22674606658861        TZA      ADM2\n4       Longido     &lt;NA&gt; 72390352B95731720096997        TZA      ADM2\n5          Meru     &lt;NA&gt; 72390352B99598192663387        TZA      ADM2\n6       Monduli     &lt;NA&gt; 72390352B11439822404473        TZA      ADM2\n7    Ngorongoro     &lt;NA&gt; 72390352B42279830137418        TZA      ADM2\n8         Ilala     &lt;NA&gt; 72390352B40584164885098        TZA      ADM2\n9     Kinondoni     &lt;NA&gt; 72390352B66429416458525        TZA      ADM2\n10       Temeke     &lt;NA&gt; 72390352B94835751472469        TZA      ADM2\n                         geometry\n1  MULTIPOLYGON (((262372 9603...\n2  MULTIPOLYGON (((251788.2 96...\n3  MULTIPOLYGON (((148006.1 96...\n4  MULTIPOLYGON (((206258.1 96...\n5  MULTIPOLYGON (((262372 9603...\n6  MULTIPOLYGON (((226729.3 96...\n7  MULTIPOLYGON (((160641.8 96...\n8  MULTIPOLYGON (((530993 9249...\n9  MULTIPOLYGON (((529848.2 92...\n10 MULTIPOLYGON (((531400.6 92...\n\n\nThe output shows that there are 170 objects loaded which corresponds to individual districts. The object is also of multipolygon class which could indicate that there are districts with discontinuous land areas, like islands.\nWe can create a simple map to visualize the boundaries using qtm() from tmap.\n\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\nqtm(tz_dist, text = \"district\", text.size = 0.4)\n\n\n\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting"
  },
  {
    "objectID": "posts/tz-fin-2023/index.html#b.2-deriving-district-centroids",
    "href": "posts/tz-fin-2023/index.html#b.2-deriving-district-centroids",
    "title": "Geographically Weighted Modeling of Financial Inclusion in Tanzania",
    "section": "B.2 Deriving District centroids",
    "text": "B.2 Deriving District centroids\nBefore we load the aspatial data, we will process the district boundary map to be able to use it for future operations with the other dataset. One step that needs to be done is to define representative points, which can be the centroids for the boundary map. The primary purpose of this is to be able to map the aspatial data for a district into a single location. In order to do this, the first step is to convert the multipolygon layer object into a polygon object which will allow for proper centroid calculations for each district.\nWe use the code chunk below to convert the sf object into polygons using st_cast() and then create a column for each individual polygon’s area using mutate() with st_area(). We then use groupby() to reduce back the object to one row per district and then filter() to keep only the largest polygon for each district.\n\ntz_dist_poly &lt;- tz_dist %&gt;%\n  st_cast(\"POLYGON\") %&gt;%\n  mutate(area = st_area(.)) %&gt;%\n  group_by(district) %&gt;%\n  filter(area == max(area)) %&gt;%\n  ungroup() %&gt;%\n  select(-area) %&gt;%\n  select(district)\n\nWarning in st_cast.sf(., \"POLYGON\"): repeating attributes for all\nsub-geometries for which they may not be constant\n\n\nWe can produce a map with tmap package to see if the operation had any irregular effects on the geography. In order to see the difference between the original map and the polygon map, we add the original map as the first layer in red, and then overlay the polygon map. The areas which now appear red are the polygons or areas that have been excluded from the original map to the polygon map.\n\ntm_shape(tz_dist) +\n  tm_polygons(\"red\") +\ntm_shape(tz_dist_poly) +\n  tm_polygons(\"grey\") +\n  tm_layout(title = \"Full vs Poly Map\",\n            title.position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\nFrom the output, we see that in addition to a few small islands, there are three inland areas that have been excluded from the original map. While this produces holes in the new map, this might not be a big concern right now as long as the centroids we get from the remaining geometries is meaningful.\nIn order to generate the centroids, we can now use st_centroid() to compute them across each district’s largest polygon.\n\ntz_dist_centroids &lt;- st_centroid(tz_dist_poly)\n\nWarning: st_centroid assumes attributes are constant over geometries\n\n\nWe can check the location of the centroids by plotting them as a layer on top of the original district boundary layer using tmap package in the code below.\n\ntm_shape(tz_dist) +\n  tm_polygons(\"grey\") +\ntm_shape(tz_dist_centroids) +\n  tm_dots(\"green\", size = 0.2) +\n  tm_layout(title = \"District Centroids\",\n            title.position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\nThe centroid locations look mostly acceptable, with a few exceptions where they might be lying somewhere away from the district boundaries given some of the districts have odd, nonconvex shapes."
  },
  {
    "objectID": "posts/tz-fin-2023/index.html#b.2-loading-finscope-tanzania-2023-respondent-data",
    "href": "posts/tz-fin-2023/index.html#b.2-loading-finscope-tanzania-2023-respondent-data",
    "title": "Geographically Weighted Modeling of Financial Inclusion in Tanzania",
    "section": "B.2 Loading Finscope Tanzania 2023 Respondent Data",
    "text": "B.2 Loading Finscope Tanzania 2023 Respondent Data\nWe can use read_csv() in the code chunk below to load the raw respondent data into an R object.\n\nfstz23 &lt;- read_csv(\"data/aspatial/FinScope Tanzania 2023_Individual Main Data_FINAL.csv\", show_col_types = FALSE)\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nThere are 9915 rows or records, and 721 columns or fields. Most of these columns should not be relevant in meeting our objective, so it is advised to limit the data we work with to those meaningful variable. These variables should be our variable(s) of concern, or the dependent variable(s), and the variables that may contribute to it, or the independent variables."
  },
  {
    "objectID": "posts/tz-fin-2023/index.html#b.4-preparing-finscope-tanzania-2023-respondent-data",
    "href": "posts/tz-fin-2023/index.html#b.4-preparing-finscope-tanzania-2023-respondent-data",
    "title": "Geographically Weighted Modeling of Financial Inclusion in Tanzania",
    "section": "B.4 Preparing Finscope Tanzania 2023 Respondent Data",
    "text": "B.4 Preparing Finscope Tanzania 2023 Respondent Data\n\nB.4.1 Selecting potential variables\nThe first step in preparing the dataset is to reduce the data by keeping only the potentially relevant fields. This means identifying fields that can be used as is or to derive both the response variable(s) and the explanatory variables.\nThis is performed by scanning the datamap file (i.e., data dictionary) that accompanies the dataset. From there, we decide to keep only the following variables. We also change the variable names to a shorter and more recognizable one.\n\nFinancial Inclusion (3)Geographic (4)Demographic (11)Economic (8)Technographic (3)\n\n\n\n\n\n\n\n\n\n\nVariable Name\nDescription\nNew Variable Name\n\n\n\n\nBANKED\nClassified as: Banked; Not Banked\nfi_banked\n\n\nOVERALL_FORMAL\nClassified as using formal instruments: Yes, No\nfi_formal\n\n\nINFORMAL\nClassified as using informal instruments: Yes, No\nfi_informal\n\n\n\n\n\n\n\n\nVariable\nDescription\nNew Variable Name\n\n\n\n\nreg_name\nRegion name\nregion\n\n\ndist_name\nDistrict name\ndistrict\n\n\nward_name\nWard name\nward\n\n\nclustertype\nIndicates if in rural or urban\nurban\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable Name\nDescription\nNew Variable Name\n\n\n\n\nc8c\nAge\nage\n\n\nc9\nGender: male. or female\nfemale\n\n\nc10\nMarital status: married, divorced, widowed, single (4 levels)\nmaritalstatus\n\n\nc11\nHighest level education (10 levels of values)\neducation\n\n\nc2\nHead of household: respondent, not the respondent\nhead_hh\n\n\nc8n_a1\nVisually impaired: Yes, No\nvisual_impaired\n\n\nc8n_b1\nHearing impaired: Yes, No\nhearing_impaired\n\n\nc8n_c1\nCommunication impaired: Yes, No\ncomm_impaired\n\n\nc8n_d1\nMovement impaired: Yes, No\nmove_impaired\n\n\nc8n_e1\nDifficulty with daily activities: Yes, No\ndaily_impaired\n\n\nc8n_f1\nDifficulty remembering and concentrating: Yes, No\ncogn_impaired\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable Name\nDescription\nNew Variable Name\n\n\n\n\nc12_1\nLand ownership (6 levels)\nland_own\n\n\nc14\nFamily involved in agriculture, fishing or aquaculture: Yes, No\nagricultural\n\n\nc18_2\nPrimary source of funds (12 levels)\nsource_of_funds\n\n\nc27__17\nHas some form of ID: Yes, No\nhas_id\n\n\nD2_1__1\nReceives salary from regular job: Yes, No\nreg_job\n\n\nD2_1__2\nReceives money from selling goods produced: Yes, No\nproduction\n\n\nD2_1__11\nDoes not receive income: Yes, No\nno_income\n\n\nIncomeMain\nDerived variable for main source of income (14 levels)\nincome_source\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable Name\nDescription\nNew Variable Name\n\n\n\n\nc23__1\nAccess to mobile phone: No, Yes\nmobile\n\n\nc23__2\nAccess to internet: No, Yes\ninternet\n\n\nc24_1\nPersonally own mobile phone: No, Yes\nown_mobile\n\n\n\n\n\n\nThe code block below compiles the 29 variables and their new names in two separate objects. These will be used in the succeeding steps for data preparation.\n\ncolstokeep &lt;- c(\"reg_name\", \"dist_name\", \"ward_name\", \"clustertype\",\n                \"c8c\", \"c9\", \"c10\", \"c11\", \"c2\",\n                \"c8n_a1\", \"c8n_b1\", \"c8n_c1\", \"c8n_d1\", \"c8n_e1\", \"c8n_f1\",\n                \"c12_1\", \"c14\", \"c18_2\",\n                \"c23__1\", \"c23__2\", \"c24_1\", \"c27__17\",\n                \"D2_1__1\", \"D2_1__2\", \"D2_1__11\",\n                \"BANKED\", \"OVERALL_FORMAL\", \"INFORMAL\", \"IncomeMain\")\nnewnames &lt;- c(\"region\", \"district\", \"ward\", \"urban\",\n              \"age\", \"female\", \"maritalstatus\", \"education\", \"head_hh\",\n              \"visual_impaired\", \"hearing_impaired\", \"comm_impaired\",\n              \"move_impaired\", \"daily_impaired\", \"cogn_impaired\",\n              \"land_own\", \"agricultural\", \"source_of_funds\",\n              \"mobile\", \"internet\", \"own_mobile\", \"has_id\",\n              \"reg_job\", \"production\", \"no_income\",\n              \"fi_banked\", \"fi_formal\", \"fi_informal\", \"income_source\")\nlength(colstokeep)\n\n[1] 29\n\nlength(newnames)\n\n[1] 29\n\n\nWe then use the code chunk below to keep the selected variables using select() and then to rename the variable or column names using colnames()\n\nfstz23_sf &lt;- fstz23 %&gt;%\n  select(all_of(colstokeep))\n\ncolnames(fstz23_sf) &lt;- newnames\n\n\n\nB.4.2 Recoding of variables\nRecoding of variables is a data preparation step where variable values are replaced by another. This may be done for reasons like cleaning the data or standardising the data. This is generally performed in R using the recode() function.\n\nB.4.2.1 Recoding of district names\nWe have district names in the map and in the survey data. As these are two different data sources, there is a chance that they mismatch. We need to ensure that they use the same names as we will use these to add the geographic information to the survey data.\nWe first check which names in each set do not have a corresponding match in the other. We can do this by performing a left join using left_join() and checking which elements do not have matches. We can the use filter() to find the records that did not return a valid value from the other dataset. The code chunk performs this left join approach twice as it needs to be checked for direction for each data source, and then we display the district names that are unmatched for each dataset.\n\nmismatched_values &lt;- tz_dist %&gt;%\n  left_join(fstz23_sf, by = \"district\") %&gt;%\n  filter(is.na(region)) %&gt;%\n  select(district)\nmismatched_tzmap &lt;- mismatched_values$district\n\nmismatched_values &lt;- fstz23_sf %&gt;%\n  left_join(tz_dist, by = \"district\") %&gt;%\n  filter(is.na(shapeType)) %&gt;%\n  select(district)\nmismatched_fstz23 &lt;- mismatched_values$district\n\nlist(\n  mismatched_in_map = mismatched_tzmap,\n  number_mm1 = length(c(mismatched_tzmap)),\n  mismatched_in_survey = unique(mismatched_fstz23),\n  number_mm2 = length(c(unique(mismatched_fstz23)))\n)\n\n$mismatched_in_map\n [1] \"Arusha Urban\"                 \"Meru\"                        \n [3] \"Dodoma Urban\"                 \"Iringa Urban\"                \n [5] \"Mafinga Township Authority\"   \"Bukoba Urban\"                \n [7] \"Mpanda Urban\"                 \"Kasulu Township Authority\"   \n [9] \"Kigoma  Urban\"                \"Moshi Urban\"                 \n[11] \"Lindi Urban\"                  \"Babati UrbanBabati Urban\"    \n[13] \"Butiam\"                       \"Musoma Urban\"                \n[15] \"Mbeya Urban\"                  \"Magharibi\"                   \n[17] \"Morogoro Urban\"               \"Masasi  Township Authority\"  \n[19] \"Mtwara Urban\"                 \"Makambako Township Authority\"\n[21] \"Njombe Urban\"                 \"Kibaha Urban\"                \n[23] \"Mafia\"                        \"Sumbawanga Urban\"            \n[25] \"Songea Urban\"                 \"Kahama Township Authority\"   \n[27] \"Shinyanga Urban\"              \"Singida Urban\"               \n[29] \"Tunduma\"                      \"Tabora Urban\"                \n[31] \"Handeni Mji\"                  \"Korogwe\"                     \n[33] \"Korogwe Township Authority\"   \"Tanga Urban\"                 \n\n$number_mm1\n[1] 34\n\n$mismatched_in_survey\n [1] \"Tanganyika\"  \"Kigamboni\"   \"Arumeru\"     \"Butiama\"     \"Dodoma\"     \n [6] \"Tanga\"       \"Malinyi\"     \"Kibiti\"      \"Magharibi B\" \"Magharibi A\"\n[11] \"Ubungo\"      \"Tabora\"     \n\n$number_mm2\n[1] 12\n\n\nThe output reveals that there are 34 district names in the boundary map that are not matched, while there are 12 in the survey data that are not matched. We do not need to ensure all 34 in the first dataset is matched as there might really be areas where there are no respondents or residents. We do, however, want almost all, if not all, of the records in the second dataset to be matched as this is where our modeling data sits. It is also worth noting that all the recoding for districts will be done on fstz23_sf.\nWe will perform checks and data cleaning for the 12 unmatched values in fstz23_sf, and we will also explore some of the remaining unmatched variables in tz_dist. We see that there are values which have “Urban” at the end so there might also be an opportunity to create matches for them.\n\nTanganyika and TangaMagharibiArumeruButiamaDodomaKibitiKigamboniMalinyiTaboraUbungoSelect Urban Areas\n\n\nFor unmatched values, there might be differences in spellings between the two sources. While this doesn’t guarantee catching misspellings, we can at least check if there are other variables that share the first few letters with the unmatched value.\nTo find district values which contains “Tang” we can use str_detect() on both dataset’s columns.\n\nto_find &lt;- \"Tanga\"\nunique(tz_dist_poly[str_detect(tz_dist_poly$district,to_find),]$district)\n\n[1] \"Tanga Urban\"\n\nunique(fstz23_sf[str_detect(fstz23_sf$district,to_find),]$district)\n\n[1] \"Tanganyika\" \"Tanga\"     \n\n\nThere is only one value in tz_dist that contains “Tang” but two in fstz_23. It should be safe to assume that Tanga and Tanga Urban are one and the same. We can perform the recoding using recode() within mutate() for the district column of fstz_23.\n\nfstz23_sf &lt;- mutate(fstz23_sf, district = recode(district,\n                            \"Tanga\" = \"Tanga Urban\"))\n\nFor Tanganyika, we can use the following code chunk to check how many survey records are affected using length(), and then show the wards and regions for the records that do have a district name of Tanganyika.\n\n# Count number of records which has district name Tanganyika\ncount(fstz23_sf[str_detect(fstz23_sf$district,\"Tanganyika\"),])\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1    89\n\n# Show regions and wards for records with district name Tanganyika\nunique(select(fstz23_sf[str_detect(fstz23_sf$district,\"Tanganyika\"),], \n       c(region, district, ward)))\n\n# A tibble: 6 × 3\n  region district   ward    \n  &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;   \n1 Katavi Tanganyika Mnyagala\n2 Katavi Tanganyika Ikola   \n3 Katavi Tanganyika Bulamata\n4 Katavi Tanganyika Mishamo \n5 Katavi Tanganyika Sibwesa \n6 Katavi Tanganyika Isengule\n\n\nThere are six wards that all reflect the same region name of “Katavi”. Upon research, it appears that the Tanganyika district was recently formed and was part of the rural area of the district Mpanda. As such, we will recode Tanganyika to Mpanda using the same approach with\n\nfstz23_sf &lt;- mutate(fstz23_sf, district = recode(district,\n                            \"Tanganyika\" = \"Mpanda\"))\n\n\n\nWe saw that there are two districts with the word Magharibi in the fstz23, and one in tz_dist. We can confirm this by using str_detect() to check for all district names containing “Mag” (so we also check some variation in spelling) in both datasets.\n\nto_find &lt;- \"Magha\"\nunique(tz_dist_poly[str_detect(tz_dist_poly$district,to_find),]$district)\n\n[1] \"Magharibi\"\n\nunique(fstz23_sf[str_detect(fstz23_sf$district,to_find),]$district)\n\n[1] \"Magharibi B\" \"Magharibi A\"\n\n\nFor this case, we drop the B and A by using recode() on the district column of ftsz23_sf.\n\nfstz23_sf &lt;- mutate(fstz23_sf, district = recode(district,\n                            \"Magharibi B\" = \"Magharibi\",\n                            \"Magharibi A\" = \"Magharibi\"))\n\n\n\nWe next look into the unmatched district “Arumeru” in fstz23. We will use the code chunk below for this district and most of the succeeding ones to check district names that match in both data sets, show the regions and wards in fstz23 for the district, and the number of records which reflect that district. For these, we continue using str_detect() which is included in tidyverse under the stringr package in order to find matches of a substring.\n\nto_find &lt;- \"Arumeru\"\nprint(\"Matches in tz_dist\")\n\n[1] \"Matches in tz_dist\"\n\nunique(tz_dist_poly[str_detect(tz_dist_poly$district,to_find),]$district)\n\ncharacter(0)\n\nprint(\"Matches in ftsz23\")\n\n[1] \"Matches in ftsz23\"\n\nunique(fstz23_sf[str_detect(fstz23_sf$district,to_find),]$district)\n\n[1] \"Arumeru\"\n\ncount(fstz23_sf[str_detect(fstz23_sf$district,to_find),])\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1   105\n\nunique(select(fstz23_sf[str_detect(fstz23_sf$district,to_find),], \n       c(region, district, ward, urban)))\n\n# A tibble: 7 × 4\n  region district ward         urban\n  &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;        &lt;chr&gt;\n1 Arusha Arumeru  Poli         Urban\n2 Arusha Arumeru  Maroroni     Rural\n3 Arusha Arumeru  Olmotonyi    Rural\n4 Arusha Arumeru  Oloirien     Urban\n5 Arusha Arumeru  Maji ya Chai Rural\n6 Arusha Arumeru  Kisongo      Rural\n7 Arusha Arumeru  Nkoaranga    Rural\n\n\nThe output shows that there are no districts in tz_dist that have a name of Arumeru, but there are 105 in fstz23. Upon research, we see that the wards of Arumeru was split between Arusha and Meru. Among the wards in the dataset, the following are now part of Meru: Maroroni, Poli, Maji ya Chai, Nkoaranga. The balance 3 are Arusha: Olmotonyi, Oloirien, Kisongo.\nWe first update the records for the last three wards to reflect a district name of Arusha using the following code chunk.\n\nfstz23_sf[(fstz23_sf$ward == \"Olmotonyi\" | fstz23_sf$ward == \"Oloirien\" | fstz23_sf$ward == \"Kisongo\"),]$district = \"Arusha\"\n\nWe can then use recode() to change the remaining records that are still reflecting “Arumeru” and change them to “Meru”\n\nfstz23_sf &lt;- mutate(fstz23_sf, district = recode(district,\n                            \"Arumeru\" = \"Meru\"))\n\n\n\nWe execute the same code chunk that makes use of recode() to find records where the district name contains “Butiam”\n\nto_find &lt;- \"Butiam\"\nprint(\"Matches in tz_dist\")\n\n[1] \"Matches in tz_dist\"\n\nunique(tz_dist_poly[str_detect(tz_dist_poly$district,to_find),]$district)\n\n[1] \"Butiam\"\n\nprint(\"Matches in ftsz23\")\n\n[1] \"Matches in ftsz23\"\n\nunique(fstz23_sf[str_detect(fstz23_sf$district,to_find),]$district)\n\n[1] \"Butiama\"\n\ncount(fstz23_sf[str_detect(fstz23_sf$district,to_find),])\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1    45\n\nunique(select(fstz23_sf[str_detect(fstz23_sf$district,to_find),], \n       c(region, district, ward, urban)))\n\n# A tibble: 3 × 4\n  region district ward    urban\n  &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;\n1 Mara   Butiama  Butiama Urban\n2 Mara   Butiama  Bukabwa Rural\n3 Mara   Butiama  Mirwa   Rural\n\n\nWe see that there is only one district from each dataset and it appears that they can only refer to the same district. We then use recode() to update “Butiama” to “Butiam”\n\nfstz23_sf &lt;- mutate(fstz23_sf, district = recode(district,\n                            \"Butiama\" = \"Butiam\"))\n\n\n\nWe execute the same code chunk that makes use of recode() to find records where the district name contains “Dodom”\n\nto_find &lt;- \"Dodom\"\nprint(\"Matches in tz_dist\")\n\n[1] \"Matches in tz_dist\"\n\nunique(tz_dist_poly[str_detect(tz_dist_poly$district,to_find),]$district)\n\n[1] \"Dodoma Urban\"\n\nprint(\"Matches in ftsz23\")\n\n[1] \"Matches in ftsz23\"\n\nunique(fstz23_sf[str_detect(fstz23_sf$district,to_find),]$district)\n\n[1] \"Dodoma\"\n\ncount(fstz23_sf[str_detect(fstz23_sf$district,to_find),])\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1   142\n\nunique(select(fstz23_sf[str_detect(fstz23_sf$district,to_find),], \n       c(region, district, ward, urban)))\n\n# A tibble: 10 × 4\n   region district ward            urban\n   &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;           &lt;chr&gt;\n 1 Dodoma Dodoma   Msalato         Urban\n 2 Dodoma Dodoma   Mnadani         Urban\n 3 Dodoma Dodoma   Ntyuka          Urban\n 4 Dodoma Dodoma   Mbabala         Urban\n 5 Dodoma Dodoma   Nkuhungu        Urban\n 6 Dodoma Dodoma   Nzuguni         Urban\n 7 Dodoma Dodoma   Hombolo Bwawani Urban\n 8 Dodoma Dodoma   Kikuyu Kusini   Urban\n 9 Dodoma Dodoma   Mkonze          Urban\n10 Dodoma Dodoma   Uhuru           Urban\n\n\nWe again see that there is a one-to-one matching for the sole district on both dataset. We will then update “Dodoma” to “Dodoma Urban” using recode() in the chunk below\n\nfstz23_sf &lt;- mutate(fstz23_sf, district = recode(district,\n                            \"Dodoma\" = \"Dodoma Urban\"))\n\n\n\nWe execute the same code chunk that makes use of recode() to find records where the district name contains “Kibit”\n\nto_find &lt;- \"Kibit\"\nprint(\"Matches in tz_dist\")\n\n[1] \"Matches in tz_dist\"\n\nunique(tz_dist_poly[str_detect(tz_dist_poly$district,to_find),]$district)\n\ncharacter(0)\n\nprint(\"Matches in ftsz23\")\n\n[1] \"Matches in ftsz23\"\n\nunique(fstz23_sf[str_detect(fstz23_sf$district,to_find),]$district)\n\n[1] \"Kibiti\"\n\ncount(fstz23_sf[str_detect(fstz23_sf$district,to_find),])\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1    29\n\nunique(select(fstz23_sf[str_detect(fstz23_sf$district,to_find),], \n       c(region, district, ward, urban)))\n\n# A tibble: 2 × 4\n  region district ward   urban\n  &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt;\n1 Pwani  Kibiti   Mbuchi Rural\n2 Pwani  Kibiti   Bungu  Rural\n\n\nUpon research, it appears that Kibiti is a relatively new district. The two wards that appear under it were actually part of Rufiji district, so we can change “Kibiti” to “Rufiji” using encode()\n\nfstz23_sf &lt;- mutate(fstz23_sf, district = recode(district,\n                            \"Kibiti\" = \"Rufiji\"))\n\n\n\nWe execute the same code chunk that makes use of recode() to find records where the district name contains “Kigamb”\n\nto_find &lt;- \"Kigamb\"\nprint(\"Matches in tz_dist\")\n\n[1] \"Matches in tz_dist\"\n\nunique(tz_dist_poly[str_detect(tz_dist_poly$district,to_find),]$district)\n\ncharacter(0)\n\nprint(\"Matches in ftsz23\")\n\n[1] \"Matches in ftsz23\"\n\nunique(fstz23_sf[str_detect(fstz23_sf$district,to_find),]$district)\n\n[1] \"Kigamboni\"\n\ncount(fstz23_sf[str_detect(fstz23_sf$district,to_find),])\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1    45\n\nunique(select(fstz23_sf[str_detect(fstz23_sf$district,to_find),], \n       c(region, district, ward, urban)))\n\n# A tibble: 3 × 4\n  region        district  ward      urban\n  &lt;chr&gt;         &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;\n1 Dar es Salaam Kigamboni Kibada    Urban\n2 Dar es Salaam Kigamboni Kigamboni Urban\n3 Dar es Salaam Kigamboni Somangila Urban\n\n\nThere is no indication of the district merging or splitting recently from another, but if we check our map, the region should occupy part of the space which appears as “Temeke”. We then recode Kigamboni as Temeke using the following code chunk.\n\nfstz23_sf &lt;- mutate(fstz23_sf, district = recode(district,\n                            \"Kigamboni\" = \"Temeke\"))\n\n\n\nWe execute the same code chunk that makes use of recode() to find records where the district name contains “Malin”\n\nto_find &lt;- \"Malin\"\nprint(\"Matches in tz_dist\")\n\n[1] \"Matches in tz_dist\"\n\nunique(tz_dist_poly[str_detect(tz_dist_poly$district,to_find),]$district)\n\ncharacter(0)\n\nprint(\"Matches in ftsz23\")\n\n[1] \"Matches in ftsz23\"\n\nunique(fstz23_sf[str_detect(fstz23_sf$district,to_find),]$district)\n\n[1] \"Malinyi\"\n\ncount(fstz23_sf[str_detect(fstz23_sf$district,to_find),])\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1    15\n\nunique(select(fstz23_sf[str_detect(fstz23_sf$district,to_find),], \n       c(region, district, ward, urban)))\n\n# A tibble: 1 × 4\n  region   district ward     urban\n  &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;\n1 Morogoro Malinyi  Usangule Rural\n\n\nIf we check the map, the location of Malinyi falls in the region of Ulanga in our map. We again use recode() to change the district names to Ulanga.\n\nfstz23_sf &lt;- mutate(fstz23_sf, district = recode(district,\n                            \"Malinyi\" = \"Ulanga\"))\n\n\n\nWe execute the same code chunk that makes use of recode() to find records where the district name contains “Tabor”\n\nto_find &lt;- \"Tabor\"\nprint(\"Matches in tz_dist\")\n\n[1] \"Matches in tz_dist\"\n\nunique(tz_dist_poly[str_detect(tz_dist_poly$district,to_find),]$district)\n\n[1] \"Tabora Urban\"\n\nprint(\"Matches in ftsz23\")\n\n[1] \"Matches in ftsz23\"\n\nunique(fstz23_sf[str_detect(fstz23_sf$district,to_find),]$district)\n\n[1] \"Tabora\"\n\ncount(fstz23_sf[str_detect(fstz23_sf$district,to_find),])\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1    45\n\nunique(select(fstz23_sf[str_detect(fstz23_sf$district,to_find),], \n       c(region, district, ward, urban)))\n\n# A tibble: 3 × 4\n  region district ward     urban\n  &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;\n1 Tabora Tabora   Mbugani  Urban\n2 Tabora Tabora   Kiloleni Urban\n3 Tabora Tabora   Mwinyi   Urban\n\n\nGiven that there is only “Tabora Urban” in tz_dist, we should be able to update the district names in fstz23 using this.\n\nfstz23_sf &lt;- mutate(fstz23_sf, district = recode(district,\n                            \"Tabora\" = \"Tabora Urban\"))\n\n\n\nWe execute the same code chunk that makes use of recode() to find records where the district name contains “Ubung”\n\nto_find &lt;- \"Ubung\"\nprint(\"Matches in tz_dist\")\n\n[1] \"Matches in tz_dist\"\n\nunique(tz_dist_poly[str_detect(tz_dist_poly$district,to_find),]$district)\n\ncharacter(0)\n\nprint(\"Matches in ftsz23\")\n\n[1] \"Matches in ftsz23\"\n\nunique(fstz23_sf[str_detect(fstz23_sf$district,to_find),]$district)\n\n[1] \"Ubungo\"\n\ncount(fstz23_sf[str_detect(fstz23_sf$district,to_find),])\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1    88\n\nunique(select(fstz23_sf[str_detect(fstz23_sf$district,to_find),], \n       c(region, district, ward, urban)))\n\n# A tibble: 6 × 4\n  region        district ward    urban\n  &lt;chr&gt;         &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;\n1 Dar es Salaam Ubungo   Mabibo  Urban\n2 Dar es Salaam Ubungo   Mbezi   Urban\n3 Dar es Salaam Ubungo   Kimara  Urban\n4 Dar es Salaam Ubungo   Msigani Urban\n5 Dar es Salaam Ubungo   Goba    Urban\n6 Dar es Salaam Ubungo   Manzese Urban\n\n\nUpon checking, the location of Ubungo appears to be within the boundaries of the district Kinondoni in our map. We recode it as such with the following code chunk.\n\nfstz23_sf &lt;- mutate(fstz23_sf, district = recode(district,\n                            \"Ubungo\" = \"Kinondoni\"))\n\n\n\nWe have removed all mismatches from fstz23 but have 28 unmatched district names in tz_dist. Among these are a few districts that are suffixed by “Urban”\n\nmismatched_values &lt;- tz_dist %&gt;%\n  left_join(fstz23_sf, by = \"district\") %&gt;%\n  filter(is.na(region)) %&gt;%\n  select(district)\nmismatch_urban &lt;- mismatched_values[str_detect(mismatched_values$district,\"Urban\"),]$district\nmismatch_urban\n\n [1] \"Arusha Urban\"             \"Iringa Urban\"            \n [3] \"Bukoba Urban\"             \"Mpanda Urban\"            \n [5] \"Kigoma  Urban\"            \"Moshi Urban\"             \n [7] \"Lindi Urban\"              \"Babati UrbanBabati Urban\"\n [9] \"Musoma Urban\"             \"Mbeya Urban\"             \n[11] \"Morogoro Urban\"           \"Mtwara Urban\"            \n[13] \"Njombe Urban\"             \"Kibaha Urban\"            \n[15] \"Sumbawanga Urban\"         \"Songea Urban\"            \n[17] \"Shinyanga Urban\"          \"Singida Urban\"           \n\n\nFor these, we will assume they refer to the urban region of a given district. For example, Arusha Urban will cover the urban area of the Arusha district. Based on this assumption, we can go through ftsz23 to check for records for that district and are in the urban area and then map them to the values above. We use a for loop in the code chunk below to find records in fstz23 that match these conditions and then apply the suffixed district names as replacements. We use the word() function from stringr package in order to pick the first word and treat it as the “plain” district name.\n\nfor (i in mismatch_urban)\n  {\n    dist = word(i, 1)\n    fstz23_sf[str_detect(fstz23_sf$district,dist) & fstz23_sf$urban == \"Urban\",]$district = i\n  }\n\n\n\n\nWe can check for the remaining mismatches by rerunning the earlier code.\n\nmismatched_values &lt;- tz_dist %&gt;%\n  left_join(fstz23_sf, by = \"district\") %&gt;%\n  filter(is.na(region)) %&gt;%\n  select(district)\nmismatched_tzmap &lt;- mismatched_values$district\n\nmismatched_values &lt;- fstz23_sf %&gt;%\n  left_join(tz_dist, by = \"district\") %&gt;%\n  filter(is.na(shapeType)) %&gt;%\n  select(district)\nmismatched_fstz23 &lt;- mismatched_values$district\n\nlist(\n  mismatched_in_map = mismatched_tzmap,\n  number_mm1 = length(c(mismatched_tzmap)),\n  mismatched_in_survey = unique(mismatched_fstz23),\n  number_mm2 = length(c(unique(mismatched_fstz23)))\n)\n\n$mismatched_in_map\n [1] \"Iringa Urban\"                 \"Mafinga Township Authority\"  \n [3] \"Kasulu Township Authority\"    \"Masasi  Township Authority\"  \n [5] \"Makambako Township Authority\" \"Mafia\"                       \n [7] \"Kahama Township Authority\"    \"Tunduma\"                     \n [9] \"Handeni Mji\"                  \"Korogwe\"                     \n[11] \"Korogwe Township Authority\"  \n\n$number_mm1\n[1] 11\n\n$mismatched_in_survey\ncharacter(0)\n\n$number_mm2\n[1] 0\n\n\nWe were able to remove all mismatches from fstz23 and then reduce the mismatches from tz_dist from 34 to 11. We can visualize the distribution of the (recoded) records in our map by first adding the number of records into the sf object. We use count() in the code chunk below to compute for the number of records per district. We then use left_join() to merge it with the district map, and replace any zero values (from mismatches) with zero.\n\ntz_dist_stat &lt;- tz_dist %&gt;%\n  left_join(count(fstz23_sf, district), by= \"district\") %&gt;%\n  rename(\"orig_records\" = \"n\") %&gt;%\n  replace(is.na(.),0)\n\nWe can then use tmap package to plot the distribution of respondents, We first add a layer in grey for the full map and then add a choropleth map for districts with nonzero number of respondents. This will show districts with no respondents as grey.\n\ntm_shape(tz_dist)+\n  tm_polygons(\"grey\") +\ntm_shape(tz_dist_stat[tz_dist_stat$orig_records &gt; 0,]) +\n  tm_polygons(\"orig_records\", title = \"Original Records\")\n\n\n\n\n\n\n\n\nThe output shows that there are only a few districts that do not have any respondents. Most of the districts have between 0 to 50 respondents. The highest number of respondents in a single district is between 150 and 200.\n\n\nB.4.2.2 Recoding of Modelling Variables\nIf we look at the data, the modelling variables that we have in fstz23 are mostly categorical. Most of these are binary, but some have more than two values. While we can use categorical variables for EDA, these do not work well once we start modelling. For our case, we will go ahead and convert most of these variables upfront.\nWe can use the following code chunk which uses the unique() function to display the distinct values. We use lapply() to run the function on each variable in the dataframe but we exclude the region, district, ward and age as these will either not be part of the modelling, or it is already in numeric form.\n\nlapply(select(fstz23_sf,-c(region, district, ward, age)), unique)\n\n$urban\n[1] \"Rural\" \"Urban\"\n\n$female\n[1] \"Female\" \"Male\"  \n\n$maritalstatus\n[1] \"Married/living together\" \"Widowed\"                \n[3] \"Divorced/separated\"      \"Single/never married\"   \n\n$education\n [1] \"Some primary\"                             \n [2] \"No formal education\"                      \n [3] \"Primary completed\"                        \n [4] \"Some secondary\"                           \n [5] \"Some University or other higher education\"\n [6] \"University or higher education completed\" \n [7] \"Secondary competed-O level\"               \n [8] \"Post primary technical training\"          \n [9] \"Secondary completed-A level\"              \n[10] \"Don’t know\"                               \n\n$head_hh\n[1] \"Respondent is hhh\"  \"Respondent not hhh\"\n\n$visual_impaired\n[1] \"Yes\" \"No\" \n\n$hearing_impaired\n[1] \"No\"  \"Yes\"\n\n$comm_impaired\n[1] \"No\"  \"Yes\"\n\n$move_impaired\n[1] \"No\"  \"Yes\"\n\n$daily_impaired\n[1] \"No\"  \"Yes\"\n\n$cogn_impaired\n[1] \"No\"  \"Yes\"\n\n$land_own\n[1] \"You personally own the land/plot where you live\" \n[2] \"The land/plot is rented\"                         \n[3] \"You own the land/plot together with someone else\"\n[4] \"You don’t own or rent the land\"                  \n[5] \"Other A household members owns the land/plot\"    \n[6] \"Don’t know (Don’t read out)\"                     \n\n$agricultural\n[1] \"Yes\" \"No\" \n\n$source_of_funds\n [1] \"Your household sells some of its crops and uses the money\"                                              \n [2] NA                                                                                                       \n [3] \"Your household has to borrow money\"                                                                     \n [4] \"Your household has money to buy it, it uses money from wages / other regular job /  sources of income\"  \n [5] \"Help from friends/relatives/neighbors/community/Government\"                                             \n [6] \"Use savings the household has\"                                                                          \n [7] \"Your household sells non-agricultural things to get money\"                                              \n [8] \"Your household gets it from a buyer to whom it has to sell its crop, livestock or fish when it is ready\"\n [9] \"Your household does piece work/casual jobs to get money to buy it\"                                      \n[10] \"Your household sells some of its livestock and uses the money\"                                          \n[11] \"Your household gets it in exchange for work it does\"                                                    \n[12] \"Your household doesn’t have to buy because it manage with what it has\"                                  \n[13] \"Your household sells products like milk, eggs that it get from its livestock to get money to buy it\"    \n\n$mobile\n[1] \"Yes\" \"No\" \n\n$internet\n[1] \"Yes\" \"No\"  NA   \n\n$own_mobile\n[1] \"Yes\" \"2\"  \n\n$has_id\n[1] \"No\"  \"Yes\"\n\n$reg_job\n[1] \"No\"  \"Yes\"\n\n$production\n[1] \"Yes\" \"No\" \n\n$no_income\n[1] \"No\"  \"Yes\"\n\n$fi_banked\n[1] \"Not Banked\" \"Banked\"    \n\n$fi_formal\n[1] \"OVERALL_FORMAL\"     \"Not OVERALL_FORMAL\"\n\n$fi_informal\n[1] \"INFORMAL incl SACCO AND CMG RISK CONTRIBUTIONS\"\n[2] \"Not INFORMAL\"                                  \n\n$income_source\n [1] \"Farmers and fishers\"                                         \n [2] \"Piece work/casual labor\"                                     \n [3] \"Traders - non-agricultural\"                                  \n [4] \"Dependents\"                                                  \n [5] \"Service providers\"                                           \n [6] \"Traders - agricultural products\"                             \n [7] \"Welfare\"                                                     \n [8] \"Formal sector salaried\"                                      \n [9] \"Pension\"                                                     \n[10] \"Informal sector salaried\"                                    \n[11] \"Other\"                                                       \n[12] \"Rental income\"                                               \n[13] \"Gambling\"                                                    \n[14] \"Interest from savings, investments, stocks, unit trusts etc.\"\n\n\nThe output reveals that 5 of the variables have more than 2 values or levels, while the balance 20 are binary. We also see that for the internet variable there are some records that show NA. We use the code below to check if each of the records have NA for internet by using is.na(), and then counting the number of records by just adding up the TRUE values using sum()\n\nsum(is.na(fstz23_sf$internet))\n\n[1] 7\n\n\nAs there are only seven with NA values, and there is no sure way of replacing them with the right value, removing them from the dataset should not produce any big issues. We use the code chunk below to remove the na’s from the internet variable. There are a number of different ways to remove na’s, here we just use a mask based on the complement of the results of the is.na() function which returns TRUE for any invalid values. We include the count of rows before and after running the code to check that there is a difference of seven rows.\n\nnrow(fstz23_sf)\n\n[1] 9915\n\nfstz23_sf &lt;- fstz23_sf[!is.na(fstz23_sf$internet),]\nnrow(fstz23_sf)\n\n[1] 9908\n\n\nFor binary variables, we will use the code below to replace the ‘positive’ value with 1 and the negative with 0. The values for each variable may vary so we need to apply the recoding individually for these variables.\n\nfstz23_sf &lt;- mutate(fstz23_sf,\n                    urban = recode(urban, \"Rural\" = 0, \"Urban\" = 1),\n                    female = recode(female, \"Male\" = 0, \"Female\" = 1),\n                    head_hh = recode(head_hh, \"Respondent not hhh\" = 0, \"Respondent is hhh\" = 1),\n                    visual_impaired = recode(visual_impaired, \"No\" = 0, \"Yes\" = 1),\n                    hearing_impaired = recode(hearing_impaired, \"No\" = 0, \"Yes\" = 1),\n                    comm_impaired = recode(comm_impaired, \"No\" = 0, \"Yes\" = 1),\n                    move_impaired = recode(move_impaired, \"No\" = 0, \"Yes\" = 1),\n                    daily_impaired = recode(daily_impaired, \"No\" = 0, \"Yes\" = 1),\n                    cogn_impaired = recode(cogn_impaired, \"No\" = 0, \"Yes\" = 1),\n                    agricultural = recode(agricultural, \"No\" = 0, \"Yes\" = 1),\n                    mobile = recode(mobile, \"No\" = 0, \"Yes\" = 1),\n                    internet = recode(internet, \"No\" = 0, \"Yes\" = 1),\n                    own_mobile = recode(own_mobile, \"2\" = 0, \"Yes\" = 1),\n                    has_id = recode(has_id, \"No\" = 0, \"Yes\" = 1),\n                    reg_job = recode(reg_job, \"No\" = 0, \"Yes\" = 1),\n                    production = recode(production, \"No\" = 0, \"Yes\" = 1),\n                    no_income = recode(no_income, \"No\" = 0, \"Yes\" = 1),\n                    fi_banked = recode(fi_banked, \"Not Banked\" = 0, \"Banked\" = 1),\n                    fi_formal = recode(fi_formal, \"Not OVERALL_FORMAL\" = 0, \"OVERALL_FORMAL\" = 1),\n                    fi_informal = recode(fi_informal, \"Not INFORMAL\" = 0,\n                                         \"INFORMAL incl SACCO AND CMG RISK CONTRIBUTIONS\" = 1))\n\nFor the non-binary variables, we start by checking the distribution of the data across their different levels. Where there is a very small amount of data in one category, we will not be too worried merging them with another (logical) one. We use the code chunk below which uses count() to give the number of records or rows for each of the values of the indicated column. We then wrap the output in arrange() to sort the values in ascending number of records.\n\nMarital StatusEducationLand OwnershipSource of FundsPrimary Source of Income\n\n\n\narrange(count(fstz23_sf, maritalstatus),n)\n\n# A tibble: 4 × 2\n  maritalstatus               n\n  &lt;chr&gt;                   &lt;int&gt;\n1 Widowed                   949\n2 Divorced/separated        956\n3 Single/never married     1934\n4 Married/living together  6069\n\n\n\n\n\narrange(count(fstz23_sf, education),n)\n\n# A tibble: 10 × 2\n   education                                     n\n   &lt;chr&gt;                                     &lt;int&gt;\n 1 Don’t know                                    4\n 2 Secondary completed-A level                  40\n 3 Post primary technical training              55\n 4 Some University or other higher education   125\n 5 University or higher education completed    292\n 6 Some secondary                              830\n 7 Secondary competed-O level                 1273\n 8 Some primary                               1354\n 9 No formal education                        1592\n10 Primary completed                          4343\n\n\n\n\n\narrange(count(fstz23_sf, land_own),n)\n\n# A tibble: 6 × 2\n  land_own                                             n\n  &lt;chr&gt;                                            &lt;int&gt;\n1 Don’t know (Don’t read out)                         14\n2 The land/plot is rented                           1022\n3 You own the land/plot together with someone else  1544\n4 You don’t own or rent the land                    1820\n5 Other A household members owns the land/plot      1974\n6 You personally own the land/plot where you live   3534\n\n\n\n\n\narrange(count(fstz23_sf, source_of_funds),n)\n\n# A tibble: 13 × 2\n   source_of_funds                                                             n\n   &lt;chr&gt;                                                                   &lt;int&gt;\n 1 Your household gets it in exchange for work it does                        30\n 2 Your household sells products like milk, eggs that it get from its liv…    32\n 3 Your household doesn’t have to buy because it manage with what it has      43\n 4 Help from friends/relatives/neighbors/community/Government                 67\n 5 Your household has to borrow money                                        101\n 6 Your household gets it from a buyer to whom it has to sell its crop, l…   146\n 7 Your household sells non-agricultural things to get money                 184\n 8 Your household sells some of its livestock and uses the money             304\n 9 Your household has money to buy it, it uses money from wages / other r…   448\n10 Use savings the household has                                             930\n11 Your household does piece work/casual jobs to get money to buy it        1164\n12 Your household sells some of its crops and uses the money                2325\n13 &lt;NA&gt;                                                                     4134\n\n\n\n\n\narrange(count(fstz23_sf, income_source),n)\n\n# A tibble: 14 × 2\n   income_source                                                    n\n   &lt;chr&gt;                                                        &lt;int&gt;\n 1 Interest from savings, investments, stocks, unit trusts etc.     2\n 2 Gambling                                                         6\n 3 Rental income                                                   46\n 4 Other                                                           67\n 5 Pension                                                         67\n 6 Welfare                                                         87\n 7 Informal sector salaried                                       209\n 8 Traders - agricultural products                                218\n 9 Service providers                                              386\n10 Formal sector salaried                                         426\n11 Traders - non-agricultural                                     643\n12 Dependents                                                    1960\n13 Piece work/casual labor                                       2559\n14 Farmers and fishers                                           3232\n\n\n\n\n\nThe output shows that there is a very large number of NAs in the source of funds. We highlight this using the is.na() function in the first code chunk below. As there are more than 40% missing values, and the income_source variable may already be holding the similar, but more complete, information, we will go ahead and drop the variable by using the select() function in the second code chunk.\n\nsum(is.na(fstz23_sf$source_of_funds))\n\n[1] 4134\n\n\n\nfstz23_sf &lt;- select(fstz23_sf, -c(source_of_funds))\n\nWhile we will be preforming a separate EDA for the variables, we will already create binary variables for certain levels of the remaining categorical variables. Common practice is to produce n-1 dummy variable for a variable with n-levels, however, we will opt to consolidate some of the variables together where it makes sense. For martial status, we can keep three levels. The first variable denotes whether the respondent is currently married, the second variable will be whether the respondent is widowed, separated or divorced. Single respondents should reflect a value of zero for both of the variables. For the code chunks, we use as.integer() to convert the logical outputs into zeros and ones.\n\nfstz23_sf$is_married &lt;- as.integer(fstz23_sf$maritalstatus == \"Married/living together\")\nfstz23_sf$was_married &lt;- as.integer(fstz23_sf$maritalstatus == \"Widowed\" | fstz23_sf$maritalstatus == \"Divorced/separated\")\n\nFor education, we will keep four levels for primary, secondary and tertiary or higher education. (with the fourth level denoting that the respondent has not completed primary)\n\nfstz23_sf$educ_primary &lt;- as.integer(fstz23_sf$education == \"Primary completed\" | fstz23_sf$education == \"Some secondary\" | fstz23_sf$education == \"Post primary technical training\")\nfstz23_sf$educ_secondary &lt;- as.integer(fstz23_sf$education == \"Secondary competed-O level\" | fstz23_sf$education == \"Secondary completed-A level\" | fstz23_sf$education == \"Some University or other higher education\")\nfstz23_sf$educ_tertiary &lt;- as.integer(fstz23_sf$education == \"University or higher education completed\")\n\nFor land ownership, we define start with four levels to denote whether the respondent personally owns the land, the land is owned by family or shared with someone, the land is rented, or the land is neither owned nor rented.\n\nfstz23_sf$land_self_own &lt;- as.integer(fstz23_sf$land_own == \"You personally own the land/plot where you live\")\nfstz23_sf$land_hh_or_shared &lt;- as.integer(fstz23_sf$land_own == \"You own the land/plot together with someone else\" | fstz23_sf$land_own == \"Other A household members owns the land/plot\")\nfstz23_sf$land_rented &lt;- as.integer(fstz23_sf$land_own == \"The land/plot is rented\")\n\nFor sources of income, we see that the largest group is “Farmers and Fishers” (3232), “Piece-work or Casual Labor” (2559) and “Dependents” (1960). We will keep these three as distinct levels. We can then define traders (861), salaried (635) and all other sources excuding welfare, gambling, pension, and service providers. We will take service providers to be very similar to casual labor as it also counts as an irregular source of funds.\n\nfstz23_sf$income_farm_and_fish &lt;- as.integer(fstz23_sf$income_source == \"Farmers and fishers\")\nfstz23_sf$income_piecework &lt;- as.integer(fstz23_sf$income_source == \"Piece work/casual labor\" | fstz23_sf$income_source == \"Service providers\")\nfstz23_sf$income_dependent &lt;- as.integer(fstz23_sf$income_source == \"Dependents\")\nfstz23_sf$income_trader &lt;- as.integer(fstz23_sf$income_source == \"Traders - non-agricultural\" | fstz23_sf$income_source == \"Traders - agricultural products\")\nfstz23_sf$income_salaried &lt;- as.integer(fstz23_sf$income_source == \"Formal sector salaried\" | fstz23_sf$income_source == \"Informal sector salaried\")\nfstz23_sf$income_other &lt;- as.integer(fstz23_sf$income_source == \"Other\" | fstz23_sf$income_source == \"Rental income\" | fstz23_sf$income_source == \"Interest from savings, investments, stocks, unit trusts etc.\")\n\nThese recoded variables will now be used for model calibrations instead of the original variables.\n\n\n\nB.4.3 Creation of overall measures\nThere are currently three different variables for financial inclusion looking at three different dimensions of financial inclusion. We can create an overall financial inclusion variable to indicate if the respondent is included in any of the three different dimensions of FI.\nWe use the following code to create a new variable which returns 1 if any of the three variables is 1, otherwise it returns zero. As the three original variables are in zero and one, we can use the logical or operator (|) to implement this operation. We then use as.integer() to convert the result from logical to a zero-one integer.\n\nfstz23_sf$fi_overall &lt;- as.integer(fstz23_sf$fi_banked | fstz23_sf$fi_formal | fstz23_sf$fi_informal)\n\nWe can also do the same for the different categories of physical impairment and create a single variable that combines it all.\n\nfstz23_sf$any_impaired &lt;- as.integer(fstz23_sf$visual_impaired | fstz23_sf$hearing_impaired | fstz23_sf$comm_impaired |\n                                       fstz23_sf$move_impaired | fstz23_sf$daily_impaired)\n\n\n\nB.4.4 Converting fstz23_sf into an sf dataframe\nThe object fstz23_sf still does not include any geospatial information and cannot be used later for geographically weighted modelling. To solve this, we can use the district centroids as the point location of the respondents. We first use left_join() to import the point geometry of the centroids, and then we use st_as_sf() in order to make sure that the new object is recognized as an sf dataframe.\n\nfstz23_sf &lt;- left_join(fstz23_sf, tz_dist_centroids, by = \"district\") %&gt;%\n  st_as_sf()\n\nWe can check if the geometries are properly mapped by plotting the respondents onto the boundary map using tmap package.\n\ntm_shape(tz_dist) +\n  tm_polygons(\"grey\") +\ntm_shape(fstz23_sf) +\n  tm_dots(\"blue\", size = 0.2) +\n  tm_layout(title = \"Respondents\",\n            title.position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\nThe respondents appear to be properly mapped to the district centroids, however, duplicate locations will be unacceptable for the methods we will use for geographically weighted modelling later. To solver this, we can slightly shift the points by introducing st_jitter(). For the amount argument, we use a value of 1000 which means that points will be shifted by up to 1km from their original point. This 1km should not be an issue and will not cause points to go beyond the district boundary.\n\nfstz23_sf &lt;- st_jitter(fstz23_sf, 1000)\n\nWe can doublecheck that the operation is successful and there are no duplicated locations by using duplicated() to check if a value is duplicated and then using any() to check if the function returned true for any value.\n\nany(duplicated(fstz23_sf$geometry))\n\n[1] FALSE\n\n\nThe code hase returned FALSE so we are assured that there are no duplicated point locations."
  },
  {
    "objectID": "posts/tz-fin-2023/index.html#c.1-respondent-age",
    "href": "posts/tz-fin-2023/index.html#c.1-respondent-age",
    "title": "Geographically Weighted Modeling of Financial Inclusion in Tanzania",
    "section": "C.1 Respondent Age",
    "text": "C.1 Respondent Age\nThe age variable in the survey data is the only numeric variable retained. We can use the following code to produce a histogram to show the distribution of values of this variable. We use ggplot package to produce the chart, but we include the central measures– the mean, median, mode, as captions for additional insights.\n\nggplot(fstz23_sf, aes(x = age)) +\n  geom_histogram(binwidth = 5, fill = \"#4A90E2\", color = \"black\") +\n  labs(title = \"Age Distribution of Respondents\",\n       x = \"Respondent Age\",\n       y = \"Number of Respondents\",\n       caption = paste(\"Mean =\", round(mean(fstz23_sf$age, na.rm = TRUE), 1), \n                       \", Median =\", round(median(fstz23_sf$age, na.rm = TRUE), 1), \n                       \", Mode =\", as.numeric(names(sort(table(fstz23_sf$age), decreasing = TRUE)[1])))) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    axis.title.x = element_text(size = 12),\n    axis.title.y = element_text(size = 12),\n    axis.text = element_text(size = 10)\n  )\n\n\n\n\n\n\n\n\nWe can also display the summary statistics using the summary() function.\n\nsummary(fstz23_sf$age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  16.00   27.00   37.00   39.68   50.00  100.00 \n\n\nThe outputs show that the ages range from 16 to 100 and is right skewed. The distribution has a mean of ~40yrs and a mode of 30 yrs. Given the shape of the distribution, we can consider scaled versions of the variable when we calibrate the model later."
  },
  {
    "objectID": "posts/tz-fin-2023/index.html#c.2-financial-inclusion-measures",
    "href": "posts/tz-fin-2023/index.html#c.2-financial-inclusion-measures",
    "title": "Geographically Weighted Modeling of Financial Inclusion in Tanzania",
    "section": "C.2 Financial Inclusion Measures",
    "text": "C.2 Financial Inclusion Measures\nWe currently have four variables that give an indication of whether the respondent is financially included. For this study, we want to limit to one, or at most two variables. We expect that some of the variables are highly correlated, while some will perform much better in a model than others.\nWe first check the overall distribution or proportion of respondents across these four measures. We use ggplot package to create a bar chart to show the proportion of respondents achieving financial inclusion based on each dimension. The first part of the code computes for the proportion numbers as plotting the data directly will result in counts rather than percentages.\n\n# Calculate the proportions for each variable\nproportions &lt;-  st_drop_geometry(fstz23_sf) %&gt;%\n  summarise(\n    fi_banked = mean(fi_banked, na.rm = TRUE) * 100,\n    fi_formal = mean(fi_formal, na.rm = TRUE) * 100,\n    fi_informal = mean(fi_informal, na.rm = TRUE) * 100,\n    fi_overall = mean(fi_overall, na.rm = TRUE) * 100\n  )\n\n# Convert the proportions to a long format for ggplot2\nproportions_long &lt;- proportions %&gt;%\n  pivot_longer(cols = everything(), names_to = \"variable\", values_to = \"value\")\n\n# Create the bar chart\nggplot(proportions_long, aes(x = variable, y = value, fill = variable)) +\n  geom_bar(stat = \"identity\", color = \"black\") +\n  geom_text(aes(label = round(value, 1)), vjust = -0.5, size = 4) +\n  scale_y_continuous(labels = scales::percent_format(scale = 1)) +\n  labs(title = \"Proportion of Respondents for Financial Inclusion Variables\",\n       x = \"\",\n       y = \"Percentage\",\n       fill = \"Variable\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    axis.title.y = element_text(size = 12),\n    axis.text = element_text(size = 10),\n    legend.position = \"none\"\n  )\n\n\n\n\n\n\n\n\nThe output shows that only 20.5% of the respondents are banked, the lowest across the three main dimensions. It also shows that the difference between formal and overall financial inclusion is a difference of 7%. This means that only 7% of respondents are banked or using informal FI instruments, but are not using formal instruments.\nWe can visualize the correlation based on overlapping values across the three dimensions. Visually, overlaps can be visualized using Venn diagrams, but we can also use an upset chart from UpSetR package. This visualization is more scalable than venn diagrams, which is not really an issue since we only have three categories. This chart makes it much easier to compare intersections and non-intersections against each other.\nIn the code chunk below, we load the UpSetR package, and then prepare the data so that we only have the three variables. The preparation ensures that the variables are in 0-1 integers which is the required format for the function. We then use upset() from UpSetR package to produce the chart by passing the data and defining the variables to be plotted.\n\nlibrary(UpSetR)\n\n# Create a binary dataframe\noverlap_data &lt;- as.data.frame(st_drop_geometry(fstz23_sf)) %&gt;%\n  mutate(\n    Banked = as.integer(fi_banked == 1),\n    Formal = as.integer(fi_formal == 1),\n    Informal = as.integer(fi_informal == 1)\n  ) %&gt;%\n  select(Banked, Formal, Informal)\n\n# Create the UpSet plot\nupset(overlap_data, sets = c(\"Banked\", \"Formal\", \"Informal\"),\n      keep.order = TRUE, order.by = \"freq\", number.angles = 45,\n      main.bar.color = \"blue\", sets.bar.color = \"red\",\n      text.scale = c(1.5, 1.5, 1.5, 1, 1.5, 1.5),\n      mainbar.y.label = \"Intersection Size\", sets.x.label = \"Set Size\")\n\n\n\n\n\n\n\n\nThe resulting chart shows that:\n\nAll banked respondents are also financial included based on formal instruments. (bank is a subset or category under formal instruments)\nThere are 663 respondents (~7%) that are financially included based on informal instruments, but not based on formal instruments\nThere are 2812 respondents (~28%) that are financially included based on formal instruments, but not based on informal instruments\nThere are 3836 respondents (~38%) that are financially included based on both formal and informal instruments\n\nWe do not have enough to already exclude any of these variables, but we expect that the overall measure covers too much of the sample to be predictable. We also expect that the informal FI measure has too much overlap with formal so we would likely choose one but not the other."
  },
  {
    "objectID": "posts/tz-fin-2023/index.html#c.3-correlation-of-predictors",
    "href": "posts/tz-fin-2023/index.html#c.3-correlation-of-predictors",
    "title": "Geographically Weighted Modeling of Financial Inclusion in Tanzania",
    "section": "C.3 Correlation of predictors",
    "text": "C.3 Correlation of predictors\nWe can check if any of the predictors are highly correlated as the precence of autocorrelation affects the performance and interpretation of the model.\nWe can do this by producing the correlation plot of the potential predictor variables. We use ggcorrmat() of ggstatsplot package to produce this. We pass a dataframe with just the predictor variables and the code will output a diagonal matrix with the correlation coeficients between each pair of variables.\n\nggcorrmat(select(st_drop_geometry(fstz23_sf),\n                 -c(region, district, ward, maritalstatus, education, land_own, income_source,\n                    fi_banked, fi_formal, fi_informal, fi_overall)))\n\n\n\n\n\n\n\n\nThe code outputs a large matrix, but we only need to focus on pairs where the correlation coefficient is high. (i.e., &gt; 0.8) Those are:\n\nage and age_standardized = 1 - This is expected as one is just a transformation. We will keep them both for now as we want to see which one will result to a better model.\nany_impaired and visual_impaired = 0.81 - This is also expected as any_impaired is a derived variable. It looks like most of the impairment reported is visual in nature. We will then drop the derived variable\nincome_salaried and reg_job = 0.95 - This appears to be redundant variables and likely refer to the same condition. We should be able to drop the latter\nproduction and income_farm_and_fish = 0.71 - while not as high as the last pair, this pair most likely also refers to the same type of work. We will follow the same approach so we only keep the variables prefixed by income in the calibration\n\nBased on those, we can clean up our dataset by dropping the three variables mentioned above. We can also drop the variables we don’t need which include the categorical variables that we already have created new variables for. We perform this with the use of the select() function and using a “-” to exclude rather than select columns.\n\nfstz23_sf &lt;- select(fstz23_sf,-c(any_impaired, reg_job, production,\n                                 maritalstatus, education, land_own, income_source))"
  },
  {
    "objectID": "posts/tz-fin-2023/index.html#d.1-global-models-without-variable-selection",
    "href": "posts/tz-fin-2023/index.html#d.1-global-models-without-variable-selection",
    "title": "Geographically Weighted Modeling of Financial Inclusion in Tanzania",
    "section": "D.1 Global Models without Variable Selection",
    "text": "D.1 Global Models without Variable Selection\nTo calibrate the model, we use glm() which calibrates generalised linear models. As the dependent variable is binary, we need to make sure that the model used is a logistic regression model. This is done by passing the value “binomial” to the family argument.\nWe will run this with all variables for all four FI measures to see if any of them are performing very well or very poorly against the others. We will then focus on finetuning and then preparing the geographically weighted models on those variables that can be best explained with this approach.\n\nD.1.1 Global Model for Formal FI (no variable selection)\nThe code below calibrates a model with all variables with fi_formal as the dependent variable. We then use summary() to output the results. While the results display the AIC as a measure, we also compute for the reduction in variance due to the predictors. This is done by comparing the deviance and the null deviance. A higher number means that the variables had a larger contribution in predicting or explaining the value of the dependent variable.\n\nfi_formal.lr &lt;- glm(fi_formal ~ urban + age + female + head_hh + visual_impaired + hearing_impaired + comm_impaired +\n                      move_impaired + daily_impaired + cogn_impaired + agricultural + mobile + internet +\n                      own_mobile + has_id + no_income + is_married + was_married+\n                      educ_primary + educ_secondary + educ_tertiary +\n                      land_self_own + land_hh_or_shared + land_rented +\n                      income_farm_and_fish + income_piecework + income_dependent + income_trader + income_salaried +\n                      income_other,\n                    family = \"binomial\",\n                    data = fstz23_sf)\nsummary(fi_formal.lr)\n\n\nCall:\nglm(formula = fi_formal ~ urban + age + female + head_hh + visual_impaired + \n    hearing_impaired + comm_impaired + move_impaired + daily_impaired + \n    cogn_impaired + agricultural + mobile + internet + own_mobile + \n    has_id + no_income + is_married + was_married + educ_primary + \n    educ_secondary + educ_tertiary + land_self_own + land_hh_or_shared + \n    land_rented + income_farm_and_fish + income_piecework + income_dependent + \n    income_trader + income_salaried + income_other, family = \"binomial\", \n    data = fstz23_sf)\n\nCoefficients:\n                      Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)          -0.909394   0.313346  -2.902 0.003705 ** \nurban                 0.632501   0.080982   7.810 5.70e-15 ***\nage                   0.005744   0.002538   2.263 0.023640 *  \nfemale                0.046569   0.074759   0.623 0.533335    \nhead_hh               0.178742   0.087275   2.048 0.040557 *  \nvisual_impaired      -0.024436   0.095515  -0.256 0.798080    \nhearing_impaired     -0.291409   0.158199  -1.842 0.065469 .  \ncomm_impaired         0.275147   0.337988   0.814 0.415603    \nmove_impaired         0.072813   0.122261   0.596 0.551477    \ndaily_impaired        0.111153   0.201332   0.552 0.580887    \ncogn_impaired         0.144262   0.159558   0.904 0.365924    \nagricultural          0.030786   0.084129   0.366 0.714411    \nmobile                0.549352   0.082138   6.688 2.26e-11 ***\ninternet              0.579452   0.081901   7.075 1.49e-12 ***\nown_mobile            1.902444   0.070026  27.168  &lt; 2e-16 ***\nhas_id               -0.675854   0.089887  -7.519 5.52e-14 ***\nno_income            -0.583239   0.122630  -4.756 1.97e-06 ***\nis_married           -0.034435   0.098156  -0.351 0.725726    \nwas_married          -0.194584   0.134307  -1.449 0.147392    \neduc_primary          0.663236   0.064346  10.307  &lt; 2e-16 ***\neduc_secondary        1.492458   0.128363  11.627  &lt; 2e-16 ***\neduc_tertiary         2.714657   0.723016   3.755 0.000174 ***\nland_self_own         0.170538   0.095351   1.789 0.073690 .  \nland_hh_or_shared     0.081153   0.085445   0.950 0.342232    \nland_rented           0.375301   0.137679   2.726 0.006412 ** \nincome_farm_and_fish -0.692323   0.268336  -2.580 0.009878 ** \nincome_piecework     -0.791856   0.269060  -2.943 0.003250 ** \nincome_dependent     -0.874045   0.275270  -3.175 0.001497 ** \nincome_trader        -0.026406   0.300939  -0.088 0.930079    \nincome_salaried      -0.157785   0.344056  -0.459 0.646518    \nincome_other         -0.660914   0.403607  -1.638 0.101522    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 11074.6  on 9907  degrees of freedom\nResidual deviance:  7563.1  on 9877  degrees of freedom\nAIC: 7625.1\n\nNumber of Fisher Scoring iterations: 7\n\n1 - summary(fi_formal.lr)$deviance / summary(fi_formal.lr)$null.deviance \n\n[1] 0.3170735\n\n\n\n\nD.1.2 Global Model for Banked FI (no variable selection)\nThe code below calibrates a model with all variables with fi_banked as the dependent variable.\n\nfi_banked.lr &lt;- glm(fi_banked ~ urban + age + female + head_hh + visual_impaired + hearing_impaired + comm_impaired +\n                      move_impaired + daily_impaired + cogn_impaired + agricultural + mobile + internet +\n                      own_mobile + has_id + no_income + is_married + was_married+\n                      educ_primary + educ_secondary + educ_tertiary +\n                      land_self_own + land_hh_or_shared + land_rented +\n                      income_farm_and_fish + income_piecework + income_dependent + income_trader + income_salaried +\n                      income_other,\n                    family = \"binomial\",\n                    data = fstz23_sf)\nsummary(fi_banked.lr)\n\n\nCall:\nglm(formula = fi_banked ~ urban + age + female + head_hh + visual_impaired + \n    hearing_impaired + comm_impaired + move_impaired + daily_impaired + \n    cogn_impaired + agricultural + mobile + internet + own_mobile + \n    has_id + no_income + is_married + was_married + educ_primary + \n    educ_secondary + educ_tertiary + land_self_own + land_hh_or_shared + \n    land_rented + income_farm_and_fish + income_piecework + income_dependent + \n    income_trader + income_salaried + income_other, family = \"binomial\", \n    data = fstz23_sf)\n\nCoefficients:\n                      Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)          -2.037948   0.289783  -7.033 2.03e-12 ***\nurban                 0.487211   0.070234   6.937 4.01e-12 ***\nage                   0.008988   0.002742   3.278 0.001046 ** \nfemale               -0.244823   0.073536  -3.329 0.000871 ***\nhead_hh               0.429123   0.084337   5.088 3.62e-07 ***\nvisual_impaired       0.220218   0.092799   2.373 0.017641 *  \nhearing_impaired     -0.545221   0.215985  -2.524 0.011592 *  \ncomm_impaired         0.168385   0.482018   0.349 0.726839    \nmove_impaired        -0.322852   0.143402  -2.251 0.024361 *  \ndaily_impaired        0.103399   0.261231   0.396 0.692242    \ncogn_impaired        -0.163345   0.189727  -0.861 0.389267    \nagricultural         -0.177993   0.076526  -2.326 0.020024 *  \nmobile               -0.054909   0.136147  -0.403 0.686723    \ninternet              0.628532   0.063778   9.855  &lt; 2e-16 ***\nown_mobile            0.856049   0.122809   6.971 3.16e-12 ***\nhas_id               -1.180586   0.146446  -8.062 7.53e-16 ***\nno_income            -0.091093   0.158109  -0.576 0.564522    \nis_married           -0.082966   0.090487  -0.917 0.359204    \nwas_married          -0.397742   0.126719  -3.139 0.001697 ** \neduc_primary          0.796899   0.086567   9.206  &lt; 2e-16 ***\neduc_secondary        1.700194   0.107407  15.829  &lt; 2e-16 ***\neduc_tertiary         3.121148   0.208886  14.942  &lt; 2e-16 ***\nland_self_own         0.272392   0.094898   2.870 0.004100 ** \nland_hh_or_shared     0.162165   0.093024   1.743 0.081288 .  \nland_rented           0.190437   0.108746   1.751 0.079909 .  \nincome_farm_and_fish -1.599462   0.207346  -7.714 1.22e-14 ***\nincome_piecework     -1.950615   0.209724  -9.301  &lt; 2e-16 ***\nincome_dependent     -1.893223   0.225727  -8.387  &lt; 2e-16 ***\nincome_trader        -1.546829   0.219310  -7.053 1.75e-12 ***\nincome_salaried      -0.644514   0.226550  -2.845 0.004442 ** \nincome_other         -1.345573   0.301468  -4.463 8.07e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 10056.8  on 9907  degrees of freedom\nResidual deviance:  7550.3  on 9877  degrees of freedom\nAIC: 7612.3\n\nNumber of Fisher Scoring iterations: 6\n\n1 - summary(fi_banked.lr)$deviance / summary(fi_banked.lr)$null.deviance \n\n[1] 0.2492412\n\n\n\n\nD.1.3 Global Model for Informal FI (no variable selection)\nThe code below calibrates a model with all variables with fi_informal as the dependent variable.\n\nfi_informal.lr &lt;- glm(fi_informal ~ urban + age + female + head_hh + visual_impaired + hearing_impaired + comm_impaired +\n                      move_impaired + daily_impaired + cogn_impaired + agricultural + mobile + internet +\n                      own_mobile + has_id + no_income + is_married + was_married+\n                      educ_primary + educ_secondary + educ_tertiary +\n                      land_self_own + land_hh_or_shared + land_rented +\n                      income_farm_and_fish + income_piecework + income_dependent + income_trader + income_salaried +\n                      income_other,\n                    family = \"binomial\",\n                    data = fstz23_sf)\nsummary(fi_informal.lr)\n\n\nCall:\nglm(formula = fi_informal ~ urban + age + female + head_hh + \n    visual_impaired + hearing_impaired + comm_impaired + move_impaired + \n    daily_impaired + cogn_impaired + agricultural + mobile + \n    internet + own_mobile + has_id + no_income + is_married + \n    was_married + educ_primary + educ_secondary + educ_tertiary + \n    land_self_own + land_hh_or_shared + land_rented + income_farm_and_fish + \n    income_piecework + income_dependent + income_trader + income_salaried + \n    income_other, family = \"binomial\", data = fstz23_sf)\n\nCoefficients:\n                     Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)          -0.85492    0.22360  -3.823 0.000132 ***\nurban                 0.06146    0.05511   1.115 0.264748    \nage                  -0.01826    0.00197  -9.270  &lt; 2e-16 ***\nfemale                0.09939    0.05470   1.817 0.069206 .  \nhead_hh               0.50994    0.06308   8.084 6.29e-16 ***\nvisual_impaired       0.17875    0.07094   2.520 0.011748 *  \nhearing_impaired      0.03574    0.13339   0.268 0.788769    \ncomm_impaired        -0.33005    0.30810  -1.071 0.284058    \nmove_impaired         0.05833    0.09647   0.605 0.545371    \ndaily_impaired       -0.19622    0.17527  -1.120 0.262915    \ncogn_impaired         0.09270    0.12797   0.724 0.468830    \nagricultural          0.10476    0.05926   1.768 0.077070 .  \nmobile                0.43162    0.07583   5.691 1.26e-08 ***\ninternet              0.40287    0.05307   7.591 3.19e-14 ***\nown_mobile            0.31775    0.06393   4.970 6.68e-07 ***\nhas_id               -0.46332    0.07398  -6.263 3.77e-10 ***\nno_income            -0.52889    0.10286  -5.142 2.72e-07 ***\nis_married            0.52809    0.06802   7.764 8.24e-15 ***\nwas_married           0.21085    0.09467   2.227 0.025933 *  \neduc_primary          0.20200    0.05155   3.919 8.90e-05 ***\neduc_secondary        0.39907    0.07977   5.003 5.64e-07 ***\neduc_tertiary         0.63482    0.16032   3.960 7.50e-05 ***\nland_self_own        -0.18105    0.06966  -2.599 0.009349 ** \nland_hh_or_shared     0.13379    0.06473   2.067 0.038751 *  \nland_rented           0.02514    0.08747   0.287 0.773773    \nincome_farm_and_fish  0.20099    0.17616   1.141 0.253888    \nincome_piecework      0.13475    0.17689   0.762 0.446201    \nincome_dependent     -0.20958    0.18466  -1.135 0.256397    \nincome_trader         0.45674    0.18825   2.426 0.015254 *  \nincome_salaried       0.42832    0.19652   2.180 0.029291 *  \nincome_other          0.32673    0.26233   1.246 0.212942    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 13683  on 9907  degrees of freedom\nResidual deviance: 12417  on 9877  degrees of freedom\nAIC: 12479\n\nNumber of Fisher Scoring iterations: 4\n\n1 - summary(fi_informal.lr)$deviance / summary(fi_informal.lr)$null.deviance \n\n[1] 0.09256485\n\n\n\n\nD.1.4 Global Model for Overall FI (no variable selection)\nThe code below calibrates a model with all variables with fi_overall as the dependent variable.\n\nfi_overall.lr &lt;- glm(fi_overall ~ urban + age + female + head_hh + visual_impaired + hearing_impaired + comm_impaired +\n                      move_impaired + daily_impaired + cogn_impaired + agricultural + mobile + internet +\n                      own_mobile + has_id + no_income + is_married + was_married+\n                      educ_primary + educ_secondary + educ_tertiary +\n                      land_self_own + land_hh_or_shared + land_rented +\n                      income_farm_and_fish + income_piecework + income_dependent + income_trader + income_salaried +\n                      income_other,\n                    family = \"binomial\",\n                    data = fstz23_sf)\nsummary(fi_overall.lr)\n\n\nCall:\nglm(formula = fi_overall ~ urban + age + female + head_hh + visual_impaired + \n    hearing_impaired + comm_impaired + move_impaired + daily_impaired + \n    cogn_impaired + agricultural + mobile + internet + own_mobile + \n    has_id + no_income + is_married + was_married + educ_primary + \n    educ_secondary + educ_tertiary + land_self_own + land_hh_or_shared + \n    land_rented + income_farm_and_fish + income_piecework + income_dependent + \n    income_trader + income_salaried + income_other, family = \"binomial\", \n    data = fstz23_sf)\n\nCoefficients:\n                      Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)          -0.055692   0.333888  -0.167 0.867527    \nurban                 0.516681   0.086899   5.946 2.75e-09 ***\nage                  -0.001405   0.002650  -0.530 0.595996    \nfemale                0.024066   0.078557   0.306 0.759332    \nhead_hh               0.235402   0.093558   2.516 0.011866 *  \nvisual_impaired       0.150577   0.103060   1.461 0.144000    \nhearing_impaired     -0.114633   0.163713  -0.700 0.483798    \ncomm_impaired         0.033323   0.328379   0.101 0.919171    \nmove_impaired         0.007001   0.126653   0.055 0.955920    \ndaily_impaired       -0.002848   0.200755  -0.014 0.988681    \ncogn_impaired         0.204732   0.165965   1.234 0.217357    \nagricultural          0.043097   0.087983   0.490 0.624249    \nmobile                0.517871   0.081228   6.376 1.82e-10 ***\ninternet              0.554273   0.091495   6.058 1.38e-09 ***\nown_mobile            1.524108   0.074932  20.340  &lt; 2e-16 ***\nhas_id               -0.592184   0.090867  -6.517 7.17e-11 ***\nno_income            -0.620211   0.119034  -5.210 1.88e-07 ***\nis_married            0.254598   0.101106   2.518 0.011798 *  \nwas_married           0.096682   0.140398   0.689 0.491059    \neduc_primary          0.563723   0.068504   8.229  &lt; 2e-16 ***\neduc_secondary        1.393584   0.141706   9.834  &lt; 2e-16 ***\neduc_tertiary         2.338111   0.722148   3.238 0.001205 ** \nland_self_own         0.108214   0.101402   1.067 0.285892    \nland_hh_or_shared     0.210526   0.088977   2.366 0.017979 *  \nland_rented           0.319084   0.146909   2.172 0.029857 *  \nincome_farm_and_fish -0.616538   0.290245  -2.124 0.033654 *  \nincome_piecework     -0.684558   0.290886  -2.353 0.018605 *  \nincome_dependent     -1.042797   0.295604  -3.528 0.000419 ***\nincome_trader        -0.036161   0.329672  -0.110 0.912657    \nincome_salaried      -0.187959   0.374037  -0.503 0.615305    \nincome_other         -0.262658   0.473406  -0.555 0.579014    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 9339.8  on 9907  degrees of freedom\nResidual deviance: 6817.2  on 9877  degrees of freedom\nAIC: 6879.2\n\nNumber of Fisher Scoring iterations: 7\n\n1 - summary(fi_overall.lr)$deviance / summary(fi_overall.lr)$null.deviance\n\n[1] 0.2700872\n\n\n\n\nD.1.5 Choosing a dependent variable based on global model calibration\nThe different models showed that the one using fi_formal and fi_overall as the dependent variable performed better than the other two. Since fi_formal gave the best reduction in deviance, and we have seen earlier that the other fi measures overlap with fi_formal anyway, we will focus on fi_formal as the main indicator for which we will finetune the model."
  },
  {
    "objectID": "posts/tz-fin-2023/index.html#d.2-variable-selection-for-the-global-model",
    "href": "posts/tz-fin-2023/index.html#d.2-variable-selection-for-the-global-model",
    "title": "Geographically Weighted Modeling of Financial Inclusion in Tanzania",
    "section": "D.2 Variable selection for the global model",
    "text": "D.2 Variable selection for the global model\nFrom the model results, we see that not all the variables contribute in the same degree as the others. We can use the output to pick the significant variables or perform a technique like forward, backward or stepwise regression to select variables by introducing or removing them one at a time.\nForward regression can be done using ols_step_forward_p() of the olsrr package. The function takes in the full model and starts from an empty model and adds variables with the highest significance one at a time. This continues doing this as long as variables with significance less than the specified p-value can be added.\n\nfi_formal_fw_mlr &lt;- ols_step_forward_p(fi_formal.lr, p_val = 0.05, details = FALSE)\n\nWe can also display the results by calling the resulting object.\n\nfi_formal_fw_mlr\n\n\n                                      Stepwise Summary                                      \n------------------------------------------------------------------------------------------\nStep    Variable             AIC          SBC             SBIC            R2       Adj. R2 \n------------------------------------------------------------------------------------------\n 0      Base Model        11452.392    11466.795     -5643875908.953    0.00000    0.00000 \n 1      own_mobile         8001.162     8022.765    -11332019050.848    0.29427    0.29420 \n 2      has_id             7826.777     7855.582    -11742615711.514    0.30673    0.30659 \n 3      urban              7661.916     7697.921    -12144725901.394    0.31830    0.31810 \n 4      mobile             7569.414     7612.621    -12378471273.642    0.32478    0.32450 \n 5      internet           7492.544     7542.952    -12576969271.789    0.33013    0.32979 \n 6      no_income          7432.878     7490.487    -12734351633.418    0.33429    0.33388 \n 7      educ_secondary     7382.226     7447.036    -12870263096.019    0.33781    0.33735 \n 8      educ_primary       7285.271     7357.282    -13129780806.723    0.34439    0.34386 \n 9      educ_tertiary      7255.572     7334.784    -13213906836.776    0.34649    0.34589 \n 10     age                7243.236     7329.649    -13252042393.875    0.34743    0.34677 \n 11     income_trader      7234.121     7327.736    -13281646286.889    0.34817    0.34744 \n 12     head_hh            7230.229     7331.044    -13297292381.943    0.34855    0.34776 \n------------------------------------------------------------------------------------------\n\nFinal Model Output \n------------------\n\n                         Model Summary                           \n----------------------------------------------------------------\nR                       0.590       RMSE                  0.348 \nR-Squared               0.349       MSE                   0.121 \nAdj. R-Squared          0.348       Coef. Var            46.241 \nPred R-Squared          0.347       AIC                7230.229 \nMAE                     0.249       SBC                7331.044 \n----------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                                 ANOVA                                  \n-----------------------------------------------------------------------\n                Sum of                                                 \n               Squares          DF    Mean Square       F         Sig. \n-----------------------------------------------------------------------\nRegression     642.088          12         53.507    441.188    0.0000 \nResidual      1200.065        9895          0.121                      \nTotal         1842.153        9907                                     \n-----------------------------------------------------------------------\n\n                                    Parameter Estimates                                     \n-------------------------------------------------------------------------------------------\n         model      Beta    Std. Error    Std. Beta      t        Sig      lower     upper \n-------------------------------------------------------------------------------------------\n   (Intercept)     0.236         0.017                 14.240    0.000     0.204     0.269 \n    own_mobile     0.388         0.011        0.388    36.683    0.000     0.367     0.408 \n        has_id    -0.114         0.011       -0.094    -9.947    0.000    -0.136    -0.091 \n         urban     0.061         0.008        0.067     7.537    0.000     0.045     0.077 \n        mobile     0.108         0.012        0.088     8.912    0.000     0.084     0.132 \n      internet     0.053         0.009        0.055     6.158    0.000     0.036     0.069 \n     no_income    -0.098         0.013       -0.066    -7.601    0.000    -0.123    -0.072 \neduc_secondary     0.162         0.013        0.133    12.745    0.000     0.137     0.187 \n  educ_primary     0.099         0.009        0.115    11.673    0.000     0.083     0.116 \n educ_tertiary     0.134         0.023        0.053     5.924    0.000     0.090     0.179 \n           age     0.001         0.000        0.028     2.850    0.004     0.000     0.001 \n income_trader     0.042         0.013        0.028     3.321    0.001     0.017     0.067 \n       head_hh     0.019         0.008        0.022     2.426    0.015     0.004     0.035 \n-------------------------------------------------------------------------------------------\n\n\nThe results show that the global model for fi_formal includes 12 explanatory variables which consist of variables for:\n\nMobile phone usage and ownership and internet access (3 variables) - has the highest combined weight\nEducation (3 variables)\nOther positive coefficient: urban, age, trader, head_hh\nNegative coefficients: no source of income, no id\n\nThe last variable is surprising as it implies that having an id is linked to a lower probability of being financially included. The model did not pick up gender which means that it doesn’t see a clear distinction between males and females for this dimension of financial inclusion."
  },
  {
    "objectID": "posts/tz-fin-2023/index.html#d.3-testing-for-spatial-autocorrelation",
    "href": "posts/tz-fin-2023/index.html#d.3-testing-for-spatial-autocorrelation",
    "title": "Geographically Weighted Modeling of Financial Inclusion in Tanzania",
    "section": "D.3 Testing for spatial autocorrelation",
    "text": "D.3 Testing for spatial autocorrelation\nBefore calibrating the geographically weighted model, we can check if there is a pattern linking global model performance and the respondent’s location. One way to do this is to plot the residuals or error and check for any patterns.\nThe first step is to export relevant output from the model as a dataframe. We just extract the residuals by referencing the model object in the results.\n\nmlr_output &lt;- as.data.frame(fi_formal_fw_mlr$model$residuals) %&gt;%\n  rename('FW_MLR_RES' = 'fi_formal_fw_mlr$model$residuals')\n\nWe then import these into fstz23_sf as a new variable MLR_RES using cbind(). This function adds the dataframe as new columns in their current order.\n\nfstz23_sf &lt;- cbind(fstz23_sf,\n                         mlr_output$FW_MLR_RES) %&gt;%\n  rename('MLR_RES' = 'mlr_output.FW_MLR_RES')\n\nWe remember that our respondent data only have the district s their location so dissplaying them as points might not be very meaningful or accurate. We can instead plot the residuals at a district level to see if there are any patterns arising from there.\nTo do this, we first compute for the average residual at a district level by using group_by() to summarise the object by district and then define the aggregate function using summarise(). We use st_drop_geometry() so that the geometry column is dropped and we only have the district name and the average residual value.\n\navg_res_df &lt;- st_drop_geometry(fstz23_sf) %&gt;%\n  group_by(district) %&gt;%\n  summarise(avg_res = mean(MLR_RES, na.rm = TRUE))\n\nhead(avg_res_df)\n\n# A tibble: 6 × 2\n  district                 avg_res\n  &lt;chr&gt;                      &lt;dbl&gt;\n1 Arusha                    0.105 \n2 Arusha Urban              0.0168\n3 Babati                   -0.0522\n4 Babati UrbanBabati Urban -0.0185\n5 Bagamoyo                  0.0395\n6 Bahi                     -0.179 \n\n\nWe then export these average residual values into the TZ boundary map by using left_join() on the district name.\n\ntz_dist_stat &lt;- tz_dist_stat %&gt;%\n  left_join(avg_res_df, by= \"district\")\n\nWe can now use tmap package to visually display the average residual value per district. We again use two layers, and then exclude any districts where there is no residuals by using a mask with !is.na()\n\ntm_shape(tz_dist)+\n  tm_polygons(\"grey\") +\ntm_shape(tz_dist_stat[!is.na(tz_dist_stat$avg_res),]) +\n  tm_polygons(\"avg_res\", title = \"Average Residuals\")\n\nVariable(s) \"avg_res\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\n\n\n\n\nThe map shows some apparent clusters with positive (average) residuals and some with negative residuals. A large number of districts have relatively low average residuals between -0.1 and 0.1.\nWe can confirm this observation by running global Moran’s I test on the average residual value. In order to do this, we first need to compute for the neighbors and the weights for each of the district. We use st_knn() to derive neighbors using knn method with a parameter of 6 neighbors, and then use equal weights for neighbors using the st_weights() function. We perform these functions on the centroids object as these methods work on points rather than shapes.\n\ntz_dist_centroids &lt;- tz_dist_centroids %&gt;%\n  mutate(nb = st_knn(geometry, k = 6,\n                     longlat = FALSE),\n         wt = st_weights(nb,\n                         style = \"W\"),\n         .before = 1)\n\nWe then run the Moran’s I test with permutations using global_moran_perm() from sfdep package. Note that we use the avg_res from the map object but have kept the neighbor list and weights in the centrodis object. The nsim argument indicates that we are running 10 simulations for this test. We also replace any na values with zero as the code will not work with any na values.\n\nset.seed(1234)\nglobal_moran_perm(replace_na(tz_dist_stat$avg_res,0),\n                  tz_dist_centroids$nb,\n                  tz_dist_centroids$wt,\n                  alternative = \"two.sided\",\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.19584, observed rank = 100, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\nAs the results are significant, the test confirms that there is spatial autocorrelation for the average residual values across districts. The positive test statistic confirms our observation that the pattern is that of clustering. We should then build geographically weighted models as they are likely to produce better results as we account for the respondents’ locations."
  },
  {
    "objectID": "posts/tz-fin-2023/index.html#e.1-computing-a-bandwidth",
    "href": "posts/tz-fin-2023/index.html#e.1-computing-a-bandwidth",
    "title": "Geographically Weighted Modeling of Financial Inclusion in Tanzania",
    "section": "E.1 Computing a bandwidth",
    "text": "E.1 Computing a bandwidth\nThe first step in calibrating a geographically weighted model is determining the bandwidth to use. The choice can either be a fixed bandwidth which is based on distance, or an adaptive bandwidth which is based on the number of neighbors. For our case, we will opt for a fixed bandwidth since we have not precisely mapped the locations of the respondents, and there is a wide range of values for the number of respondents per district. A fixed bandwidth is likely to ensure that it captures most, if not all, of the points in the same district and also some in neighboring districts.\nTo compute for the optimum fixed bandwidth, we use bw.ggwr() of GWModel package. The approach argument defines the stopping rule to be used, which is cross validation in this case. Setting the adaptive argument to FALSE indicates that we are computing for the fixed bandwidth. Like the global model, we indicate binomial for the family argument to specify we are calibrating a logistic regression model.\nThe first part of the function is the formula for the model. To be sure of the details to put in here, one may use the formula() function on the model object of the forward regression output to display the final formula.\n\nbw_fixed &lt;- bw.ggwr(formula = fi_formal ~ own_mobile + has_id + urban + mobile + internet +\n                     no_income + educ_secondary + educ_primary + educ_tertiary +\n                     age + income_trader + head_hh,\n                  data = fstz23_sf,\n                  family = \"binomial\",\n                  approach = \"CV\",\n                  kernel = \"gaussian\",\n                  adaptive = FALSE,\n                  longlat = FALSE)\n\n# To display the global model's formula, you may use\n# formula(fi_formal_fw_mlr$model)\n\nThe output shows a recommended bandwidth of ~99.5km should be used. We save the output as an rds object to save our results and prevent the need to rerun the code again.\n\nwrite_rds(bw_fixed, \"data/rds/bw_fixed.rds\")"
  },
  {
    "objectID": "posts/tz-fin-2023/index.html#e.2-deriving-the-distance-matrix",
    "href": "posts/tz-fin-2023/index.html#e.2-deriving-the-distance-matrix",
    "title": "Geographically Weighted Modeling of Financial Inclusion in Tanzania",
    "section": "E.2 Deriving the distance matrix",
    "text": "E.2 Deriving the distance matrix\nCalibration of the logistics regression model requires a properly set up distance matrix. We use the code below which computes for this using gw.dist() on the coordinates of the data points. The coordinates function does not work on an sf dataframe so we convert it into Spatial format first.\n\ndistMAT &lt;- gw.dist(dp.locat=\n                     coordinates(as_Spatial(fstz23_sf)))"
  },
  {
    "objectID": "posts/tz-fin-2023/index.html#e.3-calibrating-the-fixed-bandwidth-model",
    "href": "posts/tz-fin-2023/index.html#e.3-calibrating-the-fixed-bandwidth-model",
    "title": "Geographically Weighted Modeling of Financial Inclusion in Tanzania",
    "section": "E.3 Calibrating the fixed bandwidth model",
    "text": "E.3 Calibrating the fixed bandwidth model\nWe can now calibrate the geographically weighted model using the computed bandwidthby using ggwr.basic() from GWModel. The function uses mostly the same arguments as the previous one, with the exception that the bandwidth now becomes an input here.\n\ngwr_fixed &lt;- ggwr.basic(formula = fi_formal ~ own_mobile + has_id + urban + mobile + internet +\n                     no_income + educ_secondary + educ_primary + educ_tertiary +\n                     age + income_trader + head_hh,\n                     data = fstz23_sf,\n                     family = \"binomial\",\n                     kernel = \"gaussian\",\n                     bw = bw_fixed,\n                     adaptive = FALSE,\n                     longlat = FALSE,\n                     dMat = distMAT)\n\nWe again save this object into an rds file to save our work for future runs.\n\nwrite_rds(gwr_fixed, \"data/rds/gwr_fixed.rds\")\n\nWe can show the results by calling the object as in the code chunk below.\n\ngwr_fixed\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2024-11-09 01:56:00.625357 \n   Call:\n   ggwr.basic(formula = fi_formal ~ own_mobile + has_id + urban + \n    mobile + internet + no_income + educ_secondary + educ_primary + \n    educ_tertiary + age + income_trader + head_hh, data = fstz23_sf, \n    bw = bw_fixed, family = \"binomial\", kernel = \"gaussian\", \n    adaptive = FALSE, longlat = FALSE, dMat = distMAT)\n\n   Dependent (y) variable:  fi_formal\n   Independent variables:  own_mobile has_id urban mobile internet no_income educ_secondary educ_primary educ_tertiary age income_trader head_hh\n   Number of data points: 9908\n   Used family: binomial\n   ***********************************************************************\n   *              Results of Generalized linear Regression               *\n   ***********************************************************************\n\nCall:\nNULL\n\nCoefficients:\n                Estimate Std. Error z value Pr(&gt;|z|)    \nIntercept      -1.524234   0.125637 -12.132  &lt; 2e-16 ***\nown_mobile      1.909171   0.069174  27.600  &lt; 2e-16 ***\nhas_id         -0.713992   0.083010  -8.601  &lt; 2e-16 ***\nurban           0.634941   0.074585   8.513  &lt; 2e-16 ***\nmobile          0.544885   0.081262   6.705 2.01e-11 ***\ninternet        0.599629   0.081309   7.375 1.65e-13 ***\nno_income      -0.713969   0.097385  -7.331 2.28e-13 ***\neduc_secondary  1.504240   0.125972  11.941  &lt; 2e-16 ***\neduc_primary    0.668065   0.063948  10.447  &lt; 2e-16 ***\neduc_tertiary   2.949785   0.719341   4.101 4.12e-05 ***\nage             0.005471   0.002060   2.655  0.00792 ** \nincome_trader   0.708805   0.148055   4.787 1.69e-06 ***\nhead_hh         0.192332   0.067661   2.843  0.00447 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 11074.6  on 9907  degrees of freedom\nResidual deviance:  7601.7  on 9895  degrees of freedom\nAIC: 7627.7\n\nNumber of Fisher Scoring iterations: 7\n\n\n AICc:  7627.753\n Pseudo R-square value:  0.31359\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Fixed bandwidth: 99550.75 \n   Regression points: the same locations as observations are used.\n   Distance metric: A distance matrix is specified for this model calibration.\n\n   ************Summary of Generalized GWR coefficient estimates:**********\n                        Min.    1st Qu.     Median    3rd Qu.   Max.\n   Intercept      -2.2553256 -1.7682889 -1.5998528 -1.3662353 0.2864\n   own_mobile      1.0280291  1.8126102  1.9268403  2.0854432 2.7884\n   has_id         -1.5130176 -0.9107411 -0.7352893 -0.6028037 0.4318\n   urban          -0.3774224  0.3679404  0.6370600  0.8034331 1.5909\n   mobile         -0.6754378  0.4583577  0.7072317  0.8739171 1.1907\n   internet       -0.4507688  0.3207454  0.6938241  0.8874637 1.2307\n   no_income      -1.6221173 -0.9028388 -0.7060301 -0.5112796 0.3790\n   educ_secondary  0.5492826  1.2938089  1.4288504  1.7448408 2.3738\n   educ_primary    0.1792122  0.4986158  0.6758896  0.8737680 1.2324\n   educ_tertiary   0.7005712  2.4681136  4.0382606  5.3483262 6.5277\n   age            -0.0341697  0.0025534  0.0057795  0.0108019 0.0220\n   income_trader   0.0135346  0.3952052  0.6642425  1.0400905 3.1913\n   head_hh        -0.3123380  0.0940548  0.1752028  0.2485868 0.4916\n   ************************Diagnostic information*************************\n   Number of data points: 9908 \n   GW Deviance: 7106.258 \n   AIC : 7548.682 \n   AICc : 7558.832 \n   Pseudo R-square value:  0.3583282 \n\n   ***********************************************************************\n   Program stops at: 2024-11-09 02:01:47.510664 \n\n\nThe output shows an improvement with the gleographically weighted model across the different performance measures. AICc improved from 7627.7 to 7558.832. The pseudo R-squared value improved from 0.31359 to 0.3583282.\nThe output also shows that the coefficient values vary widely across the local models. For some variables, there is also a change in sign across those value ranges– which implies that for some variables like mobile and internet access, they are detrimental to the respondent’s level of financial inclusion depending on their location.\nWe can analyze the gwr model further by accessing the details in the output."
  },
  {
    "objectID": "posts/tz-fin-2023/index.html#e.4-importing-gwr-model-results-into-an-sf-dataframe",
    "href": "posts/tz-fin-2023/index.html#e.4-importing-gwr-model-results-into-an-sf-dataframe",
    "title": "Geographically Weighted Modeling of Financial Inclusion in Tanzania",
    "section": "E.4 Importing gwr model results into an sf dataframe",
    "text": "E.4 Importing gwr model results into an sf dataframe\nThe gwr output includes details for every local model in an object called SDF. We can combine this with the geospatial information in order to be able to visualize the values of coefficients, residuals and fit measures at an individual location level. We use the following code chunk to extract SDF as a dataframe and then combine it with with fstz23_sf using cbind(). The code only retains the district names and the gwr model output by using select() to specify those columns.\n\ngwr_fixed_output &lt;- as.data.frame(gwr_fixed$SDF) %&gt;%\n  select(-c(geometry))\n\ngwr_sf_fixed &lt;- cbind(fstz23_sf, gwr_fixed_output) %&gt;%\n  select(2, (ncol(fstz23_sf)):(ncol(fstz23_sf)+ncol(gwr_fixed_output))) %&gt;%\n  st_drop_geometry()\n\nAs the local models might not be relevant on their own, we can summarize all of the model values by district and then visualize these variables by district rather than by individual data point. We use the following code chunk to compute for the average of each of the variables by district name using the following code chunk. The code uses group_by() to aggregate by district. Columns with the mean for each variable are then added by using across() and everything() to compute the mean for each variable.\n\ngwr_sf_fixed_by_dist &lt;- gwr_sf_fixed %&gt;%\n  group_by(district) %&gt;%\n  summarise(across(everything(), ~ mean(.x, na.rm = TRUE)))\n\nWe then import to the district boundary map using left_join() on district.\n\ntz_dist_stat &lt;- tz_dist_stat %&gt;%\n  left_join(gwr_sf_fixed_by_dist, by= \"district\")"
  },
  {
    "objectID": "posts/tz-fin-2023/index.html#e.5-visualizing-model-coefficients-and-metrics",
    "href": "posts/tz-fin-2023/index.html#e.5-visualizing-model-coefficients-and-metrics",
    "title": "Geographically Weighted Modeling of Financial Inclusion in Tanzania",
    "section": "E.5 Visualizing model coefficients and metrics",
    "text": "E.5 Visualizing model coefficients and metrics\nWe can visualize the patterns in the model coefficients or of other metrics with the updated sf dataframe.\n\nE.5.1 Visualizing model coefficients\nFor model coefficients, we focus on the following that have a wide range of values: has_id, mobile, internet, no_income, education (all levels), head_hh.\n\nWith IDMobile AccessInternet AccessLack of income sourcesEducationHeads of household\n\n\nThe code chunk below produces a choropleth map for the average coefficient of has_id in each district.\n\ntm_shape(tz_dist)+\n  tm_polygons(\"grey\") +\ntm_shape(tz_dist_stat[!is.na(tz_dist_stat$has_id.1),]) +\n  tm_polygons(\"has_id.1\", title = \"ß has_id\")\n\nVariable(s) \"has_id.1\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\n\n\n\n\nThe chart shows that the coefficient of has_id is negative across most of Tanzania. There are only a few districts where it is positive. This result seems surprising as ID’s may be treated as a way of getting access to financial services. There is likely more to this and the absence of an ID for some of the respondents might be link to another condition which is not appearing here. (e.g., those on welfare that do not have access to banking might all have IDs)\n\n\nThe code chunk below produces a choropleth map for the average coefficient of mobile in each district. We highlight districts with negative (average) coefficients with a red border.\n\ntm_shape(tz_dist)+\n  tm_polygons(\"grey\") +\ntm_shape(tz_dist_stat[!is.na(tz_dist_stat$mobile.1),]) +\n  tm_polygons(\"mobile.1\", title = \"ß mobile\") +\ntm_shape(tz_dist_stat[(!is.na(tz_dist_stat$mobile.1) & (tz_dist_stat$mobile.1) &lt; 0),]) +\n  tm_borders(\"red\")\n\nVariable(s) \"mobile.1\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\n\n\n\n\nThe output shows increased incidence of financial inclusion for respondents with access to a mobile phone. However, there is a cluster of districts on the northeast and another on the west that shows a negative correlation between mobile phone access and financial inclusion. This needs to be investigated as access is linked to access to mobile banking. These clusters might have low adoption, or limited access to such services which might be causing the results to appear as such.\n\n\nThe code chunk below produces a choropleth map for the average coefficient of internet in each district. We highlight districts with negative (average) coefficients with a red border.\n\ntm_shape(tz_dist)+\n  tm_polygons(\"grey\") +\ntm_shape(tz_dist_stat[!is.na(tz_dist_stat$internet.1),]) +\n  tm_polygons(\"internet.1\", title = \"ß internet\") +\ntm_shape(tz_dist_stat[(!is.na(tz_dist_stat$internet.1) & (tz_dist_stat$internet.1) &lt; 0),]) +\n  tm_borders(\"red\")\n\nVariable(s) \"internet.1\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\n\n\n\n\nWe see a similar state as mobile access. In general, probability of being financially included with internet access, but for some regions, this is not the case. There is a cluster in the north and in the west-southwest that are showing a negative correlation between financial inclusion and internet access. These also appear to be different regions districts compared to the ones for mobiel access. This needs to be investigated as to why internet access is not bringing higher levels of financial inclusion to these districts.\n\n\nThe code chunk below produces a choropleth map for the average coefficient of no_income in each district.\n\ntm_shape(tz_dist)+\n  tm_polygons(\"grey\") +\ntm_shape(tz_dist_stat[!is.na(tz_dist_stat$no_income.1),]) +\n  tm_polygons(\"no_income.1\", title = \"ß no_income\") +\ntm_shape(tz_dist_stat[(!is.na(tz_dist_stat$no_income.1) & (tz_dist_stat$no_income.1) &gt; 0),]) +\n  tm_borders(\"darkgreen\")\n\nVariable(s) \"no_income.1\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\n\n\n\n\nWith the exception of a cluster of five districts in the centre of Tanzania, the lack of a source of income, unsurprisingly, relates to higher probability of not being financially included. What we want to focus on are districts where the impact is higher. There is a cluster in the southwest, another in the north that appear to be more impacted. These might indicate more vulnerable populations– either a high degree of unemployment, or a much lower level (quality) of welfare or government support compared to the rest of the country.\n\n\nThe code chunk below produces a row of three choropleth maps for the average coefficient of each of the three education variables in each district. Each variable’s map is created using tmap and then stored in an object which are then displayed in a row using tmap_arrange().\n\nprim &lt;- tm_shape(tz_dist)+\n  tm_polygons(\"grey\") +\ntm_shape(tz_dist_stat[!is.na(tz_dist_stat$educ_primary.1),]) +\n  tm_polygons(\"educ_primary.1\", title = \"ß educ_primary\", palette = \"Greens\")\n\nsec &lt;- tm_shape(tz_dist)+\n  tm_polygons(\"grey\") +\ntm_shape(tz_dist_stat[!is.na(tz_dist_stat$educ_secondary.1),]) +\n  tm_polygons(\"educ_secondary.1\", title = \"ß educ_secondary\", palette = \"Greens\")\n\nter &lt;- tm_shape(tz_dist)+\n  tm_polygons(\"grey\") +\ntm_shape(tz_dist_stat[!is.na(tz_dist_stat$educ_tertiary.1),]) +\n  tm_polygons(\"educ_tertiary.1\", title = \"ß educ_tertiary\", palette = \"Greens\")\n\ntmap_arrange(prim, sec, ter, ncol = 3)\n\n\n\n\n\n\n\n\nThe output shows that education increases the chances of being financially included. The degree of impact also increases, generally, by the level of education.\n\n\nThe code chunk below produces a choropleth map for the average coefficient of head_hh in each district. We highlight districts with negative (average) coefficients with a red border.\n\ntm_shape(tz_dist)+\n  tm_polygons(\"grey\") +\ntm_shape(tz_dist_stat[!is.na(tz_dist_stat$head_hh.1),]) +\n  tm_polygons(\"head_hh.1\", title = \"ß head_hh\") +\ntm_shape(tz_dist_stat[(!is.na(tz_dist_stat$head_hh.1) & (tz_dist_stat$head_hh.1) &lt; 0),]) +\n  tm_borders(\"red\")\n\nVariable(s) \"head_hh.1\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\n\n\n\n\nFor most of the districts, it appears that the head of the household is more likely to be financially included (compared to non-heads of household) However, there are districts which are showing a negative coefficient. While there may be valid reasons for this (e.g., the head of household just stays home and might not be active in managing the funds) the aim is to make sure that no resident is excluded. Further investigation is needed if this can and needs to be addressed.\n\n\n\n\n\nE.5.2 Visualizing model residuals\nWe can also visualize the model residuals using the same approach. We can use the following code chunk to display the avrage residuals beside the level of financial inclusion in the district (which is simply the proportion of the indivudals that are financially included) to see if there is any pattern in the residuals and if they are related to the level of FI in a district.\n\ny &lt;- tm_shape(tz_dist)+\n  tm_polygons(\"grey\") +\ntm_shape(tz_dist_stat[!is.na(tz_dist_stat$y),]) +\n  tm_polygons(\"y\", title = \"Formal FI\", palette=\"-viridis\") +\ntm_shape(tz_dist_stat[!is.na(tz_dist_stat$y) & tz_dist_stat$y &lt; 0.4,]) +\n  tm_text(\"district\")\n\nres &lt;- tm_shape(tz_dist)+\n  tm_polygons(\"grey\") +\ntm_shape(tz_dist_stat[!is.na(tz_dist_stat$residual),]) +\n  tm_polygons(\"residual\", title = \"Residual\")\n\ntmap_arrange(y, res, ncol = 2)\n\nVariable(s) \"residual\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\n\n\n\n\nThe output shows that most of the districts that have the most negative residuals also are the ones that have low financial inclusion. (50% and below) This may indicate that the calibrated model, which was based on the global calibration, is not as appropriate for them. There may be other factors that explain the level of financial inclusion in these districts."
  },
  {
    "objectID": "posts/creating-diagonal-reference-lines-2024/index.html",
    "href": "posts/creating-diagonal-reference-lines-2024/index.html",
    "title": "Diagonal Reference Lines in Power BI",
    "section": "",
    "text": "In this post, I present a way to create (certain) dynamic diagonal reference lines in PowerBI. The tool has built in ways to create horizonal and vertical reference lines, but there is no built-in way to create diagonal ones.\nBefore I show one way of creating them in PowerBI, let me explain what i mean by diagonal reference lines."
  },
  {
    "objectID": "posts/creating-diagonal-reference-lines-2024/index.html#adding-x--and-y--constant-lines",
    "href": "posts/creating-diagonal-reference-lines-2024/index.html#adding-x--and-y--constant-lines",
    "title": "Diagonal Reference Lines in Power BI",
    "section": "1. Adding x- and y- constant lines",
    "text": "1. Adding x- and y- constant lines\nPower BI can add x- and y- reference lines to compatible visuals through the Analytics pane. If we are adding a line that is not based on the other predefined ones like mean, median, max, then we can make use of the constant lines. As we want to add the average of ratio measures, we should use the last one rather than the average line so it gets computed properly.\nFor constant lines, the user can key in a fixed number for the line’s value (e.g., a fixed target regardless of selection), or use the fx, or conditional formatting button to set its value to a measure or parameter.\n\n\n\n\n\nFor the x-axis constant line, we will set it to the CostPerEmp measure, and for the y-axis constant line, we will set it to RevPerEmp measure. We set the Data label option to On so we can also see the average value in the resulting chart.\n\n\n\nChart with x- and y- constant lines\n\n\nThe average can now be seen as $87K for revenue per employee, and $79K for cost per employee. The average is being pulled down by the large number and large size of the companies in the lower left so the values are far from the middle of the overall range."
  },
  {
    "objectID": "posts/creating-diagonal-reference-lines-2024/index.html#building-the-average-margin-series",
    "href": "posts/creating-diagonal-reference-lines-2024/index.html#building-the-average-margin-series",
    "title": "Diagonal Reference Lines in Power BI",
    "section": "2. Building the average margin series",
    "text": "2. Building the average margin series\nSince PowerBI only creates reference lines for each axis, we need a workaround in order to add diagonal lines. In this post, we will do this by creating a series of points that plot the value that we want– which means we create a set of points on the same axes which all have the same margin.\nThe first step is computing the average margin. Margin is one less the quotient of cost and revenue, so defining a simple measure Margin is straightforward. To calculate the margin of all the companies Margin-All, we need to use CALCULATE() in order to override the filter context. We use ALLSELECTED() rather than ALL() in order to take any external filters like slicers into account.\nMargin = 1 - SUM(Data[Cost]) / SUM(Data[Revenue])\n\nMargin-All = CALCULATE([Margin], ALLSELECTED(Data[Name])) \nMargin-All holds a single value based on filters applied on the visual. For a given value of Margin-All, there will be a corresponding value of CostPerEmp or RevPerEmpl, based on the value of the other. For this demo, we will create a measure to derive the CostPerEmp value to get Margin-All for values of RevPerEmp.\nCostPerEmp-ConstMargin = (1-[Margin-All]) * [RevPerEmp]\nThe new measure CostPerEmp-ConstMargin and the original measure RevPerEmp would correspond to pairs of points that give a margin equal to the average of all companies. (based on filters) We will be overlaying these new points as a trend line to generate the reference line."
  },
  {
    "objectID": "posts/creating-diagonal-reference-lines-2024/index.html#fixing-the-axes-ranges",
    "href": "posts/creating-diagonal-reference-lines-2024/index.html#fixing-the-axes-ranges",
    "title": "Diagonal Reference Lines in Power BI",
    "section": "3. Fixing the axes ranges",
    "text": "3. Fixing the axes ranges\nBefore we generate the margin line, we need to make sure that doing so will not mess up with the axes. If we keep the axes ranges as ‘automatic’, the CostPerEmp and the CostPerEmp-ConstMargin ranges are likely to be different. The latter is likely going to be outside the range of the former so the chart for the average margin line will be ‘zoomed out’.\nTo overcome this issue, we need to override the automatic axis ranges. The axes ranges can be modified in the visual’s format pane under the Visual tab.\n\n\n\n\n\nWe can specify a fixed number as the minimum and maximum values, but it might be better to use a dynamic range since a hard-coded number might be inappropriate once the user selects or deselects some of the companies.\nOne way is to set the minimum and maximum values as a factor of the minimum or maximum values of the (original) axes. We will use an arbitrary 20% and define measures to take 80% (i.e., 1-20%) of the lowest value as the axis minimum and then take 120% of the highest value as the axis maximum.\nFor the calculation, we need to use the iterating aggregation functions MAXX() and MINX() since we still need to compute for the Revenue and Cost per Employee per row, since we do not have this as an existing column in the table.\nCPEmax = CALCULATE(MAXX(Data, Data[Cost] / Data[Employees]), ALLSELECTED(Data[Name])) * 1.2\nCPEmin = CALCULATE(MINX(Data, Data[Cost] / Data[Employees]), ALLSELECTED(Data[Name])) * 0.8\nRPEmax = CALCULATE(MAXX(Data, Data[Revenue] / Data[Employees]), ALLSELECTED(Data[Name])) * 1.2\nRPEmin = CALCULATE(MINX(Data, Data[Revenue] / Data[Employees]), ALLSELECTED(Data[Name])) * 0.8\nWe will use these measures as the minimum and maximum values of the two axes by using them in the conditional formatting portion or pressing the fx button in the axis range’s option."
  },
  {
    "objectID": "posts/creating-diagonal-reference-lines-2024/index.html#plotting-the-average-margin-line",
    "href": "posts/creating-diagonal-reference-lines-2024/index.html#plotting-the-average-margin-line",
    "title": "Diagonal Reference Lines in Power BI",
    "section": "4. Plotting the average margin line",
    "text": "4. Plotting the average margin line\nTo “add” the average GM line into the chart, we perform the following steps:\n\nDuplicate the scatterplot. (i.e., copy and paste the entire visual) Let us call the one we will use for the GM line the “GM chart” while the other one as the “original chart”\nReplace the x-axis field of the GM chart from CostPerEmp to CostPerEmp-ConstMargin. This should result to a chart with all points along a line\nUnder the Analytics pane of the GM chart, turn on the Trend line. Adjust the formatting like color and line style as needed.\nRemove the x- and y-axis titles of the by replacing their title with a space. This can be accessed under the format pane of the visual under Visual &gt;&gt; X-axis (or Y-Axis) &gt;&gt; Title. Do not turn off the title as it will change the plot area dimensions.\nSet the transparency of the background of the original chart to 100%. (via the visual’s Format tab &gt;&gt; Effects &gt;&gt; Background)\nMove the GM chart to the back (via Format ribbon &gt;&gt; Send backward &gt;&gt; Send to back) and then move it behind the original chart.\n\nAfter going through these steps, we should end up with a chart with not only the first two reference lines, but also a diagonal line for the average GM.\n\n\n\n\n\nThis chart shows that companies like Apple that lie above the diagonal line have higher margin than average, while one like Cardinal Health has below average. This chart would also adjust based on slicer selections– the reference lines and the axes will adjust based on companies selected."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "data with derek",
    "section": "",
    "text": "My blog with my personal projects and discoveries in data analytics.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nIdentifying Crime Hotspots in LA\n\n\n\n\n\n\nR\n\n\nGeospatial Analysis\n\n\nUSA\n\n\nLos Angeles\n\n\n\n\n\n\n\n\n\nDec 15, 2024\n\n\nDerek Rodriguez\n\n\n\n\n\n\n\n\n\n\n\n\nStarting Linear Programming in Python\n\n\n\n\n\n\nPython\n\n\nOptimization\n\n\nLinear Programming\n\n\nLearn With Me\n\n\n\n\n\n\n\n\n\nNov 28, 2024\n\n\nDerek Rodriguez\n\n\n\n\n\n\n\n\n\n\n\n\nDiagonal Reference Lines in Power BI\n\n\n\n\n\n\nPowerBI\n\n\nVisualization\n\n\nBusiness Analytics\n\n\n\n\n\n\n\n\n\nNov 22, 2024\n\n\nDerek Rodriguez\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysing Card Choices with Frequent Pattern Mining\n\n\n\n\n\n\nPython\n\n\nPattern Mining\n\n\nMarket Basket Analysis\n\n\nWeb Scraping\n\n\nMTG\n\n\n\n\n\n\n\n\n\nNov 19, 2024\n\n\nDerek Rodriguez\n\n\n\n\n\n\n\n\n\n\n\n\nDiscovering Impact of COVID-19 on Thai Tourism Economy\n\n\n\n\n\n\nR\n\n\nGeospatial Analysis\n\n\nThailand\n\n\n\n\n\n\n\n\n\nNov 13, 2024\n\n\nDerek Rodriguez\n\n\n\n\n\n\n\n\n\n\n\n\nGeographically Weighted Modeling of Financial Inclusion in Tanzania\n\n\n\n\n\n\nR\n\n\nGeospatial Analysis\n\n\nTanzania\n\n\n\n\n\n\n\n\n\nNov 13, 2024\n\n\nDerek Rodriguez\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi visitor. I’m Derek, data enthusiast and dog dad to a naughty corgi. I started this blog in November 2024 as I reach the end of my data analytics masters journey. I will use this to share my personal projects, including any interesting tricks or datasets that I encounter."
  },
  {
    "objectID": "posts/atraxa-edh-with-mba/index.html",
    "href": "posts/atraxa-edh-with-mba/index.html",
    "title": "Analysing Card Choices with Frequent Pattern Mining",
    "section": "",
    "text": "In this post, I use frequent pattern mining or association rule mining, which is typically used for market basket analysis, in order to analyze card choices in more than a thousand user-uploaded decks. I will be using Python’s requests, beautifulsoup4, mtg_parser, and mlxtend packages to perform the main activities.\nWe’ve learned association rule mining in multiple courses in MITB (Customer Analytics, Data Science for Business) primarily for Market Basket Analysis. The same technique should be applicable outside retail, where the questions can be addressed by finding highly correlated items or sets of items."
  },
  {
    "objectID": "posts/atraxa-edh-with-mba/index.html#using-mtg-parser-as-a-structured-way-of-pulling-mtg-decklists",
    "href": "posts/atraxa-edh-with-mba/index.html#using-mtg-parser-as-a-structured-way-of-pulling-mtg-decklists",
    "title": "Analysing Card Choices with Frequent Pattern Mining",
    "section": "Using MTG Parser as a structured way of pulling MTG decklists",
    "text": "Using MTG Parser as a structured way of pulling MTG decklists\nI found a package called mtg_parser for easily scraping MTG decklists off popular webpages. Their function parse_deck() retuns an iterable containing items of a custom class card which includes the quantity and the name of each card in a decklist.\nThe code chunk below loads the package, and then iterates through the list of deck suffixes generated earlier and passes them into parse_deck(). We include error handling using try-except as we are not sure whether each link is still live or contains a readable decklist. There are also multiple ways the link can be built. (either with or without #paper) The result is initially stored as a nested list containing the deck numbers and the card names.\n\nimport mtg_parser\n\ndecklist = []\nfail_count = 0\nfor suffix in hrefs:\n    deck = suffix[-7:]\n    try:\n        try:\n            url = 'https://www.mtggoldfish.com/deck/' + str(deck) + '#paper'\n            cards = mtg_parser.parse_deck(url)\n            for card in cards:\n              decklist.append([deck, card.name])\n        except:\n            url = 'https://www.mtggoldfish.com/deck/' + str(deck)\n            cards = mtg_parser.parse_deck(url)\n            for card in cards:\n              decklist.append([deck, card.name])\n    except:\n        print('Failed for deck', deck)\n        fail_count += 1\n\nWe included a counter to already check how many decks (deck links) didn’t work with this method, and it appears that out of the 2136 decks, 5 failed so we still have a good amount of 2131 decks to work with.\nAlso note that the last code chunk takes very long to execute as the request is done for each of the 2136 decklist pages. I have saved the results in a file so I don’t need to run the code again once I restart the Python session.\nWe then convert the resulting list object into a dataframe which will be easier to work with. This is done simply using the pandas package and its DataFrame() function which converts some collections, like a list or dictionary, into a dataframe. We pass the appropriate variable names in the columns agreement that indicate the first element as the deck (denoted by the link suffix) and the card name.\n\nimport pandas as pd\ncolumns = [\"deck\", \"card\"]\ndecklist = pd.DataFrame(decklist, columns = columns)\n\n\ndecklist.head()\n\n\n\n\n\n\n\n\ndeck\ncard\n\n\n\n\n0\n6755629\nAtraxa, Praetors' Voice\n\n\n1\n6755629\nBirds of Paradise\n\n\n2\n6755629\nDeathrite Shaman\n\n\n3\n6755629\nDelighted Halfling\n\n\n4\n6755629\nEsper Sentinel"
  },
  {
    "objectID": "posts/atraxa-edh-with-mba/index.html#identifying-staples",
    "href": "posts/atraxa-edh-with-mba/index.html#identifying-staples",
    "title": "Analysing Card Choices with Frequent Pattern Mining",
    "section": "Identifying Staples",
    "text": "Identifying Staples\nWhile it is tempting to use the algorithm for association rules mining right away, it will take too long to run it on the whole dataset that we have. (I tried, but I gave up after more than 12 hours) We can have an idea of how much effort will be required to run the algorithm by checking the size of the dataset. We use nunique() to count the unique values for each of the two columns in decklist.\n\ndecklist[\"deck\"].nunique()\ndecklist[\"card\"].nunique()\n\n6804\n\n\nUntouched, this would mean converting this first into a 2131 x 6804 dataframe. The number of itemsets can also be very high as the cap will be \\(2^n\\) where \\(n\\) is the number of unique cards. We can use standard pandas functions to identify very frequent cards and very infrequent cards– which are not necessary for our other questions about the Atraxa decks.\n\nWhich cards appear in most of the submitted decks?\nWe can count the number of times each card appears in a deck by using value_counts(). We add a new column to indicate the percentage of the 2136 decks that contain that card.\n\ncard_counts = decklist['card'].value_counts().reset_index()\ncard_counts.columns = ['card', 'count']\ncard_counts = card_counts.sort_values(by='count', ascending=False)\ncard_counts['pct'] = round(card_counts['count'] / 2136 * 100, 1)\ncard_counts.head(10)\n\n\n\n\n\n\n\n\ncard\ncount\npct\n\n\n\n\n0\nAtraxa, Praetors' Voice\n2134\n99.9\n\n\n1\nForest\n1963\n91.9\n\n\n2\nIsland\n1962\n91.9\n\n\n3\nPlains\n1943\n91.0\n\n\n4\nSwamp\n1937\n90.7\n\n\n5\nCommand Tower\n1888\n88.4\n\n\n6\nSol Ring\n1786\n83.6\n\n\n7\nArcane Signet\n1583\n74.1\n\n\n8\nExotic Orchard\n1307\n61.2\n\n\n9\nEvolution Sage\n1301\n60.9\n\n\n\n\n\n\n\nThe output shows that there are (only) five cards that appear in at least 90% of decks– Atraxa, and the four basic lands. (which are typical ‘energy’ sources for the game) There are only three that appear in 70-80% of decks and these are cards that appear in almost every deck in the format. The #9 card, Evolution Sage is very specific to decks of this strategy, but it only appears in 61% of decks.\nLet’s check out the next 10 elements of the list with the following code.\n\nprint(card_counts.iloc[10:20])\n\n                    card  count   pct\n10        Karn's Bastion   1218  57.0\n11  Swords to Plowshares   1147  53.7\n12     Astral Cornucopia   1130  52.9\n13     Tezzeret's Gambit   1122  52.5\n14         Thrummingbird   1104  51.7\n15     Chromatic Lantern   1096  51.3\n16             Cultivate   1057  49.5\n17       Inexorable Tide   1010  47.3\n18               Farseek    999  46.8\n19         Temple Garden    932  43.6\n\n\nThe next ten cards include cards specific to this deck’s strategy (e.g., Karn’s Bastion) but also contains generic cards (e..g, Swords to Plowshares) or land cards. (e.g., Temple Garden) The frequency is getting quite low as the last four cards appear in less than half of the submitted decks.\n\n\nHow many cards appear in only a handful of decks?\nWe can use the same approach as earlier to count the card counts, and then display the twenty lowest counts (most likely 1-20 decks) using head().\n\ncount_counts = card_counts['count'].value_counts().reset_index()\ncount_counts.columns = ['card_count', 'count']\ncount_counts = count_counts.sort_values(by='card_count', ascending=True)\ncount_counts.head(20)\nprint('\\n', sum(count_counts.head(20)[\"count\"]))\n\n\n 5696\n\n\nThere are 2320 (of the 6804) cards that only appear in one decklist, and 5696 in total that appear in 20 or less decklists. This means that only 1,108 cards appear in more than 21 decklists.\n\n\nCleaning up the Decklists\nWe end this part by trimming down decklist by removing the very frequent and the very infrequent cards. First, we bring the counts into decklist by joining it with card_counts using merge().\n\nimport pandas as pd\ndecklist = pd.merge(decklist, card_counts, on = 'card', how = 'left')\n\nWe want to exclude the top 9 cards, which are those that appear in 1309 or more decks, and we also want to exclude the bottom 5696 carrds, or the ones that appear in 20 decks or less.\n\ndecklist['card'].nunique()\ndecklist_trimmed = decklist[decklist['count'] &gt; 20]\ndecklist_trimmed = decklist_trimmed[decklist_trimmed['count'] &lt; 1309]\ndecklist_trimmed['card'].nunique()\n\n1100\n\n\nWe also know that there are more lands that are very common in the Atraxa decks. We create a list of the most common of these (other_common_lands) and then\n\n# Filter out rows where itemsets contain any of the common lands\ndecklist_trimmed = decklist_trimmed[~decklist_trimmed['card'].apply(lambda x: x in other_common_lands)]\n\n\ndecklist_trimmed['card'].nunique()\n\n1077\n\n\nThis step reduced the number of unique cards from 6804 to 1100 then to 1077, which could be more workable for the algorithms we are going to apply. We’ll remove the unnecessary columns first since we only need the deck id and the card names that we originally started with.\n\ndecklist_trimmed = decklist_trimmed.drop(columns =['count', 'pct'])"
  },
  {
    "objectID": "posts/atraxa-edh-with-mba/index.html#transforming-the-decklists-into-the-right-format",
    "href": "posts/atraxa-edh-with-mba/index.html#transforming-the-decklists-into-the-right-format",
    "title": "Analysing Card Choices with Frequent Pattern Mining",
    "section": "Transforming the decklists into the right format",
    "text": "Transforming the decklists into the right format\nThe apriori() function requires a dataframe where each row is a transaction (or basket, customer, or, in our case, a deck) while each column corresponds to an item. (i.e., a card) The value will be a binary (True/False or 1/0) which indicates whether the card is in that specific deck or not.\nWe use the code chunk below to perform this transformation, but there should be multiple ways to achieve this. The resulting object, as expected, would be a 2131 x 6804 dataframe. The values are all True/False which are easier for apriori() to work with.\n\nimport pandas as pd\ndecklists_encoded = decklist_trimmed.drop_duplicates()\ndecklists_encoded= decklists_encoded.pivot(index='deck', columns='card', values='card')\n\n# Fill NaN values with False (optional)\ndecklists_encoded = decklists_encoded.notna()\n\n# Reset the index if needed\ndecklists_encoded.reset_index(inplace=True)\ndecklists_encoded = decklists_encoded.drop('deck', axis=1)"
  },
  {
    "objectID": "posts/atraxa-edh-with-mba/index.html#running-the-algorithm",
    "href": "posts/atraxa-edh-with-mba/index.html#running-the-algorithm",
    "title": "Analysing Card Choices with Frequent Pattern Mining",
    "section": "Running the Algorithm",
    "text": "Running the Algorithm\nThe apriori() and fpgrowth() function is used to identify frequent item sets and returns an object which contains the itemsets and their support. The functions require a dataframe (described earlier) as a mandatory input. These differ by the way they identify frequent itemsets. For larger datasets, fpgrowth() will typically be more efficient in finding the itemsets.\nThe user can specify a minimum support threshold (min_support) for the function, otherwise it defaults to 0.5. This default value is a bit too high especially as we have not done any exploratory analysis to understand what is frequent or infrequent. We will use a value of 0.05 or 5% for our case. We also specify True for the use_colnames argument to indicate that the column names and not the indices will be used for the results. We also add a maximum itemset size of 5 using the max_len argument in order to limit the number of subsets scanned by the algorithm.\n\nfrom mlxtend.frequent_patterns import apriori, fpgrowth\nfrequent_itemsets = fpgrowth(decklists_encoded, min_support=0.05, use_colnames=True, max_len = 5)\n\n\n\n\n\n\n\nWarning\n\n\n\nThis code chunk will still run a good amount of time even with the reductions that we made.\nConsider increasing the minimum support, trimming down the data, or using an even more efficient algorithm before performing this yourself for your own purpose.\nFor MBA, using the optional argument max_len is also desirable since it specifies the maximum size of the sets generated. A set size of 2 or 3 will lead to simple and practical use for retail purposes."
  },
  {
    "objectID": "posts/atraxa-edh-with-mba/index.html#identifying-frequent-sets",
    "href": "posts/atraxa-edh-with-mba/index.html#identifying-frequent-sets",
    "title": "Analysing Card Choices with Frequent Pattern Mining",
    "section": "Identifying Frequent Sets",
    "text": "Identifying Frequent Sets\nWhile apriori() and fpgrowth() does not produce association rules yet, they already generate frequent itemsets based on the minimum support that we indicated. We can use the results to identify staples or very common or typical cards that users have included in their Atraxa decklists. For our analysis, we will consider cards that appear in 85% of decks as staples.\nWith the frequent itemset output, we should be able to identify any high frequency sets of cards.\n\nCounting the itemset sizes\nThe first step we want to do before answering the next questions is indicate the number of items. This can be done quickly by just applying the len() function to each element of the itemsets column.\n\nfrequent_itemsets['size'] = frequent_itemsets['itemsets'].apply(len)\n\nWe should be able to see a preview with the new column using head()\n\nfrequent_itemsets.head()\n\n\n\n\n\n\n\n\nsupport\nitemsets\nsize\n\n\n\n\n0\n0.612207\n(Exotic Orchard)\n1\n\n\n1\n0.537089\n(Swords to Plowshares)\n1\n\n\n2\n0.528638\n(Astral Cornucopia)\n1\n\n\n3\n0.525352\n(Tezzeret's Gambit)\n1\n\n\n4\n0.516432\n(Thrummingbird)\n1\n\n\n\n\n\n\n\n\n\nWhat are the most common set of cards included in Atraxa decks?\nAside from individual cards like the staples mentioned earlier, we expect that there are cards that will recur as a group across different users’ decks. Some of these might just be staples, but some might be tied to specific strategies or ‘builds’ for the Atraxa deck.\nWe can use the code below to find the most frequent set of five cards in the user submitted decks.\n\nfrequent_itemsets[frequent_itemsets['size'] == 5].sort_values(by='support', ascending = False).head(1)\nprint(\"\\n\")\nfrequent_itemsets[frequent_itemsets['size'] == 5].sort_values(by='support', ascending = False).head(1)[\"itemsets\"].iloc[0]\n\n\n\n\n\nfrozenset({'Evolution Sage',\n           'Exotic Orchard',\n           \"Karn's Bastion\",\n           'Swords to Plowshares',\n           \"Tezzeret's Gambit\"})\n\n\nEvolution Sage, Tezzeret’s Gambit, Karn’s Bastion, Exotic Orchard, and Swords to Plowshares are a set of five cards (excluding the staples and lands that we deleted previously) that appear in 17% of Atraxa decks.\n\n\nWhat is the next most frequent disjoint set of five cards?\nIf we just look at the top 5-card sets, we will see the same cards repeating over and over again. What if we wanted to find a unique set of frequently included 5-cards?\n\nfrequent_itemsets[frequent_itemsets['size'] == 5].sort_values(by='support', ascending = False).head(10)\n\n\n\n\n\n\n\n\nsupport\nitemsets\nsize\n\n\n\n\n413\n0.169484\n(Karn's Bastion, Evolution Sage, Exotic Orchar...\n5\n\n\n382\n0.167606\n(Karn's Bastion, Evolution Sage, Exotic Orchar...\n5\n\n\n408\n0.167606\n(Evolution Sage, Exotic Orchard, Astral Cornuc...\n5\n\n\n399\n0.167606\n(Karn's Bastion, Evolution Sage, Exotic Orchar...\n5\n\n\n18372\n0.166197\n(Karn's Bastion, Evolution Sage, Exotic Orchar...\n5\n\n\n570\n0.164319\n(Karn's Bastion, Evolution Sage, Cultivate, Ex...\n5\n\n\n1204\n0.163850\n(Karn's Bastion, Evolution Sage, Inexorable Ti...\n5\n\n\n492\n0.163850\n(Evolution Sage, Cultivate, Exotic Orchard, Te...\n5\n\n\n846\n0.163380\n(Karn's Bastion, Evolution Sage, Exotic Orchar...\n5\n\n\n410\n0.161502\n(Karn's Bastion, Evolution Sage, Astral Cornuc...\n5\n\n\n\n\n\n\n\nWe can compare each of the itemsets with the previous five card list until we find one which does not share any elements with it. We can use the following code which applies a simple function to the itemsets column and use that as a filter.\n\ntop_5cardset = frequent_itemsets[frequent_itemsets['size'] == 5].sort_values(by='support', ascending = False).head(1)[\"itemsets\"].iloc[0]\nfrequent_5cards = frequent_itemsets[frequent_itemsets['size'] == 5].sort_values(by='support', ascending = False)\n\ndef contains_card(itemset, cards):\n  return any(item in cards for item in itemset)\n\nfrequent_5cards_other = frequent_5cards[~frequent_5cards['itemsets'].apply(lambda x: contains_card(x, top_5cardset))]\n\nWe can then call the first element of the new object to find a disjoint set of five cards.\n\nfrequent_5cards_other.head(1)\nprint(\"\\n\")\nfrequent_5cards_other.head(1)[\"itemsets\"].iloc[0]\n\n\n\n\n\nfrozenset({'Ezuri, Stalker of Spheres',\n           'Infectious Inquiry',\n           'Prologue to Phyresis',\n           'Tekuthal, Inquiry Dominus',\n           \"Vraska's Fall\"})\n\n\nThe output shows that this set of cards includes: Infectious Inquiry, Prologue to Phyresis, Vraska’s Fall, Tekuthal, Inquiry Dominus, Ezuri, Stalker of Spheres. The support for this set is 0.138967– meaning it appears in 13.9% of Atraxa decks. Note that this does not imply that this set of cards do not occur with the first five identified. We simply wanted to find a unique set of five which might or might not be used with the first five cards."
  },
  {
    "objectID": "posts/atraxa-edh-with-mba/index.html#generating-association-rules",
    "href": "posts/atraxa-edh-with-mba/index.html#generating-association-rules",
    "title": "Analysing Card Choices with Frequent Pattern Mining",
    "section": "Generating Association Rules",
    "text": "Generating Association Rules\nWhile we can discover a lot with the frequent itemsets, this is limited to questions about the set frequencies in isolation. We can use association_rules() to generate a list of rules which describe the correlation or likelihood of items being present with other items.\nThe function requires a frequent itemset dataframe as an input. The user can define the metric and the minimum value to use using the metric and min_threshold arguments. The former can accept ‘support’, ‘confidence’ or ‘lift’ as metrics.\nWe use the code chunk below to generate the association rules with a minimum lift of 1. The first line retains only the sets with a length of 1 to 3. This means that we will have at most two elements in the antecedent (left side) or consequent (right side) of each rule. We do this as we will focus on single card associations, and as this will reduce the number of rules significantly. The next line of the code chunk removes the size column that we added to the dataframe to bring it back to the right format.\n\nfrom mlxtend.frequent_patterns import association_rules\n\nfrequent_itemsets = frequent_itemsets[frequent_itemsets['size'] &lt; 4]\nfrequent_itemsets = frequent_itemsets.drop(columns =['size'])\n\nrules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1)\n\n\nrules.shape\nprint(\"\\n\")\nrules.head()\nprint(\"\\n\")\nrules.columns\n\n\n\n\n\n\n\nIndex(['antecedents', 'consequents', 'antecedent support',\n       'consequent support', 'support', 'confidence', 'lift', 'leverage',\n       'conviction', 'zhangs_metric'],\n      dtype='object')\n\n\nThe resulting dataframe has 288,366 rows or rules with 10 columns each. The antecedent and the consequent are each in separate columns. There are three columns for support, which are for the antecedent, the consequent, or for the two combined. Among the other metrics, we also have columns for the confidence and the lift.\nThe code chunk below will display the five rules with the highest lift.\n\ntop_5_rules = rules.sort_values(by='lift', ascending = False).head(5)[['antecedents', 'consequents', 'confidence', 'consequent support', 'lift']]\n\nprint(top_5_rules)\n\n                               antecedents  \\\n190764  (Underground Sea, Tropical Island)   \n190769                            (Tundra)   \n190752         (Savannah, Underground Sea)   \n190757                            (Tundra)   \n190741                     (Tundra, Bayou)   \n\n                               consequents  confidence  consequent support  \\\n190764                            (Tundra)    0.972028            0.071362   \n190769  (Underground Sea, Tropical Island)    0.914474            0.067136   \n190752                            (Tundra)    0.965753            0.071362   \n190757         (Savannah, Underground Sea)    0.927632            0.068545   \n190741                   (Underground Sea)    0.979167            0.073239   \n\n             lift  \n190764  13.621181  \n190769  13.621181  \n190752  13.533255  \n190757  13.533255  \n190741  13.369391  \n\n\nThe first rule can be read as: if a decklist contains Tropical Island and Underground Sea, it is 97% likely to have Tundra. The 13.621 lift value means that Tundra is 12.6x more likely to be seen with these two cards than in general."
  },
  {
    "objectID": "posts/atraxa-edh-with-mba/index.html#using-association-rules-to-find-how-cards-are-potentially-being-used",
    "href": "posts/atraxa-edh-with-mba/index.html#using-association-rules-to-find-how-cards-are-potentially-being-used",
    "title": "Analysing Card Choices with Frequent Pattern Mining",
    "section": "Using Association Rules to find how cards are (potentially) being used",
    "text": "Using Association Rules to find how cards are (potentially) being used\nAside from simply finding high lift rules, we can use the association rules by\nThe code below shows five cards that are used in less than 250 decks. We already have the counts in the card_counts dataframe so we just need to select with the appropriate mask.\n\ncard_counts[card_counts['count'] &lt; 250].head()\n\n\n\n\n\n\n\n\ncard\ncount\npct\n\n\n\n\n191\nSimic Signet\n249\n11.7\n\n\n190\nPrairie Stream\n249\n11.7\n\n\n192\nYavimaya Coast\n248\n11.6\n\n\n193\nChampion of Lambholt\n248\n11.6\n\n\n194\nBrokers Confluence\n247\n11.6\n\n\n\n\n\n\n\nLet’s say we focus on the card Broker’s Confluence which is a fairly recent card. How can we use the association rules to find where this card is being added by players?\n\n\n\n\n\nWe can scan the association rules, by checking the ones where the consequent is Brokers Confluence, and find the ones where the confidence or lift are sufficiently high. (we use a lift of 1.5 as our filter)\n\nrules[(rules['consequents'] == {'Brokers Confluence'}) & (rules['lift'] &gt; 1.5)][[\"antecedents\",\"confidence\",\"lift\"]].head(10)\n\n\n\n\n\n\n\n\nantecedents\nconfidence\nlift\n\n\n\n\n257432\n(Grateful Apparition)\n0.218147\n1.881184\n\n\n257437\n(Contentious Plan)\n0.226069\n1.949504\n\n\n257444\n(Evolution Sage, Thrummingbird)\n0.189223\n1.631762\n\n\n257468\n(Inexorable Tide, Thrummingbird)\n0.188006\n1.621270\n\n\n257480\n(Flux Channeler, Evolution Sage)\n0.176093\n1.518531\n\n\n257486\n(Flux Channeler, Thrummingbird)\n0.212727\n1.834450\n\n\n257492\n(Tezzeret's Gambit, Flux Channeler)\n0.177419\n1.529973\n\n\n257527\n(Tekuthal, Inquiry Dominus, Evolution Sage)\n0.178730\n1.541272\n\n\n257533\n(Evolution Sage, Ezuri, Stalker of Spheres)\n0.193811\n1.671326\n\n\n257545\n(Everflowing Chalice, Evolution Sage)\n0.175159\n1.510482\n\n\n\n\n\n\n\nIt looks like Brokers Ascendancy is being added to decks 50% more when there are cards like the ones above which are themed around the proliferate ability."
  },
  {
    "objectID": "posts/thai-tourism-covid/index.html",
    "href": "posts/thai-tourism-covid/index.html",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "",
    "text": "This is my second project (take home exercise) for our course Geospatial Analysis. This exercise makes use of techniques in geospatial analysis to discover clustering and hotspots in order to analyze Thai tourism data befor and after Covid."
  },
  {
    "objectID": "posts/thai-tourism-covid/index.html#a.1-background",
    "href": "posts/thai-tourism-covid/index.html#a.1-background",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "A.1 Background",
    "text": "A.1 Background\nTourism is a major industry in Thailand as it made up to 20% of their gross domestic product pre-pandemic. However, like the rest of the world, the industry has taken a hit with COVID-19 in 2020, and has slowly been recovering since 2021. Recent reports are stating that Thailand is already, but still, at 80% of its peak level in 2019.\nWhile we speak about the industry in general, the state of tourism within Thailand, and their recovery status are not the same. For example, tourism revenues have been focused on Bangkok, Phuket and Chonburi pre-pandemic.\nWe are interested in understanding the state of tourism across Thailand with regards to its spatial distribution and time and space distribution– both in absolutes and in terms of the trend with respect to the pandemic."
  },
  {
    "objectID": "posts/thai-tourism-covid/index.html#a.2-objectives",
    "href": "posts/thai-tourism-covid/index.html#a.2-objectives",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "A.2 Objectives",
    "text": "A.2 Objectives\nFor this study, we want to understand the state of tourism in Thailand at a provincial level, and answer the following questions:\n\nAre the key tourism indicators in Thailand (at a province level) independent from space and from space and time?\nIf tourism or any tourism indicators are not independent, what are the clusters, outliers and emerging hotspots and coldspots?\n\nWe will use the appropriate packages in R in order to perform the different analysis (spatial and otherwise) to support our answers to the above questions."
  },
  {
    "objectID": "posts/thai-tourism-covid/index.html#a.3-data-sources",
    "href": "posts/thai-tourism-covid/index.html#a.3-data-sources",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "A.3 Data Sources",
    "text": "A.3 Data Sources\nThe following data sources are used for this analysis:\n\nThailand Domestic Tourism Statistics from Kaggle covering the years 2019-2023 and are at province and month level across 8 indicators:\n\nno_tourist_all - total number of domestic tourists\nno_tourist_foreign - number of foreign tourists\nno_tourist_occupied - number of hotel rooms occupied\nno_tourist_thai - number of Thai tourists\noccupancy_rate - the percentage of occupied travel accommodations (hotel rooms)\nrevenue_all - total tourism revenue, in M-THB (appears as net profit in the raw data)\nrevenue_foreign - revenue generated by foreign tourists, in M-THB (appears as net profit in the raw data)\nrevenue_thai - revenue generated by Thai tourists, in M-THB (appears as net profit in the raw data)\n\nThailand-Subnational Administrative Boundaries from Human Data Exchange in shapefile format"
  },
  {
    "objectID": "posts/thai-tourism-covid/index.html#a.4-importing-and-launching-r-packages",
    "href": "posts/thai-tourism-covid/index.html#a.4-importing-and-launching-r-packages",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "A.4 Importing and Launching R Packages",
    "text": "A.4 Importing and Launching R Packages\nFor this study, the following R packages will be used. A description of the packages and the code, using p_load() of the pacman package, to import them is given below.\n\nPackage DescriptionImport Code\n\n\nThe loaded packages include:\n\nsf - package for importing, managing and processing vector-based geospatial data\ntidyverse - collection of packages for performing data importation, wrangling and visualization\ntmap - package with functions for plotting cartographic quality maps\nsfdep - for handling spatial data\ncoorplot, ggpubr, heatmaply, factoextra - packages for multivariate data visualization and analysis\ncluster, ClustGeo, NbClust - packages for performing cluster analysis\n\n\n\n\npacman::p_load(sf, tmap, spdep, sfdep, tidyverse,\n               ggpubr, heatmaply, factoextra,\n               NbClust, cluster, ClustGeo)\n\n\n\n\nAs we will be performing simulations in the analysis later, it is good practice to define a random seed to be used so that results are consistent for viewers of this report, and the results can be reproduced.\n\nset.seed(1234)"
  },
  {
    "objectID": "posts/thai-tourism-covid/index.html#b.1.-thailand-subnational-boundary-provincial-level",
    "href": "posts/thai-tourism-covid/index.html#b.1.-thailand-subnational-boundary-provincial-level",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "B.1. Thailand Subnational Boundary, Provincial Level",
    "text": "B.1. Thailand Subnational Boundary, Provincial Level\nWe load the Thailand subnational administrative boundary shapefile into an R dataframe using st_read() from the sf package. We need to analyze at the provincial level so we will be using the files suffixed by “1”.\n\nthai_sf &lt;- st_read(dsn=\"data/geospatial\", \n                   layer=\"tha_admbnda_adm1_rtsd_20220121\")\n\nReading layer `tha_admbnda_adm1_rtsd_20220121' from data source \n  `C:\\drkrodriguez\\datawithderek\\posts\\thai-tourism-covid\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 77 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97.34336 ymin: 5.613038 xmax: 105.637 ymax: 20.46507\nGeodetic CRS:  WGS 84\n\n\nThe output states that the object is of multipolygon geometry type containing 77 features (provinces, records) across 16 fields. (columns) We can check the contents of the object using a number of methods. For the code chunk below, we use glimpse() which lists the columns, gives the data type and the first elements.\n\nglimpse(thai_sf)\n\nRows: 77\nColumns: 17\n$ Shape_Leng &lt;dbl&gt; 2.417227, 1.695100, 1.251111, 1.884945, 3.041716, 1.739908,…\n$ Shape_Area &lt;dbl&gt; 0.13133873, 0.07926199, 0.05323766, 0.12698345, 0.21393797,…\n$ ADM1_EN    &lt;chr&gt; \"Bangkok\", \"Samut Prakan\", \"Nonthaburi\", \"Pathum Thani\", \"P…\n$ ADM1_TH    &lt;chr&gt; \"กรุงเทพมหานคร\", \"สมุทรปราการ\", \"นนทบุรี\", \"ปทุมธานี\", \"พระนครศรีอ…\n$ ADM1_PCODE &lt;chr&gt; \"TH10\", \"TH11\", \"TH12\", \"TH13\", \"TH14\", \"TH15\", \"TH16\", \"TH…\n$ ADM1_REF   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ ADM1ALT1EN &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ ADM1ALT2EN &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ ADM1ALT1TH &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ ADM1ALT2TH &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ ADM0_EN    &lt;chr&gt; \"Thailand\", \"Thailand\", \"Thailand\", \"Thailand\", \"Thailand\",…\n$ ADM0_TH    &lt;chr&gt; \"ประเทศไทย\", \"ประเทศไทย\", \"ประเทศไทย\", \"ประเทศไทย\", \"ประเทศ…\n$ ADM0_PCODE &lt;chr&gt; \"TH\", \"TH\", \"TH\", \"TH\", \"TH\", \"TH\", \"TH\", \"TH\", \"TH\", \"TH\",…\n$ date       &lt;date&gt; 2019-02-18, 2019-02-18, 2019-02-18, 2019-02-18, 2019-02-18…\n$ validOn    &lt;date&gt; 2022-01-22, 2022-01-22, 2022-01-22, 2022-01-22, 2022-01-22…\n$ validTo    &lt;date&gt; -001-11-30, -001-11-30, -001-11-30, -001-11-30, -001-11-30…\n$ geometry   &lt;MULTIPOLYGON [°]&gt; MULTIPOLYGON (((100.6139 13..., MULTIPOLYGON (…\n\n\nFor clarity, we can clean up this dataframe by:\n\nKeeping only relevant columns: The province name and code, geometry\nRenaming the columns: change ADM1 to Province\n\nThe following code chunk executes these steps by using select() for the first step and rename() for the second step. We again use glimpse() to give a preview of the dataset’s columns.\n\nthai_sf &lt;- thai_sf %&gt;%\n  select(ADM1_EN, ADM1_PCODE, geometry) %&gt;%\n  rename(Province = ADM1_EN, ProvCode = ADM1_PCODE)\n\nglimpse(thai_sf)\n\nRows: 77\nColumns: 3\n$ Province &lt;chr&gt; \"Bangkok\", \"Samut Prakan\", \"Nonthaburi\", \"Pathum Thani\", \"Phr…\n$ ProvCode &lt;chr&gt; \"TH10\", \"TH11\", \"TH12\", \"TH13\", \"TH14\", \"TH15\", \"TH16\", \"TH17…\n$ geometry &lt;MULTIPOLYGON [°]&gt; MULTIPOLYGON (((100.6139 13..., MULTIPOLYGON (((…\n\n\nWe can check if there are any missing values by using is.na() and then check across each column using colSums() from Base R.\n\ncolSums(is.na(thai_sf))\n\nProvince ProvCode geometry \n       0        0        0 \n\n\nThe output shows that there are no missing values for any of the retained columns.\nFinally, we can quickly check if the object depicts Thailand properly by producing a quick map using qtm() from tmap package.\n\nqtm(thai_sf)"
  },
  {
    "objectID": "posts/thai-tourism-covid/index.html#b.2.-thailand-tourism-data-by-province-jan-2019---feb-2023",
    "href": "posts/thai-tourism-covid/index.html#b.2.-thailand-tourism-data-by-province-jan-2019---feb-2023",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "B.2. Thailand Tourism Data by Province, Jan 2019 - Feb 2023",
    "text": "B.2. Thailand Tourism Data by Province, Jan 2019 - Feb 2023\nThe code chunk below loads the tourism statistics data into a dataframe tourism. We use read_csv() to import the data from the file.\n\ntourism &lt;- read_csv(\"data/aspatial/thailand_domestic_tourism_2019_2023.csv\")\n\nRows: 30800 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): province_thai, province_eng, region_thai, region_eng, variable\ndbl  (1): value\ndate (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWe can check the contents by using the code chunk below.\n\ntourism\n\n# A tibble: 30,800 × 7\n   date       province_thai province_eng   region_thai region_eng variable value\n   &lt;date&gt;     &lt;chr&gt;         &lt;chr&gt;          &lt;chr&gt;       &lt;chr&gt;      &lt;chr&gt;    &lt;dbl&gt;\n 1 2019-01-01 กรุงเทพมหานคร  Bangkok        ภาคกลาง     central    occupan…  93.4\n 2 2019-01-01 ลพบุรี          Lopburi        ภาคกลาง     central    occupan…  61.3\n 3 2019-01-01 พระนครศรีอยุธยา Phra Nakhon S… ภาคกลาง     central    occupan…  73.4\n 4 2019-01-01 สระบุรี         Saraburi       ภาคกลาง     central    occupan…  67.3\n 5 2019-01-01 ชัยนาท         Chainat        ภาคกลาง     central    occupan…  79.3\n 6 2019-01-01 นครปฐม        Nakhon Pathom  ภาคกลาง     central    occupan…  71.7\n 7 2019-01-01 สิงห์บุรี         Sing Buri      ภาคกลาง     central    occupan…  64.6\n 8 2019-01-01 อ่างทอง        Ang Thong      ภาคกลาง     central    occupan…  71.2\n 9 2019-01-01 นนทบุรี         Nonthaburi     ภาคกลาง     central    occupan…  75.1\n10 2019-01-01 ปทุมธานี        Pathum Thani   ภาคกลาง     central    occupan…  60.8\n# ℹ 30,790 more rows\n\n\nThe imported data contains 7 fields and 30,800 records at a province and month level.\nBefore we analyze the dataset, let use remove unnecessary columns and rename the column names, similar to the previous dataset, using the code chunk below. (by using select() and rename())\n\ntourism &lt;- tourism %&gt;%\n  select(date, province_eng, region_eng, variable, value) %&gt;%\n  rename(Date = date, Province = province_eng, Region = region_eng, Indicator = variable, Value = value)\n\nhead(tourism)\n\n# A tibble: 6 × 5\n  Date       Province                 Region  Indicator      Value\n  &lt;date&gt;     &lt;chr&gt;                    &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 2019-01-01 Bangkok                  central occupancy_rate  93.4\n2 2019-01-01 Lopburi                  central occupancy_rate  61.3\n3 2019-01-01 Phra Nakhon Si Ayutthaya central occupancy_rate  73.4\n4 2019-01-01 Saraburi                 central occupancy_rate  67.3\n5 2019-01-01 Chainat                  central occupancy_rate  79.3\n6 2019-01-01 Nakhon Pathom            central occupancy_rate  71.7\n\n\nWe have kept only five of the columns which provides the date, the English descriptions for the location (province and region) as well as the (potential) tourism indicator and its value.\nWe can also check for any missing values across these five columns using the code below. (using is.na() and colSums())\n\ncolSums(is.na(tourism))\n\n     Date  Province    Region Indicator     Value \n        0         0         0         0         0 \n\n\nEach province will be repeated across multiple dates and across multiple indicators. Let us first doublecheck the different values in Indicator. We use unique() in the code chunk below to achieve this.\n\nunique(tourism$Indicator)\n\n[1] \"occupancy_rate\"      \"no_tourist_occupied\" \"no_tourist_all\"     \n[4] \"no_tourist_thai\"     \"no_tourist_foreign\"  \"net_profit_all\"     \n[7] \"net_profit_thai\"     \"net_profit_foreign\" \n\n\nWe are aware that the ‘net_profit’ indicators are actually revenue so it is better to update them now to avoid misunderstanding later. We use recode() from dplyr to replace instances with alternative values.\n\ntourism &lt;- tourism %&gt;%\n  mutate(Indicator = recode(Indicator,\n                            \"net_profit_all\" = \"revenue_all\",\n                            \"net_profit_thai\" = \"revenue_thai\",\n                            \"net_profit_foreign\" = \"revenue_foreign\"))\n\nunique(tourism$Indicator)\n\n[1] \"occupancy_rate\"      \"no_tourist_occupied\" \"no_tourist_all\"     \n[4] \"no_tourist_thai\"     \"no_tourist_foreign\"  \"revenue_all\"        \n[7] \"revenue_thai\"        \"revenue_foreign\"    \n\n\nWe will not define which indicators to use until we perform some EDA (Exploratory Data Analysis) in the next section.\nBefore we move to the next section, we will also introduce some columns into the dataset to make filtering and other analysis easier. For now, we will do this by adding columns for the months and years based on the Date column.\n\ntourism &lt;- tourism %&gt;%\n  mutate(Year = year(Date)) %&gt;%\n  mutate(MonthNum = month(Date)) %&gt;%\n  mutate(Month = month(Date, label = TRUE, abbr = TRUE)) %&gt;%\n  mutate(MonthYear = format(ymd(Date), \"%Y-%m\"))\n\nhead(tourism)\n\n# A tibble: 6 × 9\n  Date       Province      Region Indicator Value  Year MonthNum Month MonthYear\n  &lt;date&gt;     &lt;chr&gt;         &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;ord&gt; &lt;chr&gt;    \n1 2019-01-01 Bangkok       centr… occupanc…  93.4  2019        1 Jan   2019-01  \n2 2019-01-01 Lopburi       centr… occupanc…  61.3  2019        1 Jan   2019-01  \n3 2019-01-01 Phra Nakhon … centr… occupanc…  73.4  2019        1 Jan   2019-01  \n4 2019-01-01 Saraburi      centr… occupanc…  67.3  2019        1 Jan   2019-01  \n5 2019-01-01 Chainat       centr… occupanc…  79.3  2019        1 Jan   2019-01  \n6 2019-01-01 Nakhon Pathom centr… occupanc…  71.7  2019        1 Jan   2019-01"
  },
  {
    "objectID": "posts/thai-tourism-covid/index.html#c.1-tourism-revenue",
    "href": "posts/thai-tourism-covid/index.html#c.1-tourism-revenue",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "C.1 Tourism Revenue",
    "text": "C.1 Tourism Revenue\nWe first look at tourism revenue which is currently reported in million Thai baht. We will use a constant rate of 34.784 THB per USD based on 2023 exchange rates to scale down the numbers and transform it into something more recognizable for most of the readers.\nWe first create a plot for the monthly tourism revenue in total and by foreign and local tourists. The code below selects the relevant data and prepares the line plot using ggplot(). Finally, we use ggplotly() to render it as an interactive chart so we can easily examine the resulting chart.\n\n# Subset the data to just the required indicators\naggregated_data &lt;- tourism %&gt;%\n  filter(Indicator %in% c(\"revenue_all\", \"revenue_thai\", \"revenue_foreign\")) %&gt;%\n  group_by(MonthYear, Indicator) %&gt;%\n  summarise(TotalValue = sum(Value, na.rm = TRUE) / 34.784) %&gt;%\n  ungroup()\n\n`summarise()` has grouped output by 'MonthYear'. You can override using the\n`.groups` argument.\n\n# Create the line chart\np &lt;- ggplot(aggregated_data, aes(x = MonthYear, y = TotalValue, color = Indicator, group = Indicator)) +\n  geom_line(size = 1) +\n  labs(title = \"Thai Tourism Revenue by Month\",\n       y = \"Million USD\",\n       x = \"Month-Year\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),\n        legend.position = \"bottom\") +\n  scale_color_manual(values = c(\"revenue_all\" = \"blue\", \"revenue_thai\" = \"green\", \"revenue_foreign\" = \"red\"))\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n# Convert the ggplot chart to an interactive plotly chart\ninteractive_plot &lt;- ggplotly(p)\n\n# Display the interactive plot\ninteractive_plot\n\n\n\n\n\nThe resulting chart is consistent with the expectation on the impact and recovery from the pandemic. We can view the above chart as a timeline:\n\nUp to (mid)January 2020: Pre-covid. No travel restrictions have been set yet. (Jan 2019 - Jan 2020, 13 months)\nFeb 2020 to November 2021: Covid. Various lockdown measures in place. All foreign non-essential travel is banned. There is some local tourist activity, but another set of measures in May 2021 again prevents non-essential movement (Feb 2020 - Oct 2021, 22 months)\nNovember 2021 onwards: Post-Covid. Travel restrictions have been eased or lifted and tourism revenues have been recovering (Nov 2021 - Feb 2023, 16 months)\n\nPre- and post-covid, we see that foreign tourists contribute more to the overall revenue, and their contribution has a large amount of variance. Local tourists during the same period have contributed a more stable amount month-on-month.\nWe can code the three periods mentioned above into the tourism dataset for convenience. We use the ifelse() function to do this based on the cutoff dates mentioned above.\n\ntourism$Period &lt;- ifelse(tourism$Date &lt; as.Date(\"2020-02-01\"), \"Pre-Covid\",\n                         ifelse(tourism$Date &gt; as.Date(\"2021-10-01\"), \"Post-Covid\", \"Covid\"))\n\nWe can next check the tourism revenue at the province level. As plotting all 77 provinces across all the periods will not produce readable charts, we will focus on top 20 provinces for the different periods. We will also take the average monthly revenue rather than the total since each period has a different number of months.\nWe prepare a new dataframe that summarizes the indicators for each province in tourism. Aside from the average revenue per period, we will also compute for the average number of visitors as well as the average spend per visitor.\n\ntourism_period &lt;- tourism %&gt;%\n  group_by(Province) %&gt;%\n  summarise(\n    PreCovid_Revenue_total = sum(Value[Period == \"Pre-Covid\" & Indicator == \"revenue_all\"], na.rm = TRUE)/13/34.784,\n    Covid_Revenue_total = sum(Value[Period == \"Covid\" & Indicator == \"revenue_all\"], na.rm = TRUE)/22/34.784,\n    PostCovid_Revenue_total = sum(Value[Period == \"Post-Covid\" & Indicator == \"revenue_all\"], na.rm = TRUE)/16/34.784,\n    PreCovid_Revenue_foreign = sum(Value[Period == \"Pre-Covid\" & Indicator == \"revenue_foreign\"], na.rm = TRUE)/13/34.784,\n    Covid_Revenue_foreign = sum(Value[Period == \"Covid\" & Indicator == \"revenue_foreign\"], na.rm = TRUE)/22/34.784,\n    PostCovid_Revenue_foreign = sum(Value[Period == \"Post-Covid\" & Indicator == \"revenue_foreign\"], na.rm = TRUE)/16/34.784,\n    PreCovid_Revenue_thai = sum(Value[Period == \"Pre-Covid\" & Indicator == \"revenue_thai\"], na.rm = TRUE)/13/34.784,\n    Covid_Revenue_thai = sum(Value[Period == \"Covid\" & Indicator == \"revenue_thai\"], na.rm = TRUE)/22/34.784,\n    PostCovid_Revenue_thai = sum(Value[Period == \"Post-Covid\" & Indicator == \"revenue_thai\"], na.rm = TRUE)/16/34.784,\n    PreCovid_tourists_total = sum(Value[Period == \"Pre-Covid\" & Indicator == \"no_tourist_all\"], na.rm = TRUE)/13,\n    Covid_tourists_total = sum(Value[Period == \"Covid\" & Indicator == \"no_tourist_all\"], na.rm = TRUE)/22,\n    PostCovid_tourists_total = sum(Value[Period == \"Post-Covid\" & Indicator == \"no_tourist_all\"], na.rm = TRUE)/16,\n    PreCovid_tourists_foreign = sum(Value[Period == \"Pre-Covid\" & Indicator == \"no_tourist_foreign\"], na.rm = TRUE)/13,\n    Covid_tourists_foreign = sum(Value[Period == \"Covid\" & Indicator == \"no_tourist_foreign\"], na.rm = TRUE)/22,\n    PostCovid_tourists_foreign = sum(Value[Period == \"Post-Covid\" & Indicator == \"no_tourist_foreign\"], na.rm = TRUE)/16,\n    PreCovid_tourists_thai = sum(Value[Period == \"Pre-Covid\" & Indicator == \"no_tourist_thai\"], na.rm = TRUE)/13,\n    Covid_tourists_thai = sum(Value[Period == \"Covid\" & Indicator == \"no_tourist_thai\"], na.rm = TRUE)/22,\n    PostCovid_tourists_thai = sum(Value[Period == \"Post-Covid\" & Indicator == \"no_tourist_thai\"], na.rm = TRUE)/16\n  ) %&gt;%\n  mutate(PreCovidSpend_total = PreCovid_Revenue_total / PreCovid_tourists_total * 1000000) %&gt;%\n  mutate(CovidSpend_total = Covid_Revenue_total / Covid_tourists_total * 1000000) %&gt;%\n  mutate(PostCovidSpend_total = PostCovid_Revenue_total / PostCovid_tourists_total * 1000000) %&gt;%\n  mutate(PreCovidSpend_foreign = PreCovid_Revenue_foreign / PreCovid_tourists_foreign * 1000000) %&gt;%\n  mutate(CovidSpend_foreign = Covid_Revenue_foreign / Covid_tourists_foreign * 1000000) %&gt;%\n  mutate(PostCovidSpend_foreign = PostCovid_Revenue_foreign / PostCovid_tourists_foreign * 1000000) %&gt;%\n  mutate(PreCovidSpend_thai = PreCovid_Revenue_thai / PreCovid_tourists_thai * 1000000) %&gt;%\n  mutate(CovidSpend_thai = Covid_Revenue_thai / Covid_tourists_thai * 1000000) %&gt;%\n  mutate(PostCovidSpend_thai = PostCovid_Revenue_thai / PostCovid_tourists_thai * 1000000)\n\nWith the summarized dataframe prepared, we can now prepare a few visualizations to look at the provinces with regards to the average monthly tourism revenue.\nFirst, let us try using a scatterplot to see both the average revenue pre-Covid (x-axis) and post-Covid. (y-axis) Provinces with the highest pre-Covid revenue will appear the rightmost, while those that have the highest post-Covid revenue will appear the rightmost.\nThe code below uses the plotly package to produce an interactive scatterplot of the pre- and post-covid average monthly revenue for all tourists. With the interactive chart, the province names will be visible by hovering over and the user can zoom in by creating a selection in the chart.\n\nplot &lt;- plot_ly(\n  data = tourism_period,\n  x = ~PreCovid_Revenue_total,\n  y = ~PostCovid_Revenue_total,\n  type = 'scatter',\n  mode = 'markers',\n  text = ~Province,\n  hoverinfo = 'text'\n) %&gt;%\n  layout(\n    title = 'Monthly Pre- and Post-Covid Revenue - M-USD',\n    xaxis = list(title = 'Average PreCovid Revenue'),\n    yaxis = list(title = 'Average PostCovid Revenue')\n  )\n\n# Display the plot\nplot\n\n\n\n\n\nBaed on the plot, we see that Bangkok, Phuket and Chonburi have consistently been the top 3 highest revenue generating before and after the pandemic. When we look further down the list, we see some shifts for some of the provinces.\nTo aid the reader, we recreate the chart with those top 3 provinces excluded using the code chunk below.\n\nplot &lt;- plot_ly(\n  data = filter(tourism_period, !(Province %in% c(\"Bangkok\", \"Phuket\", \"Chonburi\"))),\n  x = ~PreCovid_Revenue_total,\n  y = ~PostCovid_Revenue_total,\n  type = 'scatter',\n  mode = 'markers',\n  text = ~Province,\n  hoverinfo = 'text'\n) %&gt;%\n  layout(\n    title = 'Monthly Pre- and Post-Covid Revenue - M-USD, exc Top 3 Provinces',\n    xaxis = list(title = 'Average PreCovid Revenue'),\n    yaxis = list(title = 'Average PostCovid Revenue')\n  )\n\n# Display the plot\nplot\n\n\n\n\n\nSome key observations from the above charts are:\n\nChiang Mai has moved from top 5 to top 4. A large reason for this is a drop from Krabi. Pre-covid, Krabi was top 4, but has dropped to at least top 10.\nChiang Rai and Prachuap Khiri Khan have risen to top 5 and 6. These provinces were top 9 or lower before.\nSongkhla and Phang Nga were in the top 10 pre-Covid but are also showing a drop in ranking post-Covid\n\nWe can do the same chart for just the revenue from foreign tourists using the code chunk below.\n\nplot &lt;- plot_ly(\n  data = tourism_period,\n  x = ~PreCovid_Revenue_foreign,\n  y = ~PostCovid_Revenue_foreign,\n  type = 'scatter',\n  mode = 'markers',\n  text = ~Province,\n  hoverinfo = 'text'\n) %&gt;%\n  layout(\n    title = 'Monthly Pre- and Post-Covid Revenue from Foreign Tourists - M-USD',\n    xaxis = list(title = 'Average PreCovid Revenue'),\n    yaxis = list(title = 'Average PostCovid Revenue')\n  )\n\n# Display the plot\nplot\n\n\n\n\n\nThe top 3 provinces are the same for both, but there are differences further down the list.\nWe can also show these numbers graphically in a map. Before we do this, let us add one set of measures in tourism_period to indicate the recovery rate. This will be the ratio of the post-covid and pre-covid measures and will indicate how much of the pre-covid level has been achieved. (on average)\nWe perform this using the code chunk below. We do this for all sets of measures across revenue, number of tourists and the average spending.\n\ntourism_period &lt;- tourism_period %&gt;%\n  mutate(Revenue_total_recovery = PostCovid_Revenue_total / PreCovid_Revenue_total) %&gt;%\n  mutate(Revenue_foreign_recovery = PostCovid_Revenue_foreign / PreCovid_Revenue_foreign) %&gt;%\n  mutate(Revenue_thai_recovery = PostCovid_Revenue_thai / PreCovid_Revenue_thai) %&gt;%\n  mutate(Tourists_total_recovery = PostCovid_tourists_total / PreCovid_tourists_total) %&gt;%\n  mutate(Tourists_foreign_recovery = PostCovid_tourists_foreign / PreCovid_tourists_foreign) %&gt;%\n  mutate(Tourists_thai_recovery = PostCovid_tourists_thai / PreCovid_tourists_thai) %&gt;%\n  mutate(Spend_total_recovery = PostCovidSpend_total / PreCovidSpend_total) %&gt;%\n  mutate(Spend_foreign_recovery = PostCovidSpend_foreign / PreCovidSpend_foreign) %&gt;%\n  mutate(Spend_thai_recovery = PostCovidSpend_thai / PreCovidSpend_thai)\n\nWe need to include all the indicators into the sf dataframe. This means merging the tourism_period and the thai_sf dataframes. Let us first check that the naming is the same for both dataframes by checking which values do not have a match. We use the code below which uses left_join() to match and then filter() to check those that do not have matches.\n\n# Identify mismatched Province names in tourism_period\nmismatched_values &lt;- tourism_period %&gt;%\n  left_join(thai_sf, by = \"Province\") %&gt;%\n  filter(is.na(ProvCode)) %&gt;%\n  select(Province)\n\nmismatched_tourism &lt;- mismatched_values$Province\n\n# Identify mismatched Province names in thai_sf\nmismatched_values &lt;- thai_sf %&gt;%\n  left_join(tourism_period, by = \"Province\") %&gt;%\n  filter(is.na(Covid_Revenue_total)) %&gt;%\n  select(Province)\n\nmismatched_thai &lt;- mismatched_values$Province\n\n# Print the mismatched values\nlist(\n  mismatched_in_tourism_period = mismatched_tourism,\n  mismatched_in_thai_sf = mismatched_thai\n)\n\n$mismatched_in_tourism_period\n[1] \"Buriram\"         \"Chainat\"         \"Chonburi\"        \"Lopburi\"        \n[5] \"Nong Bua Lamphu\" \"Phang Nga\"       \"Prachinburi\"     \"Sisaket\"        \n\n$mismatched_in_thai_sf\n[1] \"Lop Buri\"         \"Chai Nat\"         \"Chon Buri\"        \"Prachin Buri\"    \n[5] \"Buri Ram\"         \"Si Sa Ket\"        \"Nong Bua Lam Phu\" \"Phangnga\"        \n\n\nWe see that there are 8 mismatched province names for each of the dataframes. We need to standardize these namings to ensure that the indicators are mapped to the correct province. We will opt to keep the descriptions from tourism_period which gives more compact naming. We use recode() in the code chunk below to accomplish this in a new dataframe.\n\nthaitourism_sf &lt;- thai_sf %&gt;%\n  mutate(Province = recode(Province,\n                            \"Lop Buri\" = \"Lopburi\",\n                            \"Chai Nat\" = \"Chainat\",\n                            \"Chon Buri\" = \"Chonburi\",\n                            \"Prachin Buri\" = \"Prachinburi\",\n                            \"Buri Ram\" = \"Buriram\",\n                            \"Si Sa Ket\" = \"Sisaket\",\n                            \"Nong Bua Lam Phu\" = \"Nong Bua Lamphu\",\n                            \"Phangnga\" = \"Phang Nga\"))\n\nWe can now use leftjoin() in the codechunk below to merge the two datasets.\n\nthaitourism_sf &lt;- left_join(thaitourism_sf, tourism_period,\n                     by=c(\"Province\"=\"Province\"))\n\nThe code chunk below confirms that the new object is still an sf dataframe.\n\nclass(thaitourism_sf)\n\n[1] \"sf\"         \"data.frame\"\n\n\nWe can use the tmap package to produce side-by side maps of Thailand with the average monthly tourism revenue before and after covid using the code chunk below. Given the wide range of values, we will use quantiles for the data classes. We also include the recovery rate of each of the provinces as a third map.\n\ntm_shape(thaitourism_sf) +\n  tm_fill(col = c(\"PreCovid_Revenue_total\", \"PostCovid_Revenue_total\", \"Revenue_total_recovery\"),\n          style = \"quantile\",\n          palette = list(\"viridis\", \"viridis\", \"RdYlGn\"),\n          title = c(\"Monthly Revenue - M-USD\", \"Monthly Revenue - M-USD\", \"Post vs Pre\")) +\n  tm_borders(col = \"black\") +\n  tm_layout(title = c(\"Pre Covid\", \"Post Covid\", \"Recovery Rate\"),\n            title.position = c(\"right\", \"top\"),\n            legend.position = c(\"right\", \"bottom\"),\n            bg.color = \"grey90\")\n\n\n\n\n\n\n\n\nAs also seen in the scatterplot, we also see some change in rankings with the maps. For example, some provinces previously in the top 20% have moved down to the next 20%. (e.g., Khon Kaen and Phang Nga)\nIf we focus on the third map, we also see what seems like a cluster of provinces in the south which are lagging with regards to their recovery on the average tourism revenue. We will watch out for these once we conduct our cluster analyses."
  },
  {
    "objectID": "posts/thai-tourism-covid/index.html#c.2-number-of-tourists",
    "href": "posts/thai-tourism-covid/index.html#c.2-number-of-tourists",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "C.2 Number of Tourists",
    "text": "C.2 Number of Tourists\nThe next measure we can look at is the number of tourists. We produce a similar line chart as we did for tourism revenue with the code chunk below. We display the number of tourists in thousands.\n\n# Subset the data to just the required indicators\naggregated_data &lt;- tourism %&gt;%\n  filter(Indicator %in% c(\"no_tourist_all\", \"no_tourist_thai\", \"no_tourist_foreign\")) %&gt;%\n  group_by(MonthYear, Indicator) %&gt;%\n  summarise(TotalValue = sum(Value, na.rm = TRUE) / 1000) %&gt;%\n  ungroup()\n\n`summarise()` has grouped output by 'MonthYear'. You can override using the\n`.groups` argument.\n\n# Create the line chart\np &lt;- ggplot(aggregated_data, aes(x = MonthYear, y = TotalValue, color = Indicator, group = Indicator)) +\n  geom_line(size = 1) +\n  labs(title = \"Thailand Number of Tourists by Month\",\n       y = \"Tourists, Thousands\",\n       x = \"Month-Year\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),\n        legend.position = \"bottom\") +\n  scale_color_manual(values = c(\"no_tourist_all\" = \"blue\", \"no_tourist_thai\" = \"green\", \"no_tourist_foreign\" = \"red\"))\n\n# Convert the ggplot chart to an interactive plotly chart\ninteractive_plot &lt;- ggplotly(p)\n\n# Display the interactive plot\ninteractive_plot\n\n\n\n\n\nThe trend for the total follows the same general movement as the chart for revenue, however, it looks like tourist numbers are primarily driven by locals than foreigners.\nSimilar to the previous section, we can produce an interactive scatterplot to see the number of tourists each province gets on average before and after the pandemic. We do this first for the total number of tourists.\n\nplot &lt;- plot_ly(\n  data = tourism_period,\n  x = ~PreCovid_tourists_total,\n  y = ~PostCovid_tourists_total,\n  type = 'scatter',\n  mode = 'markers',\n  text = ~Province,\n  hoverinfo = 'text'\n) %&gt;%\n  layout(\n    title = 'Monthly Pre- and Post-Covid No of Tourists',\n    xaxis = list(title = 'PreCovid Average'),\n    yaxis = list(title = 'PostCovid Average')\n  )\n\n# Display the plot\nplot\n\n\n\n\n\nBangkok, Chonburi and Phuket had the highest number of visitors pre-Covid, but Phuket appears to have dropped off in favor of Kanchanaburi post-covid.\nWe can also look at the foreign tourists using the code chunk below. We skip local tourists and focus only on foreign ones as they deviate from the overall number.\n\nplot &lt;- plot_ly(\n  data = tourism_period,\n  x = ~PreCovid_tourists_foreign,\n  y = ~PostCovid_tourists_foreign,\n  type = 'scatter',\n  mode = 'markers',\n  text = ~Province,\n  hoverinfo = 'text'\n) %&gt;%\n  layout(\n    title = 'Monthly Pre- and Post-Covid No of Foreign Tourists',\n    xaxis = list(title = 'PreCovid Average'),\n    yaxis = list(title = 'PostCovid Average')\n  )\n\n# Display the plot\nplot\n\n\n\n\n\nBangkok, Phuket and Chonburi appear as the top 3 destinations for foreign tourists (in terms of number) before and after Covid. We also see that Krabi dropped from the top 3 post covid.\nWe can also produce side-by-side maps for the number of tourists and their recovery rates using the code chunk below.\n\ntm_shape(thaitourism_sf) +\n  tm_fill(col = c(\"PreCovid_tourists_total\", \"PostCovid_tourists_total\", \"Tourists_total_recovery\"),\n          style = \"quantile\",\n          palette = list(\"viridis\", \"viridis\", \"RdYlGn\"),\n          title = c(\"Monthly Tourists\", \"Monthly Tourists\", \"Post vs Pre\")) +\n  tm_borders(col = \"black\") +\n  tm_layout(title = c(\"Pre Covid\", \"Post Covid\", \"Recovery Rate\"),\n            title.position = c(\"right\", \"top\"),\n            legend.position = c(\"right\", \"bottom\"),\n            bg.color = \"grey90\")\n\n\n\n\n\n\n\n\nWe again see the southern region mostly lagging with regards to their recovery post covid also in terms of the number of visitors."
  },
  {
    "objectID": "posts/thai-tourism-covid/index.html#c.3-average-spend-per-visitor",
    "href": "posts/thai-tourism-covid/index.html#c.3-average-spend-per-visitor",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "C.3 Average Spend Per Visitor",
    "text": "C.3 Average Spend Per Visitor\nWe next look at the average per spend per visitor which is the quotient of the tourism revenue and the total number of tourists. This will tell us whether tourists are spending more or less around the pandemic.\nFirst we produce a similar line graph as before to look at the trend at an overall picture using the code chunk below.\n\n# Subset the data to just the required indicators\naggregated_data &lt;&lt;- tourism %&gt;%\n  group_by(MonthYear) %&gt;%\n  summarise(\n    Spend_total = sum(Value[Indicator == \"revenue_all\"]) / sum(Value[Indicator == \"no_tourist_all\"]) * 1000000 / 34.784,\n    Spend_thai = sum(Value[Indicator == \"revenue_thai\"]) / sum(Value[Indicator == \"no_tourist_thai\"]) * 1000000 / 34.784,\n    Spend_foreign = sum(Value[Indicator == \"revenue_foreign\"]) / sum(Value[Indicator == \"no_tourist_foreign\"]) * 1000000 / 34.784\n  ) %&gt;%\n  pivot_longer(cols = starts_with(\"Spend\"), names_to = \"Indicator\", values_to = \"TotalValue\") %&gt;%\n  mutate(Indicator = case_when(\n    Indicator == \"Spend_total\" ~ \"Spend_total\",\n    Indicator == \"Spend_thai\" ~ \"Spend_thai\",\n    Indicator == \"Spend_foreign\" ~ \"Spend_foreign\"\n  ))\n\n# Create the line chart\np &lt;- ggplot(aggregated_data, aes(x = MonthYear, y = TotalValue, color = Indicator, group = Indicator)) +\n  geom_line(size = 1) +\n  labs(title = \"Thailand Average Spend Per Tourist by Month\",\n       y = \"Average Spend, USD\",\n       x = \"Month-Year\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),\n        legend.position = \"bottom\") +\n  scale_color_manual(values = c(\"Spend_total\" = \"blue\", \"Spend_thai\" = \"green\", \"Spend_foreign\" = \"red\"))\n\n# Convert the ggplot chart to an interactive plotly chart\ninteractive_plot &lt;- ggplotly(p)\n\n# Display the interactive plot\ninteractive_plot\n\n\n\n\n\nThe chart shows that the average spend of foreign tourists are much higher than local ones and appears to be the same before and after Covid. There appears to be a shift in the average spend for all tourists overall which is probably driven by an increase in the contribution for the number of local tourists versus foreign tourists. Local tourists appear to show a step decrease after covid as well in terms of their average spending.\nNext, we check the average spending of tourists across provinces using a similar scatterplot as before.\n\nplot &lt;- plot_ly(\n  data = tourism_period,\n  x = ~PreCovidSpend_total,\n  y = ~PostCovidSpend_total,\n  type = 'scatter',\n  mode = 'markers',\n  text = ~Province,\n  hoverinfo = 'text'\n) %&gt;%\n  layout(\n    title = 'Monthly Pre- and Post-Covid Average Spend Per Tourist',\n    xaxis = list(title = 'PreCovid Average $'),\n    yaxis = list(title = 'PostCovid Average $')\n  )\n\n# Display the plot\nplot\n\n\n\n\n\nPhuket saw the highest average spend pre- and post-covid. Krabi was second highest pre-Covid but drop to third, as Bangkok rose from fourth to second over that period.\nWe can do the same for foreign tourists as the earlier chart showed that it was very different from the total. We use the code chunk below tor produce a similar scatterplot but taking the foreign tourist figures.\n\nplot &lt;- plot_ly(\n  data = tourism_period,\n  x = ~PreCovidSpend_foreign,\n  y = ~PostCovidSpend_foreign,\n  type = 'scatter',\n  mode = 'markers',\n  text = ~Province,\n  hoverinfo = 'text'\n) %&gt;%\n  layout(\n    title = 'Monthly Pre- and Post-Covid Average Spend Per Foreign Tourist',\n    xaxis = list(title = 'PreCovid Average $'),\n    yaxis = list(title = 'PostCovid Average $')\n  )\n\n# Display the plot\nplot\n\n\n\n\n\nPhuket, Bangkok and Chonburi have been consistent as top 4 highest spend for foreign tourists. From the chart above, we see that Chiang Rai has risen to the number five spot for highest foreign tourist spending post-Covid.\nWe can also produce a map visualization of the average spend using tmap package. We focus on the total tourist population in the visualization below.\n\ntm_shape(thaitourism_sf) +\n  tm_fill(col = c(\"PreCovidSpend_total\", \"PostCovidSpend_total\", \"Spend_total_recovery\"),\n          style = \"quantile\",\n          palette = list(\"viridis\", \"viridis\", \"RdYlGn\"),\n          title = c(\"Average Tourist Spending\", \"Average Tourist Spending\", \"Post vs Pre\")) +\n  tm_borders(col = \"black\") +\n  tm_layout(title = c(\"Pre Covid\", \"Post Covid\", \"Recovery Rate\"),\n            title.position = c(\"right\", \"top\"),\n            legend.position = c(\"right\", \"bottom\"),\n            bg.color = \"grey90\")\n\n\n\n\n\n\n\n\nWe again see slow recovery in the southern region, but at the same time, this region has the highest average spending pre- and post-covid. (i.e., top 20%)"
  },
  {
    "objectID": "posts/thai-tourism-covid/index.html#c.4-occupancy-rate",
    "href": "posts/thai-tourism-covid/index.html#c.4-occupancy-rate",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "C.4 Occupancy Rate",
    "text": "C.4 Occupancy Rate\nThe final single indicator we will look at is the occupancy rate. We have not prepared and included the data for occupancy rate before as this is a ratio measure which be just summed or averaged. In order to be able to aggregate occupancy rate, we not only need the actual occupancy rate from the data, but we also need the number of rooms occupied which is given by the no_tourist_occupied indicator in the data, and also the number of rooms in total– which is not included in the data.\nThe following code chunk prepares a new dataframe from tourist with the following transformation steps:\n\nRetain MonthYear, Province, and records for occupany rate and number of rooms occupied\nKeep values for the two indicators as separate columns. We use average just in case a province appears multiple times on the same date (to resolve conflicts)\nWe compute the total number of rooms as the number of rooms occupied divided by the occupancy rate\nWe add the tags for the period for pre- and post- covid\n\n\noccupancy_df &lt;- tourism %&gt;%\n  filter(Indicator %in% c(\"occupancy_rate\", \"no_tourist_occupied\")) %&gt;%\n  group_by(Date, MonthYear, Province) %&gt;%\n  summarise(\n    occupancy = mean(Value[Indicator == \"occupancy_rate\"], na.rm = TRUE),\n    occupied_rooms = mean(Value[Indicator == \"no_tourist_occupied\"], na.rm = TRUE)\n  ) %&gt;%\n  ungroup() %&gt;%\n  mutate(total_rooms = ifelse(occupancy == 0, 0, occupied_rooms / occupancy * 100))\n\n`summarise()` has grouped output by 'Date', 'MonthYear'. You can override using\nthe `.groups` argument.\n\noccupancy_df$Period &lt;- ifelse(occupancy_df$Date &lt; as.Date(\"2020-02-01\"), \"Pre-Covid\",\n                         ifelse(occupancy_df$Date &gt; as.Date(\"2021-10-01\"), \"Post-Covid\", \"Covid\"))\n\nhead(occupancy_df)\n\n# A tibble: 6 × 7\n  Date       MonthYear Province      occupancy occupied_rooms total_rooms Period\n  &lt;date&gt;     &lt;chr&gt;     &lt;chr&gt;             &lt;dbl&gt;          &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt; \n1 2019-01-01 2019-01   Amnat Charoen      65.2           8551      13125. Pre-C…\n2 2019-01-01 2019-01   Ang Thong          71.2          19140      26878. Pre-C…\n3 2019-01-01 2019-01   Bangkok            93.4        3334971    3571780. Pre-C…\n4 2019-01-01 2019-01   Bueng Kan          73.0          37974      52055. Pre-C…\n5 2019-01-01 2019-01   Buriram            71.3         113655     159493. Pre-C…\n6 2019-01-01 2019-01   Chachoengsao       59.4          38687      65130. Pre-C…\n\n\nBefore we produce the charts, let us update thaitourism_sf with the aggregated occupancy rates by:\n\nComputing Pre- and Post-covid total number of rooms and occupied rooms per province\nComputing Pre- and Post-covid occupancy rate per province based on step 1\nAdd the new columns into thaitourism_sf using left_join()\n\nThe first two steps are accomplished in the first code block while the third step is accomplished in the second.\n\noccupancy_summary &lt;- occupancy_df %&gt;%\n  group_by(Province) %&gt;%\n  summarise(\n    PreCovid_occupied_rooms = sum(occupied_rooms[Period == \"Pre-Covid\"], na.rm = TRUE),\n    PostCovid_occupied_rooms = sum(occupied_rooms[Period == \"Post-Covid\"], na.rm = TRUE),\n    PreCovid_total_rooms = sum(total_rooms[Period == \"Pre-Covid\"], na.rm = TRUE),\n    PostCovid_total_rooms = sum(total_rooms[Period == \"Post-Covid\"], na.rm = TRUE)\n  ) %&gt;%\n  mutate(\n    PreCovid_occupancy = ifelse(PreCovid_total_rooms == 0, 0, (PreCovid_occupied_rooms / PreCovid_total_rooms) * 100),\n    PostCovid_occupancy = ifelse(PostCovid_total_rooms == 0, 0, (PostCovid_occupied_rooms / PostCovid_total_rooms) * 100)\n  )\n\nhead(occupancy_summary)\n\n# A tibble: 6 × 7\n  Province    PreCovid_occupied_ro…¹ PostCovid_occupied_r…² PreCovid_total_rooms\n  &lt;chr&gt;                        &lt;dbl&gt;                  &lt;dbl&gt;                &lt;dbl&gt;\n1 Amnat Char…                 106667                  91166              189198.\n2 Ang Thong                   208960                 116396              324038.\n3 Bangkok                   39621389               23666935            47956613.\n4 Bueng Kan                   362415                 465507              608851.\n5 Buriram                    1319062                1827050             2137721.\n6 Chachoengs…                 518580                 372576              917461.\n# ℹ abbreviated names: ¹​PreCovid_occupied_rooms, ²​PostCovid_occupied_rooms\n# ℹ 3 more variables: PostCovid_total_rooms &lt;dbl&gt;, PreCovid_occupancy &lt;dbl&gt;,\n#   PostCovid_occupancy &lt;dbl&gt;\n\n\n\nthaitourism_sf &lt;- left_join(thaitourism_sf, occupancy_summary, by = c(\"Province\"=\"Province\"))\n\nclass(thaitourism_sf)\n\n[1] \"sf\"         \"data.frame\"\n\n\nFinally, we add a column for the recovery of the occupancy rate using the code chunk below.\n\nthaitourism_sf &lt;- mutate(thaitourism_sf, Occupancy_recovery = ifelse(PreCovid_occupancy == 0, 0, (PostCovid_occupancy / PreCovid_occupancy)), .before = -1)\n\n\nclass(thaitourism_sf)\n\n[1] \"sf\"         \"data.frame\"\n\n\nFor the first visualization, let us plot the occupancy rate at a total level. The code below summarizes based on occupancy_df and computes a national occupancy rate to plot in a line chart.\n\naggregated_data &lt;&lt;- occupancy_df %&gt;%\n  group_by(MonthYear) %&gt;%\n  summarise(\n    occupancy_rate = sum(occupied_rooms, na.rm = TRUE) / sum(total_rooms, na.rm = TRUE) * 100\n  )\n\nggplot(aggregated_data, aes(x = MonthYear, y = occupancy_rate, group = 1)) +\n  geom_line() +\n  labs(\n    title = \"Occupancy Rate Over Time\",\n    x = \"MonthYear\",\n    y = \"Occupancy Rate (%)\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\n\n\n\n\n\n\n\nThe chart shows that occupancy rate has picked up after October 2021 and appears to have more or less reached pre-covid levels in the most recent months.\nWe can also produce a scatterplot to show pre- and post-covid occupancy rates at a province level. We do this using the code chunk below which produces an interactive plot.\n\nplot &lt;- plot_ly(\n  data = occupancy_summary,\n  x = ~PreCovid_occupancy,\n  y = ~PostCovid_occupancy,\n  type = 'scatter',\n  mode = 'markers',\n  text = ~paste('Province:', Province, '&lt;br&gt;PreCovid Occupancy:', round(PreCovid_occupancy), '&lt;br&gt;PostCovid Occupancy:', round(PostCovid_occupancy)),\n  hoverinfo = 'text'\n) %&gt;%\n  layout(\n    title = 'Monthly Pre- and Post-Covid Occupancy Rate (%)',\n    xaxis = list(title = 'PreCovid Occupancy'),\n    yaxis = list(title = 'PostCovid Occupancy')\n  )\n\n# Display the plot\nplot\n\n\n\n\n\nThe occupancy rates are showing a dispersed pattern as the overall rankings on occupancy rates have significantly changed pre- and post-covid. Bangkok, Chonburi and Suphan Buri reported the highest occupancy rates before covid, but Nan, Chang Rai and Nakhon Phanom have the highest rates post covid.\nThe next visualization for this measure is a side-by-side map for the pre- and post-covid occupancy rates as well as the recovery rate for occupancy. We use the code chunk below which uses tmap package to produce the maps.\n\ntm_shape(thaitourism_sf) +\n  tm_fill(col = c(\"PreCovid_occupancy\", \"PostCovid_occupancy\", \"Occupancy_recovery\"),\n          style = \"quantile\",\n          palette = list(\"viridis\", \"viridis\", \"RdYlGn\"),\n          title = c(\"Average Occupancy Rate\", \"Average Occupancy Rate\", \"Post vs Pre\")) +\n  tm_borders(col = \"black\") +\n  tm_layout(title = c(\"Pre Covid\", \"Post Covid\", \"Recovery Rate\"),\n            title.position = c(\"right\", \"top\"),\n            legend.position = c(\"right\", \"bottom\"),\n            bg.color = \"grey90\")\n\n\n\n\n\n\n\n\nBefore we leave this section, let us try to understand occupancy rate a bit more. Going back to earlier, we want to understand if high occupancy rate post-Covid is being driven by the number of available rooms. To help us answer this, we create a scatterplot of the available rooms against the occupancy rate post-Covid using the code chunk below.\n\nplot &lt;- plot_ly(\n  data = occupancy_summary,\n  x = ~PostCovid_total_rooms,\n  y = ~PostCovid_occupancy,\n  type = 'scatter',\n  mode = 'markers',\n  text = ~paste('Province:', Province, '&lt;br&gt;Number of Rooms:', round(PostCovid_total_rooms), '&lt;br&gt;Occupancy Rate:', round(PostCovid_occupancy)),\n  hoverinfo = 'text'\n) %&gt;%\n  layout(\n    title = 'Post-Covid Number of Rooms and Occupancy Rate (%)',\n    xaxis = list(title = 'Number of Rooms'),\n    yaxis = list(title = 'Occupancy Rate')\n  )\n\n# Display the plot\nplot\n\n\n\n\n\nBelow 10M rooms, there appears to be an upward trend in occupancy rate to the number of rooms. Provinces with more than 10M rooms go against the trend and appear to be capped to 60% occupancy."
  },
  {
    "objectID": "posts/thai-tourism-covid/index.html#d.1-variable-selection",
    "href": "posts/thai-tourism-covid/index.html#d.1-variable-selection",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "D.1 Variable Selection",
    "text": "D.1 Variable Selection\nWe will turn our attention to the first three measures discussed discussed in the previous section: tourism revenue, number of tourists and average tourist spending. We will not analyse the occupancy rate further as it is highly dependent on the number of rooms.\nWe will focus on checking signs of spatial autocorrelation or association before and after covid, as well as the recovery rate at the overall level for these three indicators– so we will be looking at 9 variables for our analysis."
  },
  {
    "objectID": "posts/thai-tourism-covid/index.html#d.2-deriving-the-contiguity-and-weight-matrix",
    "href": "posts/thai-tourism-covid/index.html#d.2-deriving-the-contiguity-and-weight-matrix",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "D.2 Deriving the Contiguity and Weight Matrix",
    "text": "D.2 Deriving the Contiguity and Weight Matrix\nFor the tests for this section, we need to derive a neighbor list as well as a weight matrix for each province to its neighbors. Given the presence of islands, we need to use distance rather than contiguity to define neighbors.\nThe first step is to understand the distribution of distances between nearest neighbors to find a proper cut-off distance. The code chunk below\n\nlongitude &lt;- map_dbl(thaitourism_sf$geometry, ~st_centroid(.x)[[1]])\nlatitude &lt;- map_dbl(thaitourism_sf$geometry, ~st_centroid(.x)[[2]])\ncoords &lt;- cbind(longitude, latitude)\n\nk1 &lt;- knn2nb(knearneigh(coords))\nk1dists &lt;- unlist(nbdists(k1, coords, longlat = TRUE))\nsummary(k1dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  21.55   51.92   64.33   63.24   76.77  110.94 \n\n\nThe maximum distance is 110.94 so setting a distance threshold of 111 should ensure that each province should have at least one neighbor. We then produce a nearest neighbor list for each province using dnearneigh()\n\nwm_d111 &lt;- dnearneigh(coords, 0, 111, longlat = TRUE)\nwm_d111\n\nNeighbour list object:\nNumber of regions: 77 \nNumber of nonzero links: 350 \nPercentage nonzero weights: 5.903188 \nAverage number of links: 4.545455 \n2 disjoint connected subgraphs\n\n\nWe import this as a new column in our sf object and compute for weights using st_weights() based on this.\n\nwm_thai &lt;- thaitourism_sf %&gt;%\n  mutate(nb = I(wm_d111),\n         wt = st_weights(nb,\n                         style=\"W\"),\n         .before=1)"
  },
  {
    "objectID": "posts/thai-tourism-covid/index.html#d.3-global-morans-i-test",
    "href": "posts/thai-tourism-covid/index.html#d.3-global-morans-i-test",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "D.3 Global Moran’s I Test",
    "text": "D.3 Global Moran’s I Test\nGlobal tests of spatial autocorrelation compares the value of each point/province to the overall value in order to conclude on spatial dependence. For this, we will focus on using Global Moran’s I which will work on the following general hypotheses:\n\n\\(H_0\\) - The value of (variable) is randomly distributed across provinces in Thailand\n\\(H_1\\) - The value of (variable) is not randomly distributed across provinces in Thailand\n\nFurther, the value of the test statistic \\(I\\) will also give indication on the underlying pattern:\n\n\\(I &gt; 0\\) - Clustering; observations tend to be similar\n\\(I &lt; 0\\) - Dispersed / regular; observations tend to be dissimilar\nWhere \\(I\\) close to zero - observations are arranged randomly\n\nTo perform Global Moran’s I test, with permutations, we use global_moran_perm() from sfdep package. We will use a 5% significance level for all the testing to be performed, and we will run 100 permutations / simulations for each test.\n\nD.3.1 Global Moran’s Test on Tourism Revenue\nThe code chunks in the tabs below run the Global Moran’s I permutation test on total tourism revenue (per month) pre-Covid, post-Covid and the recovery rate.\n\nPre-Covid RevenuePost-Covid RevenueRevenue Recovery Rate\n\n\n\nglobal_moran_perm(wm_thai$PreCovid_Revenue_total,\n                  wm_thai$nb,\n                  wm_thai$wt,\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.049413, observed rank = 93, p-value = 0.14\nalternative hypothesis: two.sided\n\n\n\n\n\nglobal_moran_perm(wm_thai$PostCovid_Revenue_total,\n                  wm_thai$nb,\n                  wm_thai$wt,\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.020361, observed rank = 84, p-value = 0.32\nalternative hypothesis: two.sided\n\n\n\n\n\nglobal_moran_perm(wm_thai$Revenue_total_recovery,\n                  wm_thai$nb,\n                  wm_thai$wt,\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.43763, observed rank = 100, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\n\n\n\nThe results show no evidence to reject spatial independence for the total revenue pre- and post-Covid. However, it shows signs of clustering for the revenue recovery rate as the p-value is below 0.05 and the statistic is above 1.\n\n\nD.3.2 Global Moran’s Test on Number of Tourists\nThe code chunks in the tabs below run the Global Moran’s I permutation test on total number of tourists (per month) pre-Covid, post-Covid and the recovery rate.\n\nPre-Covid Number of TouristsPost-Covid Number of TouristsNumber of Tourists Recovery Rate\n\n\n\nglobal_moran_perm(wm_thai$PreCovid_tourists_total,\n                  wm_thai$nb,\n                  wm_thai$wt,\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.062493, observed rank = 92, p-value = 0.16\nalternative hypothesis: two.sided\n\n\n\n\n\nglobal_moran_perm(wm_thai$PostCovid_tourists_total,\n                  wm_thai$nb,\n                  wm_thai$wt,\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.10696, observed rank = 95, p-value = 0.1\nalternative hypothesis: two.sided\n\n\n\n\n\nglobal_moran_perm(wm_thai$Tourists_total_recovery,\n                  wm_thai$nb,\n                  wm_thai$wt,\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.27768, observed rank = 100, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\n\n\n\nWe see similar results here. The results show no evidence to reject spatial independence for the total number of tourists pre- and post-Covid. However, it shows signs of clustering for the number of tourists recovery rate as the p-value is below 0.05 and the statistic is above 1.\n\n\nD.3.3 Global Moran’s Test on Average Tourist Spend\nThe code chunks in the tabs below run the Global Moran’s I permutation test on average tourist spend pre-Covid, post-Covid and the recovery rate.\n\nPre-Covid Average SpendPost-Covid Average SpendAverage Spend Recovery Rate\n\n\n\nglobal_moran_perm(wm_thai$PreCovidSpend_total,\n                  wm_thai$nb,\n                  wm_thai$wt,\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.423, observed rank = 100, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\n\n\n\nglobal_moran_perm(wm_thai$PostCovidSpend_total,\n                  wm_thai$nb,\n                  wm_thai$wt,\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.20651, observed rank = 100, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\n\n\n\nglobal_moran_perm(wm_thai$Spend_total_recovery,\n                  wm_thai$nb,\n                  wm_thai$wt,\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.31497, observed rank = 100, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\n\n\n\nAverage tourist spending is showing signs of clustering on all dimensions (pre-Covid, post-Covid and the recovery rate) as the p-value is below 0.05 and the I statistic is above 0 in all cases.\n\n\nD.3.3 Global Moran’s Test Summary\nBased on the results of the testing on the total revenue, number of tourists and spend, we see that the following variables are not exhibiting a random distribution, and show signs of clustering:\n\nTotal tourism revenue recovery rate\nTotal number of tourists recovery rate\nPre-Covid Average spend per Tourist\nPost-Covid Average spend per Tourist\nAverage spend per tourist recovery rate\n\nWe will conduct tests for local association to identify the clusters and outliers among provinces for each of these variables."
  },
  {
    "objectID": "posts/thai-tourism-covid/index.html#e.1-analysing-lisa-total-tourism-recovery-rate",
    "href": "posts/thai-tourism-covid/index.html#e.1-analysing-lisa-total-tourism-recovery-rate",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "E.1 Analysing LISA: Total tourism recovery rate",
    "text": "E.1 Analysing LISA: Total tourism recovery rate\nWe first compute for the LISA for the recovery rate of the total number of tourists using local_moran() function in the code chunk below. The code below uses 100 simulations to produce the test results.\n\nlisa &lt;- wm_thai %&gt;%\n  mutate(local_moran = local_moran(\n    Revenue_total_recovery, nb, wt, nsim = 99),\n    .before = 1) %&gt;%\n  unnest(local_moran)\n\nWe can then visualize the test statistic and p-values for each province in a map using tmap package in the code chunk below.\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_shape(lisa) +\n  tm_fill(c(\"ii\", \"p_ii_sim\"), title = c(\"Local Moran's I\",\"P Value\")) +\n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(\n    main.title = \"LISA for Total Revenue Recovery Rate\",\n    legend.position = c(\"right\", \"bottom\"),\n    bg.color = \"grey90\")\n\nVariable(s) \"ii\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\n\n\n\n\nOutliers are generally provinces where the test statistic is negative, and clusters where it is positive– if they are significant. We see some potential outliers. We can produce a different set of plots to allow us to identify these types of provinces.\nUsing a LISA map, we can show graphically the location of clusters and outliers based on this (tourism revenue recovery rate)\n\nlisa_sig &lt;- lisa %&gt;%\n  filter(p_ii_sim &lt; 0.05)\n\ntm_shape(lisa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig) +\n  tm_fill(\"mean\", title = \"LISA Class α=5% \") +\n  tm_borders(alpha = 0.4) +\n  tm_text(\"Province\", size = 0.5) +\ntm_layout(\n    main.title = \"Revenue Recovery Rate\",\n    legend.position = c(\"right\", \"bottom\"),\n    bg.color = \"beige\",\n    asp = 0)\n\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them).\n\n\n\n\n\n\n\n\n\nWe can also observe the contents of the object lisa_sig to see the statistics for the identified significant provinces. The code chunk below shows each class separately for easier reference.\n\nlisa_sig[lisa_sig$mean == \"Low-Low\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 7 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97.63083 ymin: 6.422716 xmax: 100.3366 ymax: 10.12626\nGeodetic CRS:  WGS 84\n# A tibble: 7 × 3\n  Province            mean                                              geometry\n  &lt;chr&gt;               &lt;fct&gt;                                   &lt;MULTIPOLYGON [°]&gt;\n1 Nakhon Si Thammarat Low-Low (((99.77467 9.313729, 99.77478 9.313647, 99.77488…\n2 Krabi               Low-Low (((99.11329 7.489274, 99.11337 7.489274, 99.11343…\n3 Phang Nga           Low-Low (((98.61471 7.74431, 98.61461 7.74431, 98.61437 7…\n4 Phuket              Low-Low (((98.31437 7.478515, 98.31425 7.478502, 98.31417…\n5 Surat Thani         Low-Low (((99.96396 9.309907, 99.96376 9.30955, 99.96353 …\n6 Satun               Low-Low (((100.0903 6.425736, 100.09 6.425543, 100.0896 6…\n7 Trang               Low-Low (((99.47579 6.97262, 99.47565 6.972616, 99.47537 …\n\nlisa_sig[lisa_sig$mean == \"High-High\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 2 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 98.984 ymin: 14.94191 xmax: 101.3582 ymax: 19.63808\nGeodetic CRS:  WGS 84\n# A tibble: 2 × 3\n  Province    mean                                                      geometry\n  &lt;chr&gt;       &lt;fct&gt;                                           &lt;MULTIPOLYGON [°]&gt;\n1 Nan         High-High (((100.8948 19.63432, 100.8952 19.63431, 100.8957 19.63…\n2 Uthai Thani High-High (((99.13905 15.79655, 99.13918 15.79652, 99.13965 15.79…\n\nlisa_sig[lisa_sig$mean == \"High-Low\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 0 features and 2 fields\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nGeodetic CRS:  WGS 84\n# A tibble: 0 × 3\n# ℹ 3 variables: Province &lt;chr&gt;, mean &lt;fct&gt;, geometry &lt;GEOMETRY [°]&gt;\n\nlisa_sig[lisa_sig$mean == \"Low-High\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 0 features and 2 fields\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nGeodetic CRS:  WGS 84\n# A tibble: 0 × 3\n# ℹ 3 variables: Province &lt;chr&gt;, mean &lt;fct&gt;, geometry &lt;GEOMETRY [°]&gt;\n\n\nWe can summarize the findings from this analysis as:\n\nThere is a cluster of 7 provinces at the south of Thailand that have slower recovery in terms of their average tourism revenue. This includes popular destinations like Phuket and Krabi and their neighboring provinces\nThere is a cluster of 3 provinces in the north that have faster recovery on the same metric. This includes Chiang Rai, Nan and Phayao\nThe analysis revealed two outliers. Chachoengsao has high recovery while neighboring provinces are low. Nakhon Ratchasima has low recovery while neighboring provinces are high\n\nreveal the following"
  },
  {
    "objectID": "posts/thai-tourism-covid/index.html#e.2-analysing-lisa-number-of-tourists-recovery-rate",
    "href": "posts/thai-tourism-covid/index.html#e.2-analysing-lisa-number-of-tourists-recovery-rate",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "E.2 Analysing LISA: Number of tourists recovery rate",
    "text": "E.2 Analysing LISA: Number of tourists recovery rate\nWe compute for the LISA using local_moran() function for the number of tourist recovery rate in the code chunk below.\n\nlisa &lt;- wm_thai %&gt;%\n  mutate(local_moran = local_moran(\n    Tourists_total_recovery, nb, wt, nsim = 99),\n    .before = 1) %&gt;%\n  unnest(local_moran)\n\nWe will go straight to producing the LISA map based on the lisa object. Again, we use 0.05 to determine statistical significance.\n\nlisa_sig &lt;- lisa %&gt;%\n  filter(p_ii_sim &lt; 0.05)\n\ntm_shape(lisa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig) +\n  tm_fill(\"mean\", title = \"LISA Class α=5% \") +\n  tm_borders(alpha = 0.4) +\n  tm_text(\"Province\", size = 0.5) +\ntm_layout(\n    main.title = \"No of Tourist Recovery Rate\",\n    legend.position = c(\"right\", \"bottom\"),\n    bg.color = \"beige\",\n    asp = 0)\n\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them).\n\n\n\n\n\n\n\n\n\nTo aid interpretation, we display results tabularly as before using the code chunk below.\n\nlisa_sig[lisa_sig$mean == \"Low-Low\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 5 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97.63083 ymin: 7.467277 xmax: 100.3366 ymax: 10.12626\nGeodetic CRS:  WGS 84\n# A tibble: 5 × 3\n  Province            mean                                              geometry\n  &lt;chr&gt;               &lt;fct&gt;                                   &lt;MULTIPOLYGON [°]&gt;\n1 Nakhon Si Thammarat Low-Low (((99.77467 9.313729, 99.77478 9.313647, 99.77488…\n2 Krabi               Low-Low (((99.11329 7.489274, 99.11337 7.489274, 99.11343…\n3 Phang Nga           Low-Low (((98.61471 7.74431, 98.61461 7.74431, 98.61437 7…\n4 Phuket              Low-Low (((98.31437 7.478515, 98.31425 7.478502, 98.31417…\n5 Surat Thani         Low-Low (((99.96396 9.309907, 99.96376 9.30955, 99.96353 …\n\nlisa_sig[lisa_sig$mean == \"High-High\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 0 features and 2 fields\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nGeodetic CRS:  WGS 84\n# A tibble: 0 × 3\n# ℹ 3 variables: Province &lt;chr&gt;, mean &lt;fct&gt;, geometry &lt;GEOMETRY [°]&gt;\n\nlisa_sig[lisa_sig$mean == \"High-Low\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 2 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 99.15364 ymin: 6.422716 xmax: 101.9901 ymax: 13.9767\nGeodetic CRS:  WGS 84\n# A tibble: 2 × 3\n  Province     mean                                                     geometry\n  &lt;chr&gt;        &lt;fct&gt;                                          &lt;MULTIPOLYGON [°]&gt;\n1 Chachoengsao High-Low (((101.0612 13.97613, 101.0625 13.976, 101.0629 13.9760…\n2 Satun        High-Low (((100.0903 6.425736, 100.09 6.425543, 100.0896 6.42572…\n\nlisa_sig[lisa_sig$mean == \"Low-High\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 0 features and 2 fields\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nGeodetic CRS:  WGS 84\n# A tibble: 0 × 3\n# ℹ 3 variables: Province &lt;chr&gt;, mean &lt;fct&gt;, geometry &lt;GEOMETRY [°]&gt;\n\n\nThe findings can be summarized as:\n\nThere is a cluster at the south of five provinces with slower recovery with regards to the number of tourists– including Phuket and Krabi (similar to previous metric)\nSatun at the southern part has high recovery rate while its neighboring provinces are lower"
  },
  {
    "objectID": "posts/thai-tourism-covid/index.html#e.3-analysing-lisa-pre-covid-average-spend",
    "href": "posts/thai-tourism-covid/index.html#e.3-analysing-lisa-pre-covid-average-spend",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "E.3 Analysing LISA: Pre-Covid Average Spend",
    "text": "E.3 Analysing LISA: Pre-Covid Average Spend\nWe compute for the LISA using local_moran() function for the average tourist spending post-Covid in the code chunk below.\n\nlisa &lt;- wm_thai %&gt;%\n  mutate(local_moran = local_moran(\n    PreCovidSpend_total, nb, wt, nsim = 99),\n    .before = 1) %&gt;%\n  unnest(local_moran)\n\nWe will now produce the LISA map based on the generated lisa object. Again, we use 0.05 to determine statistical significance.\n\nlisa_sig &lt;- lisa %&gt;%\n  filter(p_ii_sim &lt; 0.05)\n\ntm_shape(lisa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig) +\n  tm_fill(\"mean\", title = \"LISA Class α=5% \") +\n  tm_borders(alpha = 0.4) +\n  tm_text(\"Province\", size = 0.5) +\ntm_layout(\n    main.title = \"Pre-Covid Average Tourist Spend\",\n    legend.position = c(\"right\", \"bottom\"),\n    bg.color = \"beige\",\n    asp = 0)\n\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them).\n\n\n\n\n\n\n\n\n\nWe display the results tabularly using the code chunk below to make it easier to interpret.\n\nlisa_sig[lisa_sig$mean == \"Low-Low\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 10 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 99.08612 ymin: 14.06424 xmax: 105.637 ymax: 18.22037\nGeodetic CRS:  WGS 84\n# A tibble: 10 × 3\n   Province         mean                                                geometry\n   &lt;chr&gt;            &lt;fct&gt;                                     &lt;MULTIPOLYGON [°]&gt;\n 1 Lopburi          Low-Low (((101.3453 15.75254, 101.3457 15.75224, 101.3466 1…\n 2 Sing Buri        Low-Low (((100.3691 15.0894, 100.3697 15.0891, 100.3708 15.…\n 3 Chainat          Low-Low (((100.1199 15.41243, 100.121 15.41234, 100.1229 15…\n 4 Ubon Ratchathani Low-Low (((105.0633 16.09675, 105.0634 16.09671, 105.0638 1…\n 5 Yasothon         Low-Low (((104.3952 16.34843, 104.3983 16.34707, 104.4 16.3…\n 6 Khon Kaen        Low-Low (((102.7072 17.08713, 102.708 17.087, 102.7096 17.0…\n 7 Loei             Low-Low (((102.095 18.21708, 102.0962 18.21675, 102.0971 18…\n 8 Roi Et           Low-Low (((104.314 16.43758, 104.3135 16.43452, 104.3137 16…\n 9 Nakhon Sawan     Low-Low (((100.0266 16.189, 100.0267 16.18889, 100.0268 16.…\n10 Suphan Buri      Low-Low (((99.37118 15.05073, 99.37454 15.0495, 99.3762 15.…\n\nlisa_sig[lisa_sig$mean == \"High-High\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 3 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97.63083 ymin: 7.467277 xmax: 99.41499 ymax: 9.478956\nGeodetic CRS:  WGS 84\n# A tibble: 3 × 3\n  Province  mean                                                        geometry\n  &lt;chr&gt;     &lt;fct&gt;                                             &lt;MULTIPOLYGON [°]&gt;\n1 Krabi     High-High (((99.11329 7.489274, 99.11337 7.489274, 99.11343 7.48929…\n2 Phang Nga High-High (((98.61471 7.74431, 98.61461 7.74431, 98.61437 7.744467,…\n3 Phuket    High-High (((98.31437 7.478515, 98.31425 7.478502, 98.31417 7.47851…\n\nlisa_sig[lisa_sig$mean == \"High-Low\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 0 features and 2 fields\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nGeodetic CRS:  WGS 84\n# A tibble: 0 × 3\n# ℹ 3 variables: Province &lt;chr&gt;, mean &lt;fct&gt;, geometry &lt;GEOMETRY [°]&gt;\n\nlisa_sig[lisa_sig$mean == \"Low-High\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 0 features and 2 fields\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nGeodetic CRS:  WGS 84\n# A tibble: 0 × 3\n# ℹ 3 variables: Province &lt;chr&gt;, mean &lt;fct&gt;, geometry &lt;GEOMETRY [°]&gt;\n\n\nThe findings can be summarized as:\n\nThere are multiple clusters, totaling 10 provinces, in the center of Thailand that have low average spending pre-Covid. These are composed of lesser known tourist destinations\nPhuket and Phang Nga make up a two-province cluster with high average tourist spending\nThere are no outliers identified in the analysis"
  },
  {
    "objectID": "posts/thai-tourism-covid/index.html#e.4-analysing-lisa-post-covid-average-spend",
    "href": "posts/thai-tourism-covid/index.html#e.4-analysing-lisa-post-covid-average-spend",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "E.4 Analysing LISA: Post-Covid Average Spend",
    "text": "E.4 Analysing LISA: Post-Covid Average Spend\nWe compute for the LISA using local_moran() function for the average tourist spending post-Covid in the code chunk below.\n\nlisa &lt;- wm_thai %&gt;%\n  mutate(local_moran = local_moran(\n    PostCovidSpend_total, nb, wt, nsim = 99),\n    .before = 1) %&gt;%\n  unnest(local_moran)\n\nWe will now produce the LISA map based on the generated lisa object. Again, we use 0.05 to determine statistical significance.\n\nlisa_sig &lt;- lisa %&gt;%\n  filter(p_ii_sim &lt; 0.05)\n\ntm_shape(lisa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig) +\n  tm_fill(\"mean\", title = \"LISA Class α=5% \") +\n  tm_borders(alpha = 0.4) +\n  tm_text(\"Province\", size = 0.5) +\ntm_layout(\n    main.title = \"Post-Covid Average Tourist Spend\",\n    legend.position = c(\"right\", \"bottom\"),\n    bg.color = \"beige\",\n    asp = 0)\n\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them).\n\n\n\n\n\n\n\n\n\nWe again display the results tabularly using the code chunk below to make it easier to interpret.\n\nlisa_sig[lisa_sig$mean == \"Low-Low\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 7 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 99.08612 ymin: 14.06424 xmax: 105.637 ymax: 18.22037\nGeodetic CRS:  WGS 84\n# A tibble: 7 × 3\n  Province         mean                                                 geometry\n  &lt;chr&gt;            &lt;fct&gt;                                      &lt;MULTIPOLYGON [°]&gt;\n1 Sing Buri        Low-Low (((100.3691 15.0894, 100.3697 15.0891, 100.3708 15.0…\n2 Ubon Ratchathani Low-Low (((105.0633 16.09675, 105.0634 16.09671, 105.0638 16…\n3 Loei             Low-Low (((102.095 18.21708, 102.0962 18.21675, 102.0971 18.…\n4 Roi Et           Low-Low (((104.314 16.43758, 104.3135 16.43452, 104.3137 16.…\n5 Mukdahan         Low-Low (((104.2527 16.89302, 104.2527 16.89274, 104.2527 16…\n6 Nakhon Sawan     Low-Low (((100.0266 16.189, 100.0267 16.18889, 100.0268 16.1…\n7 Suphan Buri      Low-Low (((99.37118 15.05073, 99.37454 15.0495, 99.3762 15.0…\n\nlisa_sig[lisa_sig$mean == \"High-High\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 3 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97.63083 ymin: 7.467277 xmax: 99.41499 ymax: 9.478956\nGeodetic CRS:  WGS 84\n# A tibble: 3 × 3\n  Province  mean                                                        geometry\n  &lt;chr&gt;     &lt;fct&gt;                                             &lt;MULTIPOLYGON [°]&gt;\n1 Krabi     High-High (((99.11329 7.489274, 99.11337 7.489274, 99.11343 7.48929…\n2 Phang Nga High-High (((98.61471 7.74431, 98.61461 7.74431, 98.61437 7.744467,…\n3 Phuket    High-High (((98.31437 7.478515, 98.31425 7.478502, 98.31417 7.47851…\n\nlisa_sig[lisa_sig$mean == \"High-Low\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 0 features and 2 fields\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nGeodetic CRS:  WGS 84\n# A tibble: 0 × 3\n# ℹ 3 variables: Province &lt;chr&gt;, mean &lt;fct&gt;, geometry &lt;GEOMETRY [°]&gt;\n\nlisa_sig[lisa_sig$mean == \"Low-High\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 0 features and 2 fields\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nGeodetic CRS:  WGS 84\n# A tibble: 0 × 3\n# ℹ 3 variables: Province &lt;chr&gt;, mean &lt;fct&gt;, geometry &lt;GEOMETRY [°]&gt;\n\n\nThe results very closely resember the ones for Pre-Covid average spending. One significant change is that the high spend cluster at the south now includes Krabi. (so it now consists of three provinces)"
  },
  {
    "objectID": "posts/thai-tourism-covid/index.html#e.5-analysing-lisa-average-spend-recovery-rate",
    "href": "posts/thai-tourism-covid/index.html#e.5-analysing-lisa-average-spend-recovery-rate",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "E.5 Analysing LISA: Average Spend Recovery Rate",
    "text": "E.5 Analysing LISA: Average Spend Recovery Rate\nWe compute for the LISA using local_moran() function for the average tourist spending recovery rate in the code chunk below.\n\nlisa &lt;- wm_thai %&gt;%\n  mutate(local_moran = local_moran(\n    Spend_total_recovery, nb, wt, nsim = 99),\n    .before = 1) %&gt;%\n  unnest(local_moran)\n\nWe will now produce the LISA map based on the generated lisa object. Again, we use 0.05 to determine statistical significance.\n\nlisa_sig &lt;- lisa %&gt;%\n  filter(p_ii_sim &lt; 0.05)\n\ntm_shape(lisa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig) +\n  tm_fill(\"mean\", title = \"LISA Class α=5% \") +\n  tm_borders(alpha = 0.4) +\n  tm_text(\"Province\", size = 0.5) +\ntm_layout(\n    main.title = \"Average Tourist Spend Recovery Rate\",\n    legend.position = c(\"right\", \"bottom\"),\n    bg.color = \"beige\",\n    asp = 0)\n\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them).\n\n\n\n\n\n\n\n\n\nWe again display the results tabularly using the code chunk below to make it easier to interpret.\n\nlisa_sig[lisa_sig$mean == \"Low-Low\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 6 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 98.32594 ymin: 5.613038 xmax: 101.7248 ymax: 10.78906\nGeodetic CRS:  WGS 84\n# A tibble: 6 × 3\n  Province            mean                                              geometry\n  &lt;chr&gt;               &lt;fct&gt;                                   &lt;MULTIPOLYGON [°]&gt;\n1 Nakhon Si Thammarat Low-Low (((99.77467 9.313729, 99.77478 9.313647, 99.77488…\n2 Surat Thani         Low-Low (((99.96396 9.309907, 99.96376 9.30955, 99.96353 …\n3 Ranong              Low-Low (((98.35294 9.440758, 98.35316 9.440558, 98.3533 …\n4 Trang               Low-Low (((99.47579 6.97262, 99.47565 6.972616, 99.47537 …\n5 Pattani             Low-Low (((101.2827 6.952051, 101.2839 6.95182, 101.2848 …\n6 Yala                Low-Low (((101.2927 6.681118, 101.2937 6.679529, 101.2939…\n\nlisa_sig[lisa_sig$mean == \"High-High\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 2 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 99.01629 ymin: 15.3183 xmax: 101.7972 ymax: 17.178\nGeodetic CRS:  WGS 84\n# A tibble: 2 × 3\n  Province       mean                                                   geometry\n  &lt;chr&gt;          &lt;fct&gt;                                        &lt;MULTIPOLYGON [°]&gt;\n1 Kamphaeng Phet High-High (((99.48875 16.91044, 99.48883 16.91016, 99.48884 16…\n2 Phetchabun     High-High (((101.3987 17.17792, 101.399 17.17781, 101.3993 17.…\n\nlisa_sig[lisa_sig$mean == \"High-Low\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 1 feature and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 98.25791 ymin: 7.478502 xmax: 98.48333 ymax: 8.200333\nGeodetic CRS:  WGS 84\n# A tibble: 1 × 3\n  Province mean                                                         geometry\n  &lt;chr&gt;    &lt;fct&gt;                                              &lt;MULTIPOLYGON [°]&gt;\n1 Phuket   High-Low (((98.31437 7.478515, 98.31425 7.478502, 98.31417 7.478513,…\n\nlisa_sig[lisa_sig$mean == \"Low-High\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 1 feature and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 100.42 ymin: 14.64684 xmax: 101.4044 ymax: 15.75613\nGeodetic CRS:  WGS 84\n# A tibble: 1 × 3\n  Province mean                                                         geometry\n  &lt;chr&gt;    &lt;fct&gt;                                              &lt;MULTIPOLYGON [°]&gt;\n1 Lopburi  Low-High (((101.3453 15.75254, 101.3457 15.75224, 101.3466 15.75236,…\n\n\nThe findings can be summarized as:\n\nThere are two clusters at the south, totaling 5 provinces, that have slow recovery. This includes the provinces Surat Thani, Nakhon Si Thammarat, Trang, Pattani and Yala\nThere is a cluster of three provinces at the center that have high recovery rate. This includes Kanpaeng Phet, Phichit and Phetchabun\nPhuket is appearing as an outlier with high tourist spend recovery rate relative to its neighbors."
  },
  {
    "objectID": "posts/thai-tourism-covid/index.html#f.1-ehsa---post-covid-tourism-revenue",
    "href": "posts/thai-tourism-covid/index.html#f.1-ehsa---post-covid-tourism-revenue",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "F.1 EHSA - Post-covid tourism revenue",
    "text": "F.1 EHSA - Post-covid tourism revenue\nWe perform EHSA on the post-covid (2022 onwards) values for the total tourism revenue.\nWe create post-Covid versions of our datasets using the code chunk below so it is easier to refer to them later. We also include a column for the year and month as integers as EHSA requires discrete numeric values as a time variable.\n\ntourism_postCov &lt;- subset(tourism, Date &gt; as.Date(\"2021-12-31\"))\ntourism_postCov$YYYYMM &lt;- as.integer(format(tourism_postCov$Date, \"%Y%m\"))\n\noccupancy_postCov &lt;- subset(occupancy_df, Date &gt; as.Date(\"2021-12-31\"))\noccupancy_postCov$YYYYMM &lt;- as.integer(format(occupancy_postCov$Date, \"%Y%m\"))\n\n\nF.1.1 Creating spacetime cube\nWe then create the space-time cube using spacetime() for just the post-covid tourism revenue. We use filter() to only select the rows for the measure of interest, and then select() to limit the original dataframe to only the date, province and variable.\n\nstcube &lt;- spacetime(select(filter(tourism_postCov, tourism_postCov$Indicator == \"revenue_all\"),\n                           YYYYMM, Province, Value),\n                    thai_sf,\n                    .loc_col = \"Province\",\n                    .time_col = \"YYYYMM\")\n\nTo check that the object is in the correct format, we can use is_spacetime_cube() and confirm that it returns TRUE\n\nis_spacetime_cube(stcube)\n\n[1] TRUE\n\n\n\n\nF.1.2 Calculating \\(G_i^*\\) statistics\nBefore computing for the \\(G_i^*\\) statistics, we need to derive the spatial weights. We use the code chunk below to identify the neighbors and to derive inverse distance weights. The alpha argument of st_inverse_distance() determines the level of distance decay.\n\nstcube_nb &lt;- stcube %&gt;%\n  activate(\"geometry\") %&gt;%\n  mutate(nb = include_self(\n    st_contiguity(geometry)),\n    wt=st_inverse_distance(nb,\n                  geometry,\n                  scale = 1,\n                  alpha = 1),\n    .before = 1) %&gt;%\n  set_nbs(\"nb\") %&gt;%\n  set_wts(\"wt\")\n\n! Polygon provided. Using point on surface.\n\n\nWarning: There was 1 warning in `stopifnot()`.\nℹ In argument: `wt = st_inverse_distance(nb, geometry, scale = 1, alpha = 1)`.\nCaused by warning in `st_point_on_surface.sfc()`:\n! st_point_on_surface may not give correct results for longitude/latitude data\n\n\nWe then use the following chunk to calculate the local \\(G_i^*\\) for each location. We do this using local_gstar_perm() of sfdep package. We then use unnest() to unnest the gi_star column of the newly created data frame.\n\ngi_stars &lt;- stcube_nb %&gt;%\n  group_by(YYYYMM) %&gt;%\n  mutate(gi_star = local_gstar_perm(\n    Value, nb, wt)) %&gt;%\n  tidyr::unnest(gi_star)\n\n\n\nF.1.3 Identifying Hot- and Cold-spots using Mann Kendall test\nThe Mann Kendall test checks for signs of monotonicity for a the local \\(G_i^*\\) statistic. Where the results are significant, the test infers that there are signs of monotonicity for that province / observation.\nThe code chunk below runs the Mann Kendall test (without permutations) on each province for the selected variable using MannKendall(). Provinces where the tau value is positive are showing an increasing trend, while negative tau values show decreasing trend.\nWe display the records with significant test results, and show negative and positive tau values as separate tables.\n\nehsa &lt;- gi_stars %&gt;%\n  group_by(Province) %&gt;%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %&gt;%\n  tidyr::unnest_wider(mk)\n\nfilter(ehsa, sl &lt; 0.05, tau &gt; 0)\n\n# A tibble: 33 × 6\n   Province         tau         sl     S     D  varS\n   &lt;chr&gt;          &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Amnat Charoen  0.956 0.00000250    87  91.0  334.\n 2 Bangkok        0.473 0.0215        43  91.0  334.\n 3 Bueng Kan      0.626 0.00217       57  91.0  334.\n 4 Buriram        0.758 0.000197      69  91.0  334.\n 5 Chaiyaphum     0.780 0.000127      71  91.0  334.\n 6 Chanthaburi    0.626 0.00217       57  91.0  334.\n 7 Kalasin        0.802 0.0000809     73  91.0  334.\n 8 Kamphaeng Phet 0.890 0.0000119     81  91.0  334.\n 9 Lampang        0.626 0.00217       57  91.0  334.\n10 Lamphun        0.956 0.00000250    87  91.0  334.\n# ℹ 23 more rows\n\nfilter(ehsa, sl &lt; 0.05, tau &lt; 0)\n\n# A tibble: 21 × 6\n   Province        tau        sl     S     D  varS\n   &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Ang Thong    -0.582 0.00442     -53  91.0  334.\n 2 Chachoengsao -0.802 0.0000809   -73  91.0  334.\n 3 Chainat      -0.626 0.00217     -57  91.0  334.\n 4 Chiang Rai   -0.429 0.0375      -39  91.0  334.\n 5 Chumphon     -0.890 0.0000119   -81  91.0  334.\n 6 Kanchanaburi -0.890 0.0000119   -81  91.0  334.\n 7 Mae Hong Son -0.429 0.0375      -39  91.0  334.\n 8 Mukdahan     -0.473 0.0215      -43  91.0  334.\n 9 Phetchabun   -0.824 0.0000510   -75  91.0  334.\n10 Phetchaburi  -0.802 0.0000809   -73  91.0  334.\n# ℹ 11 more rows\n\n\nThe results show that 33 of the 77 provinces are showing significant positive trend– which might be expected as provinces are recovering post-Covid. However, there are 21 provinces which are showing a significant negative trend which might be a concern if any of these are expected to be major tourist destinations.\n\n\nF.1.4 Emerging Hot Spot Analysis\nWe can use emerging_hot_spot_analysis() to perform EHSA, including the previous hypothesis testing, and classify provinces based on the test results.\nThe code chunk below uses emerging_hot_spot_analysis() directly on the spacetime cube to run the test with 100 permutations.\n\nehsa &lt;- emerging_hotspot_analysis(\n  x = stcube,\n  .var = \"Value\",\n  k = 1,\n  nsim = 99\n)\n\nWe can visualize the number of provinces per category using the code chunk below which utilizes ggplot() to build a bar chart.\n\nggplot(data = ehsa,\n       aes(x = classification)) +\n  geom_bar(fill = \"lightblue\") +\n  labs(title = \"EHSA Classification - PostCovid Tourism Revenue\",\n       y = \"Number of Provinces\", x=\"\") +\n  theme_minimal() +\n  geom_text(stat = 'count', aes(label = ..count..), vjust = 0) +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(count)` instead.\n\n\n\n\n\n\n\n\n\nWe can also visualize the hot- and coldspot classifications visually. First, we need to combine the ehsa object with the sf dataframe with the code chunk below.\n\nthai_ehsa &lt;- thai_sf %&gt;%\n  left_join(ehsa,\n            by = join_by(Province == location))\n\nNext, we use tmap package to create a choropleth map based on the EHSA classification. We only include provinces that have significant test results and a pattern detected.\n\nehsa_sig &lt;- filter(thai_ehsa, !(classification == \"no pattern detected\"), p_value &lt; 0.05)\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_shape(thai_ehsa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(ehsa_sig) +\n  tm_fill(\"classification\", title = \"EHSA Classification\") +\n  tm_text(\"Province\", size = 0.5) +\n  tm_borders(alpha = 0.4) +\ntm_layout(\n    main.title = \"Post Covid Tourism Revenue\",\n    legend.position = c(\"right\", \"bottom\"),\n    bg.color = \"beige\",\n    asp = 0)\n\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them).\n\n\n\n\n\n\n\n\n\nWe can display the same provinces tabularly using the code chunk below\n\nfor (i in unique(ehsa_sig$classification)){\nprint(as.matrix(ehsa_sig[ehsa_sig$classification == i, c(\"Province\", \"classification\")]))\n}\n\n  Province  classification         geometry                      \n1 \"Bangkok\" \"intensifying hotspot\" MULTIPOLYGON (((100.6139 13...\n   Province        classification        geometry                      \n2  \"Nonthaburi\"    \"consecutive hotspot\" MULTIPOLYGON (((100.3415 14...\n8  \"Khon Kaen\"     \"consecutive hotspot\" MULTIPOLYGON (((102.7072 17...\n10 \"Sakon Nakhon\"  \"consecutive hotspot\" MULTIPOLYGON (((103.5404 18...\n11 \"Nakhon Phanom\" \"consecutive hotspot\" MULTIPOLYGON (((104.192 18....\n15 \"Kanchanaburi\"  \"consecutive hotspot\" MULTIPOLYGON (((98.58631 15...\n   Province                   classification     \n3  \"Phra Nakhon Si Ayutthaya\" \"sporadic coldspot\"\n4  \"Sing Buri\"                \"sporadic coldspot\"\n5  \"Trat\"                     \"sporadic coldspot\"\n6  \"Chachoengsao\"             \"sporadic coldspot\"\n7  \"Buri Ram\"                 \"sporadic coldspot\"\n9  \"Nong Khai\"                \"sporadic coldspot\"\n12 \"Mukdahan\"                 \"sporadic coldspot\"\n13 \"Lamphun\"                  \"sporadic coldspot\"\n14 \"Lampang\"                  \"sporadic coldspot\"\n16 \"Prachuap Khiri Khan\"      \"sporadic coldspot\"\n17 \"Ranong\"                   \"sporadic coldspot\"\n18 \"Phatthalung\"              \"sporadic coldspot\"\n19 \"Pattani\"                  \"sporadic coldspot\"\n   geometry                      \n3  MULTIPOLYGON (((100.5131 14...\n4  MULTIPOLYGON (((100.3691 15...\n5  MULTIPOLYGON (((102.5216 11...\n6  MULTIPOLYGON (((101.0612 13...\n7  MULTIPOLYGON (((102.9303 15...\n9  MULTIPOLYGON (((103.2985 18...\n12 MULTIPOLYGON (((104.2527 16...\n13 MULTIPOLYGON (((99.18821 18...\n14 MULTIPOLYGON (((99.58445 19...\n16 MULTIPOLYGON (((99.56326 11...\n17 MULTIPOLYGON (((98.35294 9....\n18 MULTIPOLYGON (((99.96416 7....\n19 MULTIPOLYGON (((101.2827 6....\n\n\nThe results identifies Bangkok as an intensifying hotspot, and six others as consecutive hotspots.\nThere are 13 provinces identified as sporadic coldspots which are locations that are cold spots for less than 90% of the time, but never identified as significant hotspots."
  },
  {
    "objectID": "posts/thai-tourism-covid/index.html#f.2-ehsa---post-covid-number-of-tourists",
    "href": "posts/thai-tourism-covid/index.html#f.2-ehsa---post-covid-number-of-tourists",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "F.2 EHSA - Post-covid number of tourists",
    "text": "F.2 EHSA - Post-covid number of tourists\nWe perform EHSA on the post-covid (2022 onwards) values for the total number of tourists.\n\nF.2.1 Creating spacetime cube\nWe then create the space-time cube using spacetime() for just the post-covid number of tourists. We use filter() to only select the rows for the measure of interest, and then select() to limit the original dataframe to only the date, province and variable.\n\nstcube &lt;- spacetime(select(filter(tourism_postCov, tourism_postCov$Indicator == \"no_tourist_all\"),\n                           YYYYMM, Province, Value),\n                    thai_sf,\n                    .loc_col = \"Province\",\n                    .time_col = \"YYYYMM\")\n\nTo check that the object is in the correct format, we can use is_spacetime_cube() and confirm that it returns TRUE\n\nis_spacetime_cube(stcube)\n\n[1] TRUE\n\n\n\n\nF.2.2 Calculating \\(G_i^*\\) statistics\nBefore computing for the \\(G_i^*\\) statistics, we need to derive the spatial weights. We use the code chunk below to identify the neighbors and to derive inverse distance weights. The alpha argument of st_inverse_distance() determines the level of distance decay.\n\nstcube_nb &lt;- stcube %&gt;%\n  activate(\"geometry\") %&gt;%\n  mutate(nb = include_self(\n    st_contiguity(geometry)),\n    wt=st_inverse_distance(nb,\n                  geometry,\n                  scale = 1,\n                  alpha = 1),\n    .before = 1) %&gt;%\n  set_nbs(\"nb\") %&gt;%\n  set_wts(\"wt\")\n\nWe then use the following chunk to calculate the local \\(G_i^*\\) for each location. We do this using local_gstar_perm() of sfdep package. We then use unnest() to unnest the gi_star column of the newly created data frame.\n\ngi_stars &lt;- stcube_nb %&gt;%\n  group_by(YYYYMM) %&gt;%\n  mutate(gi_star = local_gstar_perm(\n    Value, nb, wt)) %&gt;%\n  tidyr::unnest(gi_star)\n\n\n\nF.2.3 Identifying Hot- and Cold-spots using Mann Kendall test\nThe code chunk below runs the Mann Kendall test (without permutations) on each province using MannKendall(). Provinces where the tau value is positive are showing an increasing trend, while negative tau values show decreasing trend.\nWe display the records with significant test results, and show negative and positive tau values as separate tables.\n\nehsa &lt;- gi_stars %&gt;%\n  group_by(Province) %&gt;%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %&gt;%\n  tidyr::unnest_wider(mk)\n\nfilter(ehsa, sl &lt; 0.05, tau &gt; 0)\n\n# A tibble: 14 × 6\n   Province                   tau        sl     S     D  varS\n   &lt;chr&gt;                    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Bangkok                  0.648 0.00150      59  91.0  334.\n 2 Krabi                    0.714 0.000459     65  91.0  334.\n 3 Nakhon Si Thammarat      0.890 0.0000119    81  91.0  334.\n 4 Nonthaburi               0.626 0.00217      57  91.0  334.\n 5 Pattani                  0.648 0.00150      59  91.0  334.\n 6 Phatthalung              0.780 0.000127     71  91.0  334.\n 7 Phra Nakhon Si Ayutthaya 0.604 0.00311      55  91.0  334.\n 8 Phuket                   0.670 0.00102      61  91.0  334.\n 9 Prachuap Khiri Khan      0.648 0.00150      59  91.0  334.\n10 Ratchaburi               0.429 0.0375       39  91.0  334.\n11 Samut Prakan             0.473 0.0215       43  91.0  334.\n12 Surat Thani              0.648 0.00150      59  91.0  334.\n13 Tak                      0.692 0.000688     63  91.0  334.\n14 Trang                    0.451 0.0285       41  91.0  334.\n\nfilter(ehsa, sl &lt; 0.05, tau &lt; 0)\n\n# A tibble: 18 × 6\n   Province            tau        sl     S     D  varS\n   &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Chachoengsao     -0.758 0.000197    -69  91.0  334.\n 2 Kanchanaburi     -0.692 0.000688    -63  91.0  334.\n 3 Khon Kaen        -0.604 0.00311     -55  91.0  334.\n 4 Loei             -0.516 0.0118      -47  91.0  334.\n 5 Mukdahan         -0.648 0.00150     -59  91.0  334.\n 6 Nakhon Nayok     -0.824 0.0000510   -75  91.0  334.\n 7 Nakhon Pathom    -0.495 0.0160      -45  91.0  334.\n 8 Nong Khai        -0.648 0.00150     -59  91.0  334.\n 9 Phetchabun       -0.560 0.00620     -51  91.0  334.\n10 Phetchaburi      -0.780 0.000127    -71  91.0  334.\n11 Samut Sakhon     -0.802 0.0000809   -73  91.0  334.\n12 Samut Songkhram  -0.780 0.000127    -71  91.0  334.\n13 Sing Buri        -0.626 0.00217     -57  91.0  334.\n14 Surin            -0.560 0.00620     -51  91.0  334.\n15 Trat             -0.780 0.000127    -71  91.0  334.\n16 Ubon Ratchathani -0.495 0.0160      -45  91.0  334.\n17 Udon Thani       -0.407 0.0487      -37  91.0  334.\n18 Yasothon         -0.758 0.000197    -69  91.0  334.\n\n\nThe results show that 14 of the 77 provinces are showing significant positive trend. However, there are 18 provinces (more) which are showing a significant negative trend.\n\n\nF.2.4 Emerging Hot Spot Analysis\nWe can use emerging_hot_spot_analysis() to perform EHSA, including the previous hypothesis testing, and classify provinces based on the test results.\nThe code chunk below uses emerging_hot_spot_analysis() directly on the spacetime cube to run the test with 100 permutations.\n\nehsa &lt;- emerging_hotspot_analysis(\n  x = stcube,\n  .var = \"Value\",\n  k = 1,\n  nsim = 99\n)\n\nWe can visualize the number of provinces per category using the code chunk below which utilizes ggplot() to build a bar chart.\n\nggplot(data = ehsa,\n       aes(x = classification)) +\n  geom_bar(fill = \"lightblue\") +\n  labs(title = \"EHSA Classification - PostCovid Tourism Revenue\",\n       y = \"Number of Provinces\", x=\"\") +\n  theme_minimal() +\n  geom_text(stat = 'count', aes(label = ..count..), vjust = 0) +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))\n\n\n\n\n\n\n\n\nWe can also visualize the hot- and coldspot classifications visually. First, we need to combine the ehsa object with the sf dataframe with the code chunk below.\n\nthai_ehsa &lt;- thai_sf %&gt;%\n  left_join(ehsa,\n            by = join_by(Province == location))\n\nNext, we use tmap package to create a choropleth map based on the EHSA classification. We only include provinces that have significant test results and a pattern detected.\n\nehsa_sig &lt;- filter(thai_ehsa, !(classification == \"no pattern detected\"), p_value &lt; 0.05)\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_shape(thai_ehsa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(ehsa_sig) +\n  tm_fill(\"classification\", title = \"EHSA Classification\") +\n  tm_text(\"Province\", size = 0.5) +\n  tm_borders(alpha = 0.4) +\ntm_layout(\n    main.title = \"Post Covid Number of Tourists\",\n    legend.position = c(\"right\", \"bottom\"),\n    bg.color = \"beige\",\n    asp = 0)\n\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them).\n\n\n\n\n\n\n\n\n\nWe can display the same provinces tabularly using the code chunk below\n\nfor (i in unique(ehsa_sig$classification)){\nprint(as.matrix(ehsa_sig[ehsa_sig$classification == i, c(\"Province\", \"classification\")]))\n}\n\n  Province       classification     geometry                      \n1 \"Nonthaburi\"   \"sporadic hotspot\" MULTIPOLYGON (((100.3415 14...\n2 \"Sakon Nakhon\" \"sporadic hotspot\" MULTIPOLYGON (((103.5404 18...\n  Province  classification      geometry                      \n3 \"Lamphun\" \"sporadic coldspot\" MULTIPOLYGON (((99.18821 18...\n6 \"Ranong\"  \"sporadic coldspot\" MULTIPOLYGON (((98.35294 9....\n7 \"Yala\"    \"sporadic coldspot\" MULTIPOLYGON (((101.2927 6....\n  Province       classification        geometry                      \n4 \"Nakhon Sawan\" \"consecutive hotspot\" MULTIPOLYGON (((100.0266 16...\n5 \"Phuket\"       \"consecutive hotspot\" MULTIPOLYGON (((98.31437 7....\n\n\nThe results identifies Phuket and Nakhon Sawan as consecutive hotspots which means they had a single uninterrupted run of being significant hotspots, but have been significant hotspot for less than 90% of the time.\nNonthaburi and Sakon Nakhon are sporadic hotspots, while Lamphun, Ranong and Yala are sporadic coldspots which mean that they have been on-and-off as hot and coldspots for the number of tourists."
  },
  {
    "objectID": "posts/thai-tourism-covid/index.html#f.3-ehsa---occupancy-rate",
    "href": "posts/thai-tourism-covid/index.html#f.3-ehsa---occupancy-rate",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "F.3 EHSA - Occupancy Rate",
    "text": "F.3 EHSA - Occupancy Rate\nWe perform EHSA on the post-covid (2022 onwards) values for the tourist occupancy rates.\n\nF.3.1 Creating spacetime cube\nWe then create the space-time cube using spacetime() for just the occupancy rates. We use select() to limit the original dataframe to only the date, province and variable.\n\nstcube &lt;- spacetime(select(occupancy_postCov,\n                           YYYYMM, Province, occupancy),\n                    thai_sf,\n                    .loc_col = \"Province\",\n                    .time_col = \"YYYYMM\")\n\nTo check that the object is in the correct format, we can use is_spacetime_cube() and confirm that it returns TRUE\n\nis_spacetime_cube(stcube)\n\n[1] TRUE\n\n\n\n\nF.3.2 Calculating \\(G_i^*\\) statistics\nBefore computing for the \\(G_i^*\\) statistics, we need to derive the spatial weights. We use the code chunk below to identify the neighbors and to derive inverse distance weights. The alpha argument of st_inverse_distance() determines the level of distance decay.\n\nstcube_nb &lt;- stcube %&gt;%\n  activate(\"geometry\") %&gt;%\n  mutate(nb = include_self(\n    st_contiguity(geometry)),\n    wt=st_inverse_distance(nb,\n                  geometry,\n                  scale = 1,\n                  alpha = 1),\n    .before = 1) %&gt;%\n  set_nbs(\"nb\") %&gt;%\n  set_wts(\"wt\")\n\nWe then use the following chunk to calculate the local \\(G_i^*\\) for each location. We do this using local_gstar_perm() of sfdep package. We then use unnest() to unnest the gi_star column of the newly created data frame.\n\ngi_stars &lt;- stcube_nb %&gt;%\n  group_by(YYYYMM) %&gt;%\n  mutate(gi_star = local_gstar_perm(\n    occupancy, nb, wt)) %&gt;%\n  tidyr::unnest(gi_star)\n\n\n\nF.3.3 Identifying Hot- and Cold-spots using Mann Kendall test\nThe code chunk below runs the Mann Kendall test (without permutations) on each province using MannKendall(). Provinces where the tau value is positive are showing an increasing trend, while negative tau values show decreasing trend.\nWe display the records with significant test results, and show negative and positive tau values as separate tables.\n\nehsa &lt;- gi_stars %&gt;%\n  group_by(Province) %&gt;%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %&gt;%\n  tidyr::unnest_wider(mk)\n\nfilter(ehsa, sl &lt; 0.05, tau &gt; 0)\n\n# A tibble: 17 × 6\n   Province              tau         sl     S     D  varS\n   &lt;chr&gt;               &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Bangkok             0.934 0.00000429    85  91.0  334.\n 2 Buriram             0.604 0.00311       55  91.0  334.\n 3 Chumphon            0.670 0.00102       61  91.0  334.\n 4 Krabi               0.714 0.000459      65  91.0  334.\n 5 Nakhon Si Thammarat 0.824 0.0000510     75  91.0  334.\n 6 Narathiwat          0.626 0.00217       57  91.0  334.\n 7 Nonthaburi          0.714 0.000459      65  91.0  334.\n 8 Phatthalung         0.692 0.000688      63  91.0  334.\n 9 Phuket              0.890 0.0000119     81  91.0  334.\n10 Prachuap Khiri Khan 0.934 0.00000429    85  91.0  334.\n11 Ranong              0.560 0.00620       51  91.0  334.\n12 Ratchaburi          0.604 0.00311       55  91.0  334.\n13 Rayong              0.626 0.00217       57  91.0  334.\n14 Samut Prakan        0.736 0.000302      67  91.0  334.\n15 Songkhla            0.648 0.00150       59  91.0  334.\n16 Surat Thani         0.912 0.00000715    83  91.0  334.\n17 Yala                0.495 0.0160        45  91.0  334.\n\nfilter(ehsa, sl &lt; 0.05, tau &lt; 0)\n\n# A tibble: 23 × 6\n   Province         tau       sl     S     D  varS\n   &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Amnat Charoen -0.780 0.000127   -71  91.0  334.\n 2 Chachoengsao  -0.582 0.00442    -53  91.0  334.\n 3 Chainat       -0.473 0.0215     -43  91.0  334.\n 4 Chaiyaphum    -0.736 0.000302   -67  91.0  334.\n 5 Chanthaburi   -0.429 0.0375     -39  91.0  334.\n 6 Kanchanaburi  -0.582 0.00442    -53  91.0  334.\n 7 Lopburi       -0.451 0.0285     -41  91.0  334.\n 8 Nakhon Nayok  -0.604 0.00311    -55  91.0  334.\n 9 Nakhon Pathom -0.648 0.00150    -59  91.0  334.\n10 Nakhon Sawan  -0.451 0.0285     -41  91.0  334.\n# ℹ 13 more rows\n\n\nThe results show that 17 of the 77 provinces are showing significant positive trend. However, there are 23 provinces (more) which are showing a significant negative trend.\n\n\nF.3.4 Emerging Hot Spot Analysis\nWe can use emerging_hot_spot_analysis() to perform EHSA, including the previous hypothesis testing, and classify provinces based on the test results.\nThe code chunk below uses emerging_hot_spot_analysis() directly on the spacetime cube to run the test with 100 permutations.\n\nehsa &lt;- emerging_hotspot_analysis(\n  x = stcube,\n  .var = \"occupancy\",\n  k = 1,\n  nsim = 99\n)\n\nWe can visualize the number of provinces per category using the code chunk below which utilizes ggplot() to build a bar chart.\n\nggplot(data = ehsa,\n       aes(x = classification)) +\n  geom_bar(fill = \"lightblue\") +\n  labs(title = \"EHSA Classification - PostCovid Tourism Revenue\",\n       y = \"Number of Provinces\", x=\"\") +\n  theme_minimal() +\n  geom_text(stat = 'count', aes(label = ..count..), vjust = 0) +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))\n\n\n\n\n\n\n\n\nWe can also visualize the hot- and coldspot classifications visually. First, we need to combine the ehsa object with the sf dataframe with the code chunk below.\n\nthai_ehsa &lt;- thai_sf %&gt;%\n  left_join(ehsa,\n            by = join_by(Province == location))\n\nNext, we use tmap package to create a choropleth map based on the EHSA classification. We only include provinces that have significant test results and a pattern detected.\n\nehsa_sig &lt;- filter(thai_ehsa, !(classification == \"no pattern detected\"), p_value &lt; 0.05)\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_shape(thai_ehsa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(ehsa_sig) +\n  tm_fill(\"classification\", title = \"EHSA Classification\") +\n  tm_text(\"Province\", size = 0.5) +\n  tm_borders(alpha = 0.4) +\ntm_layout(\n    main.title = \"Post Covid Tourist Occupancy Rate\",\n    legend.position = c(\"right\", \"bottom\"),\n    bg.color = \"beige\",\n    asp = 0)\n\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them).\n\n\n\n\n\n\n\n\n\nWe can display the same provinces tabularly using the code chunk below\n\nfor (i in unique(ehsa_sig$classification)){\nprint(as.matrix(ehsa_sig[ehsa_sig$classification == i, c(\"Province\", \"classification\")]))\n}\n\n   Province                   classification      \n1  \"Nonthaburi\"               \"oscilating hotspot\"\n2  \"Phra Nakhon Si Ayutthaya\" \"oscilating hotspot\"\n5  \"Chanthaburi\"              \"oscilating hotspot\"\n7  \"Sa Kaeo\"                  \"oscilating hotspot\"\n8  \"Loei\"                     \"oscilating hotspot\"\n9  \"Maha Sarakham\"            \"oscilating hotspot\"\n11 \"Sakon Nakhon\"             \"oscilating hotspot\"\n13 \"Lamphun\"                  \"oscilating hotspot\"\n16 \"Nakhon Sawan\"             \"oscilating hotspot\"\n18 \"Kamphaeng Phet\"           \"oscilating hotspot\"\n19 \"Tak\"                      \"oscilating hotspot\"\n20 \"Sukhothai\"                \"oscilating hotspot\"\n21 \"Kanchanaburi\"             \"oscilating hotspot\"\n25 \"Nakhon Si Thammarat\"      \"oscilating hotspot\"\n26 \"Phuket\"                   \"oscilating hotspot\"\n   geometry                      \n1  MULTIPOLYGON (((100.3415 14...\n2  MULTIPOLYGON (((100.5131 14...\n5  MULTIPOLYGON (((102.2517 12...\n7  MULTIPOLYGON (((102.1877 14...\n8  MULTIPOLYGON (((102.095 18....\n9  MULTIPOLYGON (((103.1562 16...\n11 MULTIPOLYGON (((103.5404 18...\n13 MULTIPOLYGON (((99.18821 18...\n16 MULTIPOLYGON (((100.0266 16...\n18 MULTIPOLYGON (((99.48875 16...\n19 MULTIPOLYGON (((97.97318 17...\n20 MULTIPOLYGON (((99.60051 17...\n21 MULTIPOLYGON (((98.58631 15...\n25 MULTIPOLYGON (((99.77467 9....\n26 MULTIPOLYGON (((98.31437 7....\n   Province          classification      geometry                      \n3  \"Sing Buri\"       \"sporadic coldspot\" MULTIPOLYGON (((100.3691 15...\n4  \"Chai Nat\"        \"sporadic coldspot\" MULTIPOLYGON (((100.1199 15...\n10 \"Roi Et\"          \"sporadic coldspot\" MULTIPOLYGON (((104.314 16....\n12 \"Chiang Mai\"      \"sporadic coldspot\" MULTIPOLYGON (((99.52512 20...\n15 \"Phrae\"           \"sporadic coldspot\" MULTIPOLYGON (((100.1597 18...\n17 \"Uthai Thani\"     \"sporadic coldspot\" MULTIPOLYGON (((99.13905 15...\n23 \"Samut Songkhram\" \"sporadic coldspot\" MULTIPOLYGON (((100.0116 13...\n27 \"Surat Thani\"     \"sporadic coldspot\" MULTIPOLYGON (((99.96396 9....\n28 \"Songkhla\"        \"sporadic coldspot\" MULTIPOLYGON (((100.5973 7....\n29 \"Satun\"           \"sporadic coldspot\" MULTIPOLYGON (((100.0903 6....\n31 \"Narathiwat\"      \"sporadic coldspot\" MULTIPOLYGON (((101.6323 6....\n   Province       classification         geometry                      \n6  \"Prachin Buri\" \"consecutive coldspot\" MULTIPOLYGON (((101.4881 14...\n24 \"Phetchaburi\"  \"consecutive coldspot\" MULTIPOLYGON (((99.75869 13...\n   Province        classification        geometry                      \n14 \"Uttaradit\"     \"consecutive hotspot\" MULTIPOLYGON (((101.0924 18...\n22 \"Nakhon Pathom\" \"consecutive hotspot\" MULTIPOLYGON (((100.2231 14...\n30 \"Yala\"          \"consecutive hotspot\" MULTIPOLYGON (((101.2927 6....\n\n\nThe results identify a number of cold and hotspots. If we focus on the coldspots:\n\nwe see that Prachin Buri and Phetchaburi are consecutive coldspots\nA list of 11 provinces, including Chiang Mai, are sporadic coldspots"
  },
  {
    "objectID": "posts/starting-linear-programming-in-python/index.html",
    "href": "posts/starting-linear-programming-in-python/index.html",
    "title": "Starting Linear Programming in Python",
    "section": "",
    "text": "In this post, I will solve a simple linear programming (LP) problem using three Python packages: Google’s ORTools, PuLP and SciPy.\nThis is the first time I am trying optimization in Python. My previous experience with solver software has been with some commercial solvers or MS Excel while I was in college, and using supply chain modeling software at work. For my first try, I am being guided by the posts of Mirko Stojiljković from Real Python, and Maximme Labonne from Towards Data Science.\nSince I will be focusing on the packages, I won’t be explaining Linear or Mathematical Programming in this post, and will go straight to the problem, the mathematical formulation, and then the building and optimizing of the model using the three packages mentioned. Mirko’s and Maximme’s posts give good overviews, but there are a large number of material online and offline covering linear programming and operations research."
  },
  {
    "objectID": "posts/starting-linear-programming-in-python/index.html#a-simple-profit-maximization-problem",
    "href": "posts/starting-linear-programming-in-python/index.html#a-simple-profit-maximization-problem",
    "title": "Starting Linear Programming in Python",
    "section": "A simple profit maximization problem",
    "text": "A simple profit maximization problem\nA company produces tools at two plants and sells them to three customers.The cost of producing 1000 tools at a plant and shipping them to a customer is given in the table below.\n\n\n\nPlant\nCustomer 1\nCustomer 2\nCustomer 3\n\n\n\n\n1\n60\n30\n160\n\n\n2\n130\n70\n170\n\n\n\nCustomers 1 and 3 pay $200 per thousand tools; Customer 2 pays $150. To produce 1,000 tools at Plant 1, 200 hours of labor are needed, while 300 hours are needed at Plant 2. A total of 5,500 hours of labor are available are available for use at the two plants. Additional labor hours can be purchased at $20 per labor hour. Plant 1 can produce up to 10,000 tools while Plant 2 can produce up to 12,000 tools. Demand by each customer is assumed to be unlimited.\nHow many tools should be produced and shipped by each plant to each customer?"
  },
  {
    "objectID": "posts/starting-linear-programming-in-python/index.html#lp-formulation",
    "href": "posts/starting-linear-programming-in-python/index.html#lp-formulation",
    "title": "Starting Linear Programming in Python",
    "section": "LP Formulation",
    "text": "LP Formulation\nI will jump straight into fomulating the problem as an LP model by defining its three major parts: the decision variables, the objective funtion, the constraints.\nThe decision variables are the unknowns that we want to solve for. In our problem, these are:\n$ X_{ij} =$ the number of tools in thousands produced in plant \\(i\\) and shipped to customer \\(j\\) for \\(i = 1, 2\\), and \\(j = 1, 2, 3\\)\n$ L =$ additional labor hours to purchase\nThe objective function is a value that we want to maximize or minimize and is a function of the decision variables. As we have sales and cost figures, we would expect that we want to maximize the profit generated by our decisions:\nMaximize:\n\\(Z =\\) sales from customers + cost to ship to customers + cost of additional labor hours\n\\(Z = 200 \\sum_{i=1}^{2} X_{i1}\\ + 150 \\sum_{i=1}^{2} X_{i2}\\ + 200 \\sum_{i=1}^{2} X_{i3}\\ - \\sum t_{ij}X_{ij} \\ - 20L\\) , where \\(t_{ij}\\) is the cost of shipping 1000 units from plant \\(i\\) to customer \\(j\\)\n\\(Z = (200-60)X_{11} + (200-130)X_{21} + (150-30)X_{12} + (150-70)X_{22} + (200-160)X_{13} + (200-170)X_{23} - 20L\\)\n\\(Z = 140X_{11} + 70X_{21} + 120X_{12} + 80X_{22} + 40X_{13} + 30X_{23} - 20L\\)\nThe constraints represent the conditions or requirements and limit the values that variables can take.\nLabor constraint:\n\\(200 (X_{11}+X_{12}+X_{13}) + 300(X_{21}+X_{22}+X_{23}) \\leq 5500 + L\\)\nProduction constraints:\n\\(X_{11} + X_{12} + X_{13} \\leq 10\\)\n\\(X_{21} + X_{22} + X_{23} \\leq 12\\)\nNonnegativity: All variables are nonnegative\n\\(X_{ij} \\geq 0\\) ; for every plant \\(i\\) and customer \\(j\\)\n\\(L \\geq 0\\)"
  },
  {
    "objectID": "posts/la-crimes/index.html",
    "href": "posts/la-crimes/index.html",
    "title": "Identifying Crime Hotspots in LA",
    "section": "",
    "text": "In this post, I look at reported crimes in LA (2020-2023) to see which areas have high and low incidence– which might be one way to gauge the level of safety in those areas. I will be using R, specifically the tidyverse, sf, tmap, spatstat, sfdep and spdep packages in this post."
  },
  {
    "objectID": "posts/la-crimes/index.html#importing-and-transforming-the-data",
    "href": "posts/la-crimes/index.html#importing-and-transforming-the-data",
    "title": "Identifying Crime Hotspots in LA",
    "section": "Importing and Transforming the Data",
    "text": "Importing and Transforming the Data\nThe code chunk below loads tidyverse and then loads the csv into the object la_crimes. tidyverse is a collection of multiple packages that are commonly used for data wrangling and cleaning.\n\npacman::p_load(tidyverse)\nla_crimes &lt;- read_csv(\"data/raw/Crime_Data_from_2020_to_Present.csv\", show_col_types = FALSE)\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nThe file contains 987K rows and 28 columns. We can check all column names using colnames().\n\ncolnames(la_crimes)\n\n [1] \"DR_NO\"          \"Date Rptd\"      \"DATE OCC\"       \"TIME OCC\"      \n [5] \"AREA\"           \"AREA NAME\"      \"Rpt Dist No\"    \"Part 1-2\"      \n [9] \"Crm Cd\"         \"Crm Cd Desc\"    \"Mocodes\"        \"Vict Age\"      \n[13] \"Vict Sex\"       \"Vict Descent\"   \"Premis Cd\"      \"Premis Desc\"   \n[17] \"Weapon Used Cd\" \"Weapon Desc\"    \"Status\"         \"Status Desc\"   \n[21] \"Crm Cd 1\"       \"Crm Cd 2\"       \"Crm Cd 3\"       \"Crm Cd 4\"      \n[25] \"LOCATION\"       \"Cross Street\"   \"LAT\"            \"LON\"           \n\n\nI am only concerned about the location of the crimes, the types of crimes, and the date (especially the year) the crimes were committed. I might also be interested in whether the crime happened indoors or outdoors. This means that I can just focus on five columns/variables– which I will define in an object cols_to_keep for easy selecting later. I will also be making the column names clearer by replacing them with the ones I am defining in cols_newnames.\n\ncols_to_keep &lt;- c(\"DATE OCC\", \"Crm Cd Desc\", \"LAT\", \"LON\", \"Premis Desc\")\ncols_newnames &lt;- c(\"Date\", \"Crime Type\", \"latitude\", \"longitude\", \"Premise\")\n\nWe can check the selected columns by using head() on a subset of the data.\n\nhead(select(la_crimes, all_of(cols_to_keep)))\n\n# A tibble: 6 × 5\n  `DATE OCC`             `Crm Cd Desc`                   LAT   LON `Premis Desc`\n  &lt;chr&gt;                  &lt;chr&gt;                         &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;        \n1 03/01/2020 12:00:00 AM VEHICLE - STOLEN               34.0 -118. STREET       \n2 02/08/2020 12:00:00 AM BURGLARY FROM VEHICLE          34.0 -118. BUS STOP/LAY…\n3 11/04/2020 12:00:00 AM BIKE - STOLEN                  34.0 -118. MULTI-UNIT D…\n4 03/10/2020 12:00:00 AM SHOPLIFTING-GRAND THEFT ($95…  34.2 -118. CLOTHING STO…\n5 09/09/2020 12:00:00 AM VEHICLE - STOLEN               34.1 -118. STREET       \n6 05/02/2020 12:00:00 AM VEHICLE - STOLEN               34.1 -118. STREET       \n\n\nThis shows that the date (DATE OCC) columns is currently a character string with the first portion being the date in US format. (i.e., month before day)\nIn the following code chunk, we will reload the data and keep only the five columns using select(). Changing the names can be done by assigning a list to colnames(). We then convert the Date column into the right type and then keep only records that happened before 2024.\n\nla_crimes &lt;- read_csv(\"data/raw/Crime_Data_from_2020_to_Present.csv\", show_col_types = FALSE) %&gt;%\n  select(all_of(cols_to_keep)) #Keep five columns\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\ncolnames(la_crimes) &lt;- cols_newnames #Rename columns\n\nla_crimes$Date &lt;- mdy_hms(la_crimes$Date) #Convert date column from string\n\nla_crimes &lt;- la_crimes[la_crimes$Date &lt; ymd(\"2024-01-01\"),] #Keep only pre2024 data\nhead(la_crimes)\n\n# A tibble: 6 × 5\n  Date                `Crime Type`                    latitude longitude Premise\n  &lt;dttm&gt;              &lt;chr&gt;                              &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;  \n1 2020-03-01 00:00:00 VEHICLE - STOLEN                    34.0     -118. STREET \n2 2020-02-08 00:00:00 BURGLARY FROM VEHICLE               34.0     -118. BUS ST…\n3 2020-11-04 00:00:00 BIKE - STOLEN                       34.0     -118. MULTI-…\n4 2020-03-10 00:00:00 SHOPLIFTING-GRAND THEFT ($950.…     34.2     -118. CLOTHI…\n5 2020-09-09 00:00:00 VEHICLE - STOLEN                    34.1     -118. STREET \n6 2020-05-02 00:00:00 VEHICLE - STOLEN                    34.1     -118. STREET \n\n\nThe new object now has 867K rows and 5 columns, from the original 987K rows and 28 columns.\nI will also be adding a few date-derived columns to make the analyses easier later. These are done by the ccode chunk below for the year, the month and the day of the week.\n\nla_crimes$Year &lt;- year(la_crimes$Date)\nla_crimes$Month &lt;- month(la_crimes$Date, label = TRUE, abbr = FALSE)\nla_crimes$Day_of_Week &lt;- wday(la_crimes$Date, label = TRUE, abbr = FALSE)"
  },
  {
    "objectID": "posts/la-crimes/index.html#data-quality-checking",
    "href": "posts/la-crimes/index.html#data-quality-checking",
    "title": "Identifying Crime Hotspots in LA",
    "section": "Data Quality Checking",
    "text": "Data Quality Checking\nI can perform a few data quality checks on the data that we have left– before I go into analyzing the data.\nOne possible check is to see the range of the dates. Do they all fall between Jan 1st 2020 and Dec 31st 2023? Are there any invalid dates? summary() gives the range and quartiles so it can be used for displaying both the minimum and maximum. The function is.na() returns TRUE if a value is invalid.\n\nsummary(as.Date(la_crimes$Date))\n\n        Min.      1st Qu.       Median         Mean      3rd Qu.         Max. \n\"2020-01-01\" \"2020-11-09\" \"2022-02-25\" \"2022-01-05\" \"2023-01-24\" \"2023-12-31\" \n\nsum(is.na(la_crimes$Date))\n\n[1] 0\n\n\nThe output above shows that the dates are indeed between the beginning of 2020 to the end of 2023, and that there are no invalid dates.\nAnother check that can be done is to check if there are any irregularities or errors in the string columns: Crime Type and Premise. I can first check the number of unique values that these two values contain using the function n_distinct().\n\nn_distinct(la_crimes$'Crime Type')\n\n[1] 138\n\nn_distinct(la_crimes$Premise)\n\n[1] 306\n\n\nThere appears to be a very large number of entries for these two columns. The unique() function can show the unique values in a column. The code chunk below displays the unique values for Crime Type as a single string with entries separated by ‘//’\n\nprint(paste(unique(la_crimes$'Crime Type'), collapse = \" // \"))\n\n[1] \"VEHICLE - STOLEN // BURGLARY FROM VEHICLE // BIKE - STOLEN // SHOPLIFTING-GRAND THEFT ($950.01 & OVER) // ARSON // BURGLARY // PIMPING // PANDERING // OTHER MISCELLANEOUS CRIME // VANDALISM - MISDEAMEANOR ($399 OR UNDER) // INTIMATE PARTNER - SIMPLE ASSAULT // ROBBERY // THEFT-GRAND ($950.01 & OVER)EXCPT,GUNS,FOWL,LIVESTK,PROD // ASSAULT WITH DEADLY WEAPON, AGGRAVATED ASSAULT // THEFT OF IDENTITY // BATTERY - SIMPLE ASSAULT // SHOPLIFTING - PETTY THEFT ($950 & UNDER) // BUNCO, GRAND THEFT // VIOLATION OF COURT ORDER // VIOLATION OF RESTRAINING ORDER // THEFT PLAIN - PETTY ($950 & UNDER) // VANDALISM - FELONY ($400 & OVER, ALL CHURCH VANDALISMS) // RAPE, FORCIBLE // THEFT FROM MOTOR VEHICLE - GRAND ($950.01 AND OVER) // TRESPASSING // VEHICLE - ATTEMPT STOLEN // RESISTING ARREST // EMBEZZLEMENT, GRAND THEFT ($950.01 & OVER) // BURGLARY FROM VEHICLE, ATTEMPTED // LETTERS, LEWD  -  TELEPHONE CALLS, LEWD // CRIMINAL THREATS - NO WEAPON DISPLAYED // SEX OFFENDER REGISTRANT OUT OF COMPLIANCE // UNAUTHORIZED COMPUTER ACCESS // THEFT FROM MOTOR VEHICLE - PETTY ($950 & UNDER) // CRM AGNST CHLD (13 OR UNDER) (14-15 & SUSP 10 YRS OLDER) // BRANDISH WEAPON // BURGLARY, ATTEMPTED // DISCHARGE FIREARMS/SHOTS FIRED // BATTERY POLICE (SIMPLE) // VEHICLE, STOLEN - OTHER (MOTORIZED SCOOTERS, BIKES, ETC) // ORAL COPULATION // INDECENT EXPOSURE // THEFT FROM PERSON - ATTEMPT // CHILD ABUSE (PHYSICAL) - SIMPLE ASSAULT // OTHER ASSAULT // DISTURBING THE PEACE // INTIMATE PARTNER - AGGRAVATED ASSAULT // BOMB SCARE // FAILURE TO YIELD // CONTEMPT OF COURT // ATTEMPTED ROBBERY // ASSAULT WITH DEADLY WEAPON ON POLICE OFFICER // DOCUMENT FORGERY / STOLEN FELONY // BUNCO, PETTY THEFT // SEXUAL PENETRATION W/FOREIGN OBJECT // SHOTS FIRED AT INHABITED DWELLING // CHILD STEALING // DEFRAUDING INNKEEPER/THEFT OF SERVICES, $950 & UNDER // KIDNAPPING - GRAND ATTEMPT // SHOTS FIRED AT MOVING VEHICLE, TRAIN OR AIRCRAFT // THEFT, PERSON // CHILD ABUSE (PHYSICAL) - AGGRAVATED ASSAULT // EXTORTION // CHILD NEGLECT (SEE 300 W.I.C.) // TILL TAP - GRAND THEFT ($950.01 & OVER) // SEX,UNLAWFUL(INC MUTUAL CONSENT, PENETRATION W/ FRGN OBJ // BATTERY WITH SEXUAL CONTACT // HUMAN TRAFFICKING - COMMERCIAL SEX ACTS // CHILD ANNOYING (17YRS & UNDER) // DOCUMENT WORTHLESS ($200.01 & OVER) // RAPE, ATTEMPTED // FALSE IMPRISONMENT // THROWING OBJECT AT MOVING VEHICLE // LEWD CONDUCT // PEEPING TOM // KIDNAPPING // CRIMINAL HOMICIDE // STALKING // THEFT PLAIN - ATTEMPT // SODOMY/SEXUAL CONTACT B/W PENIS OF ONE PERS TO ANUS OTH // VIOLATION OF TEMPORARY RESTRAINING ORDER // CHILD PORNOGRAPHY // WEAPONS POSSESSION/BOMBING // DRIVING WITHOUT OWNER CONSENT (DWOC) // THEFT FROM MOTOR VEHICLE - ATTEMPT // PICKPOCKET // SHOPLIFTING - ATTEMPT // COUNTERFEIT // BUNCO, ATTEMPT // DEFRAUDING INNKEEPER/THEFT OF SERVICES, OVER $950.01 // CRUELTY TO ANIMALS // FALSE POLICE REPORT // PROWLER // DISHONEST EMPLOYEE - GRAND THEFT // THREATENING PHONE CALLS/LETTERS // PURSE SNATCHING // EMBEZZLEMENT, PETTY THEFT ($950 & UNDER) // DOCUMENT WORTHLESS ($200 & UNDER) // ILLEGAL DUMPING // LEWD/LASCIVIOUS ACTS WITH CHILD // BATTERY ON A FIREFIGHTER // PETTY THEFT - AUTO REPAIR // MANSLAUGHTER, NEGLIGENT // RECKLESS DRIVING // TILL TAP - PETTY ($950 & UNDER) // PURSE SNATCHING - ATTEMPT // LYNCHING - ATTEMPTED // CREDIT CARDS, FRAUD USE ($950.01 & OVER) // CREDIT CARDS, FRAUD USE ($950 & UNDER // THEFT, COIN MACHINE - PETTY ($950 & UNDER) // HUMAN TRAFFICKING - INVOLUNTARY SERVITUDE // BIKE - ATTEMPTED STOLEN // CONTRIBUTING // BRIBERY // BOAT - STOLEN // CONSPIRACY // GRAND THEFT / INSURANCE FRAUD // DRUGS, TO A MINOR // CHILD ABANDONMENT // THEFT, COIN MACHINE - GRAND ($950.01 & OVER) // DISRUPT SCHOOL // THEFT, COIN MACHINE - ATTEMPT // DISHONEST EMPLOYEE - PETTY THEFT // LYNCHING // FIREARMS RESTRAINING ORDER (FIREARMS RO) // REPLICA FIREARMS(SALE,DISPLAY,MANUFACTURE OR DISTRIBUTE) // GRAND THEFT / AUTO REPAIR // DRUNK ROLL // PICKPOCKET, ATTEMPT // TELEPHONE PROPERTY - DAMAGE // BEASTIALITY, CRIME AGAINST NATURE SEXUAL ASSLT WITH ANIM // FIREARMS EMERGENCY PROTECTIVE ORDER (FIREARMS EPO) // BIGAMY // INCEST (SEXUAL ACTS BETWEEN BLOOD RELATIVES) // BLOCKING DOOR INDUCTION CENTER // INCITING A RIOT // DISHONEST EMPLOYEE ATTEMPTED THEFT // FAILURE TO DISPERSE\"\n\n\nIt shows a very diverse list of types of crimes. Some seem to be specific variants of certain crimes. It would have been great if I could consolidate this into a fewer number of types, but we can wait until we dive into the data to see if I can just focus on a few types to answer my questions.\nThe code chunk below does the same and shows the unique values for Premise.\n\nprint(paste(unique(la_crimes$Premise), collapse = \" // \"))\n\n[1] \"STREET // BUS STOP/LAYOVER (ALSO QUERY 124) // MULTI-UNIT DWELLING (APARTMENT, DUPLEX, ETC) // CLOTHING STORE // PUBLIC STORAGE // JEWELRY STORE // OTHER BUSINESS // PARKING LOT // ALLEY // GAS STATION // CAR WASH // SINGLE FAMILY DWELLING // CONDOMINIUM/TOWNHOUSE // RESTAURANT/FAST FOOD // SIDEWALK // NURSING/CONVALESCENT/RETIREMENT HOME // MARKET // GOVERNMENT FACILITY (FEDERAL,STATE, COUNTY & CITY) // DRIVEWAY // MINI-MART // YARD (RESIDENTIAL/BUSINESS) // VEHICLE, PASSENGER/TRUCK // GARAGE/CARPORT // PARKING UNDERGROUND/BUILDING // NAIL SALON // PORCH, RESIDENTIAL // DRUG STORE // MTA BUS // MTA - RED LINE - VERMONT/BEVERLY // OTHER PREMISE // DAY CARE/CHILDREN* // BUS STOP // POLICE FACILITY // MISSIONS/SHELTERS // CHURCH/CHAPEL (CHANGED 03-03 FROM CHURCH/TEMPLE) // PHARMACY INSIDE STORE OR SUPERMARKET* // DISCOUNT STORE (99 CENT,DOLLAR,ETC. // MTA - RED LINE - 7TH AND METRO CENTER // OFFICE BUILDING/OFFICE // OTHER STORE // DELIVERY SERVICE (FED EX, UPS, COURIERS,COURIER SERVICE)* // HOTEL // MORTUARY // OTHER RESIDENCE // OTHER/OUTSIDE // MTA - EXPO LINE - EXPO/LA BREA // JUNIOR HIGH SCHOOL // NIGHT CLUB (OPEN EVENINGS ONLY) // COFFEE SHOP (STARBUCKS, COFFEE BEAN, PEET'S, ETC.) // CYBERSPACE // HIGH SCHOOL // CONSTRUCTION SITE // STUDIO (FILM/PHOTOGRAPHIC/MUSIC) // BEAUTY/BARBER SHOP // MOTEL // VALET // PARK/PLAYGROUND // TRANSITIONAL HOUSING/HALFWAY HOUSE // LAUNDROMAT // AUTO SALES LOT // ELEMENTARY SCHOOL // LA UNION STATION (NOT LINE SPECIFIC) // MTA - EXPO LINE - EXPO/VERMONT // BUS DEPOT/TERMINAL, OTHER THAN MTA // SPECIALTY SCHOOL/OTHER // CONVENTION CENTER // DETENTION/JAIL FACILITY // DIY CENTER (LOWE'S,HOME DEPOT,OSH,CONTRACTORS WAREHOUSE) // NURSERY/FLOWER SHOP // GROUP HOME // RECYCLING CENTER // 7TH AND METRO CENTER (NOT LINE SPECIFIC) // FRAT HOUSE/SORORITY/DORMITORY // TAXI // NA // HOSPITAL // DEPARTMENT STORE // PROJECT/TENEMENT/PUBLIC HOUSING // TRANSIENT ENCAMPMENT // MTA - RED LINE - WESTLAKE/MACARTHUR PARK // THE GROVE // SINGLE RESIDENCE OCCUPANCY (SRO'S) LOCATIONS // COLLEGE/JUNIOR COLLEGE/UNIVERSITY // VETERINARIAN/ANIMAL HOSPITAL // CHECK CASHING* // UNDERPASS/BRIDGE* // HEALTH SPA/GYM // MAIL BOX // MEDICAL/DENTAL OFFICES // LIQUOR STORE // AUTO REPAIR SHOP // TUNNEL // AUTO SUPPLY STORE* // MTA - EXPO LINE - EXPO/WESTERN // SHOPPING MALL (COMMON AREA) // BEAUTY SUPPLY STORE // DRIVE THRU* // THE BEVERLY CONNECTION // PUBLIC RESTROOM(INDOORS-INSIDE) // MTA - GOLD LINE - UNION STATION // POST OFFICE // APARTMENT/CONDO COMMON LAUNDRY ROOM // SYNAGOGUE/TEMPLE // MASSAGE PARLOR // MUNICIPAL BUS LINE INCLUDES LADOT/DASH // ABANDONED BUILDING ABANDONED HOUSE // PAWN SHOP // HARDWARE/BUILDING SUPPLY // STAPLES CENTER * // MTA - BLUE LINE - WASHINGTON // LIBRARY // FURNITURE STORE // STAIRWELL* // STORAGE SHED // HIGH-RISE BUILDING // THE BEVERLY CENTER // TV/RADIO/APPLIANCE // TOBACCO SHOP // MTA - RED LINE - UNION STATION // PATIO* // CELL PHONE STORE // MUSEUM // MTA - PURPLE LINE - CIVIC CENTER/GRAND PARK // CLEANER/LAUNDROMAT // MTA - RED LINE - CIVIC CENTER/GRAND PARK // AUTOMATED TELLER MACHINE (ATM) // SHORT-TERM VACATION RENTAL // PET STORE // MTA - GOLD LINE - HIGHLAND PARK // ELEVATOR // MOBILE HOME/TRAILERS/CONSTRUCTION TRAILERS/RV'S/MOTORHOME // DRIVE THRU BANKING (WINDOW)* // FREEWAY // BAR/COCKTAIL/NIGHTCLUB // MEMBERSHIP STORE (COSTCO,SAMS CLUB)* // BANK // DAY CARE/ADULTS* // MTA - RED LINE - PERSHING SQUARE // SLIPS/DOCK/MARINA/BOAT // BALCONY* // THEATRE/MOVIE // VEHICLE STORAGE LOT (CARS, TRUCKS, RV'S, BOATS, TRAILERS, ETC.) // CEMETARY* // MTA - EXPO LINE - FARMDALE // PRIVATE SCHOOL/PRESCHOOL // CREDIT UNION // WAREHOUSE // OIL REFINERY // SKATEBOARD FACILITY/SKATEBOARD PARK* // MTA - RED LINE - VERMONT/SUNSET // BEACH // TRAIN TRACKS // GUN/SPORTING GOODS // COLISEUM // MEDICAL MARIJUANA FACILITIES/BUSINESSES // WEBSITE // FOSTER HOME BOYS OR GIRLS* // MTA - RED LINE - VERMONT/SANTA MONICA // MTA - EXPO LINE - EXPO/SEPULVEDA // ELECTRONICS STORE (IE:RADIO SHACK, ETC.) // TOW YARD* // VISION CARE FACILITY* // TRASH CAN/TRASH DUMPSTER // TRANSPORTATION FACILITY (AIRPORT) // MTA - RED LINE - HOLLYWOOD/HIGHLAND // AIRCRAFT // SEX ORIENTED/BOOK STORE/STRIP CLUB/GENTLEMAN'S CLUB // BAR/SPORTS BAR (OPEN DAY & NIGHT) // AUTO DEALERSHIP (CHEVY, FORD, BMW, MERCEDES, ETC.) // PEDESTRIAN OVERCROSSING // MTA - SILVER LINE - HARBOR GATEWAY TRANSIT CTR // BASKETBALL COURTS // MTA - EXPO LINE - 7TH AND METRO CENTER // ENTERTAINMENT/COMEDY CLUB (OTHER) // MTA - EXPO LINE - EXPO/BUNDY // GOLF COURSE* // FIRE STATION // TELECOMMUNICATION FACILITY/LOCATION // FACTORY // MTA - RED LINE - HOLLYWOOD/VINE // SPORTS VENUE, OTHER // PUBLIC RESTROOM/OUTSIDE* // MTA - PURPLE LINE - WESTLAKE/MACARTHUR PARK // HOSPICE // MTA - BLUE LINE - SAN PEDRO // BUS-CHARTER/PRIVATE // RIVER BED* // MTA - ORANGE LINE - RESEDA // MTA - SILVER LINE - ROSECRANS // MANUFACTURING COMPANY // TATTOO PARLOR* // OPTICAL OFFICE INSIDE STORE OR SUPERMARKET* // MTA - EXPO LINE - EXPO PARK/USC // MTA - SILVER LINE - HARBOR FWY // MTA - ORANGE LINE - TAMPA // POOL-PUBLIC/OUTDOOR OR INDOOR* // METROLINK TRAIN // MTA - EXPO LINE - EXPO/CRENSHAW // MTA - EXPO LINE - LA CIENEGA/JEFFERSON // EQUIPMENT RENTAL // VACANT LOT // MTA - BLUE LINE - GRAND/LATTC // MTA - BLUE LINE - PICO // BANKING INSIDE MARKET-STORE * // AMUSEMENT PARK* // MTA - EXPO LINE - JEFFERSON/USC // SPORTS ARENA // MTA - BLUE LINE - 7TH AND METRO CENTER // MTA - RED LINE - HOLLYWOOD/WESTERN // SWAP MEET // BUS, SCHOOL, CHURCH // MTA - GOLD LINE - SOUTHWEST MUSEUM // GREYHOUND OR INTERSTATE BUS // TRADE SCHOOL (MEDICAL-TECHNICAL-BUSINESS)* // MTA - PURPLE LINE - UNION STATION // MTA - GOLD LINE - SOTO // MTA - BLUE LINE - 103RD/WATTS TOWERS // BOWLING ALLEY* // MTA - ORANGE LINE - WOODMAN // MTA - EXPO LINE - PICO // MTA - ORANGE LINE - VAN NUYS // OTHER PLACE OF WORSHIP // MTA PROPERTY OR PARKING LOT // MTA - PURPLE LINE - 7TH AND METRO CENTER // OTHER RR TRAIN (UNION PAC, SANTE FE ETC // BANK DROP BOX/MONEY DROP-OUTSIDE OF BANK* // COMPUTER SERVICES/REPAIRS/SALES // MTA - GOLD LINE - LINCOLN/CYPRESS // ABATEMENT LOCATION // MTA - GOLD LINE - MARIACHI PLAZA // MTA - EXPO LINE - WESTWOOD/RANCHO PARK // BOOK STORE // SAVINGS & LOAN // MTA - SILVER LINE - SLAUSON // MTA - ORANGE LINE - BALBOA // ABORTION CLINIC/ABORTION FACILITY* // TRAIN DEPOT/TERMINAL, OTHER THAN MTA // MTA - PURPLE LINE - PERSHING SQUARE // MTA - ORANGE LINE - WOODLEY // MTA - GOLD LINE - HERITAGE SQ // RECORD-CD MUSIC/COMPUTER GAME STORE // MTA - BLUE LINE - VERNON // MTA - SILVER LINE - UNION STATION // SEWAGE FACILITY/PIPE // DODGER STADIUM // MTA - SILVER LINE - 37TH ST/USC // MTA - GOLD LINE - CHINATOWN // MTA - GOLD LINE - INDIANA // TOOL SHED* // METHADONE CLINIC // PAY PHONE // AMTRAK TRAIN // MTA - SILVER LINE - DOWNTOWN STREET STOPS // VIDEO RENTAL STORE // TRUCK, COMMERICAL // MTA - SILVER LINE - PACIFIC COAST HWY // SURPLUS SURVIVAL STORE // HORSE RACING/SANTA ANITA PARK* // CATERING/ICE CREAM TRUCK // FINANCE COMPANY // DAM/RESERVOIR // CULTURAL SIGNIFICANCE/MONUMENT // MTA - ORANGE LINE - SEPULVEDA // MTA - GOLD LINE - LITTLE TOKYO/ARTS DISTRICT // MTA - GOLD LINE - PICO/ALISO // MASS GATHERING LOCATION // WATER FACILITY // OTHER INTERSTATE, CHARTER BUS // MTA - EXPO LINE - LATTC/ORTHO INSTITUTE // MTA - ORANGE LINE - VALLEY COLLEGE // ESCALATOR* // MTA - ORANGE LINE - SHERMAN WAY // MTA - ORANGE LINE - NORTH HOLLYWOOD // MTA - RED LINE - NORTH HOLLYWOOD // MTA - PURPLE LINE - WILSHIRE/NORMANDIE // ENERGY PLANT/FACILITY // MTA - ORANGE LINE - CHATSWORTH // MTA - PURPLE LINE - WILSHIRE/WESTERN // MTA - RED LINE - UNIVERSAL CITY/STUDIO CITY // MTA - RED LINE - WILSHIRE/VERMONT // MTA - GREEN LINE - AVALON // MTA - PURPLE LINE - WILSHIRE/VERMONT // MTA - GREEN LINE - HARBOR FWY // MTA - EXPO LINE - PALMS // SKATING RINK* // HANDBALL COURTS // MTA - ORANGE LINE - CANOGA // MTA - ORANGE LINE - NORDHOFF // TERMINAL, OTHER THAN MTA // MUSCLE BEACH // MTA - GREEN LINE - AVIATION/LAX // ARCADE,GAME ROOM/VIDEO GAMES (EXAMPLE CHUCKIE CHEESE)* // GARMENT MANUFACTURER // MTA - ORANGE LINE - LAUREL CANYON // MTA - ORANGE LINE - PIERCE COLLEGE // MTA - ORANGE LINE - ROSCOE // MOSQUE* // MTA - ORANGE LINE - DE SOTO // MTA - SILVER LINE - LAC/USC MEDICAL CENTER // DEPT OF DEFENSE FACILITY // RETIRED (DUPLICATE) DO NOT USE THIS CODE // HOCKEY RINK/ICE HOCKEY // MTA - SILVER LINE - MANCHESTER // CHEMICAL STORAGE/MANUFACTURING PLANT // TRAIN, OTHER THAN MTA (ALSO QUERY 809/810/811) // HARBOR FRWY STATION (NOT LINE SPECIFIC)\"\n\n\nWhile this is a longer list, it looks like there are a lot more identifiable groups. I will attempt to reduce this into a smaller list of items which I will call as Premise_Type. The first step is to create the new column as a copy of Premise.\n\nla_crimes$'Premise_Type' &lt;- la_crimes$Premise\n\nMy goal is primarily to make certain types of outdoor or public locations easier to find since these will be more relevant for me as a tourist.\n\n\nConsolidate MTA/Train\nI will be taking public transport during this upcoming trip so crimes that occur on trains and the stations are important.\n\nn_distinct(la_crimes$Premise_Type)\n\n[1] 306\n\nla_crimes &lt;- la_crimes %&gt;% mutate(Premise_Type = if_else(str_detect(Premise_Type, \"MTA\"), \"MTA/Train\", Premise_Type))\nla_crimes &lt;- la_crimes %&gt;% mutate(Premise_Type = if_else(str_detect(Premise_Type, \"LA UNION STATION\"), \"MTA/Train\", Premise_Type))\nla_crimes &lt;- la_crimes %&gt;% mutate(Premise_Type = if_else(str_detect(Premise_Type, \"TRAIN\"), \"MTA/Train\", Premise_Type))\n\nn_distinct(la_crimes$Premise_Type)\n\n[1] 211\n\n\n\n\nConsolidate Bus / Bus Station\nFor the same reason as trains, I will also be interested in crimes that occur in buses and bus stations.\n\nn_distinct(la_crimes$Premise_Type)\n\n[1] 211\n\nla_crimes &lt;- la_crimes %&gt;% mutate(Premise_Type = if_else(str_detect(Premise_Type, \"BUS\"), \"Bus (or Stop/Station)\", Premise_Type))\n\nn_distinct(la_crimes$Premise_Type)\n\n[1] 201\n\n\n\n\nConsolidate Schools\nThere are many premises that represent schools. While we are not going to visit any of these, consolidating them will make it easy to filter or pick them out when looking at results.\n\nn_distinct(la_crimes$Premise_Type)\n\n[1] 201\n\nla_crimes &lt;- la_crimes %&gt;% mutate(Premise_Type = if_else(str_detect(Premise_Type, \"SCHOOL\"), \"School\", Premise_Type))\nla_crimes &lt;- la_crimes %&gt;% mutate(Premise_Type = if_else(str_detect(Premise_Type, \"COLLEGE\"), \"School\", Premise_Type))\nla_crimes &lt;- la_crimes %&gt;% mutate(Premise_Type = if_else(str_detect(Premise_Type, \"DAY CARE\"), \"School\", Premise_Type))\nla_crimes &lt;- la_crimes %&gt;% mutate(Premise_Type = if_else(str_detect(Premise_Type, \"DORMITORY\"), \"School\", Premise_Type))\n\nn_distinct(la_crimes$Premise_Type)\n\n[1] 193\n\n\n\n\nConsolidate Residential\nThis will also make filtering out private residences– as we do not have any plans to visit or stay residences other than our hotel.\n\nn_distinct(la_crimes$Premise_Type)\n\n[1] 193\n\nla_crimes &lt;- la_crimes %&gt;% mutate(Premise_Type = if_else(str_detect(Premise_Type, \"DWELLING\"), \"Residential\", Premise_Type))\nla_crimes &lt;- la_crimes %&gt;% mutate(Premise_Type = if_else(str_detect(Premise_Type, \"CONDO\"), \"Residential\", Premise_Type))\nla_crimes &lt;- la_crimes %&gt;% mutate(Premise_Type = if_else(str_detect(Premise_Type, \"RESIDENTIAL\"), \"Residential\", Premise_Type))\nla_crimes &lt;- la_crimes %&gt;% mutate(Premise_Type = if_else(str_detect(Premise_Type, \"RESIDENCE\"), \"Residential\", Premise_Type))\nla_crimes &lt;- la_crimes %&gt;% mutate(Premise_Type = if_else(str_detect(Premise_Type, \"MOBILE HOME\"), \"Residential\", Premise_Type))\n\nn_distinct(la_crimes$Premise_Type)\n\n[1] 186"
  },
  {
    "objectID": "posts/la-crimes/index.html#exploratory-data-analysis",
    "href": "posts/la-crimes/index.html#exploratory-data-analysis",
    "title": "Identifying Crime Hotspots in LA",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nI will perform some EDA or exploratory data analysis even before bringing in the geospatial data. The charts generated will be from ggplot which is already included in tidyverse. (so there is no need to import the package)\n\nNumber of Crimes Over Time (Total)\nI first want to see whether there are trends or seasonality in the occurrence of crimes overall. The code below produces a bar chart for the total number of crimes recorded per year.\n\nchart_data &lt;- la_crimes %&gt;%\n    group_by(Year) %&gt;%\n    summarise(count = round(n()/1000,1), .groups = 'drop')\n\nggplot(chart_data, aes(x = factor(Year), y = count)) +\n    geom_bar(stat = \"identity\", fill = \"skyblue\") +\n    geom_text(aes(label = count), vjust = -0.5, size = 3.5) +\n    labs(title = \"Number of Crimes Recorded by Year (Thousands)\", x = \"Year\") +\n    theme_minimal() +\n    theme(\n        axis.title.y = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank()\n    )\n\n\n\n\n\n\n\n\nThe chart shows that, with the exception of 2021, there were between 230K and 255K crimes reported per year in LA. The number of crimes reported in 2021 deviates with only 146K crimes reported.\nI can also look at any seasonality across the different years. The code chunk below creates a line chart which shows the number of crimes recorded per month.\n\nchart_data &lt;- la_crimes %&gt;%\n  group_by(Year, Month) %&gt;%\n  summarise(count = n(), .groups = 'drop')\n\nggplot(chart_data, aes(x = Month, y = count, color = factor(Year), group = Year)) +\n  geom_line() +\n  geom_point() +\n  labs(title = \"Number of Crimes Recorded by Month\", x = \"Month\", y = \"Number of Crimes\", color = \"Year\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1), panel.grid.major.x = element_blank())\n\n\n\n\n\n\n\n\nVisually, the only recurring pattern I see is for the period from August to December where there is an alternate drop and rise. October is the month with the highest number of reported crimes in 2022 and 2023.\n\n\nNumber of Crimes By Type\nThe code below produces a table to show the top 20 crime types over the four year period based on the number of reports. As some of the descriptions are very long, I opted for a table than a chart to make sure they are all readable.\n\nchart_data &lt;- count(la_crimes, `Crime Type`) %&gt;%\n  arrange(desc(n)) %&gt;%\n  mutate(\n        Percentage = n / sum(n) * 100,\n        Running_Total_Percentage = cumsum(n / sum(n) * 100)\n    ) %&gt;%\n  slice_head(n = 20)\n\nchart_data &lt;- chart_data %&gt;%\n    rename(\n        `Number of Records` = n,\n        `Percentage of Total` = Percentage,\n        `Running Total Percentage` = Running_Total_Percentage\n    )\n\nprint(chart_data)\n\n# A tibble: 20 × 4\n   `Crime Type` `Number of Records` `Percentage of Total` Running Total Percen…¹\n   &lt;chr&gt;                      &lt;int&gt;                 &lt;dbl&gt;                  &lt;dbl&gt;\n 1 VEHICLE - S…               91496                 10.6                    10.6\n 2 BATTERY - S…               68879                  7.94                   18.5\n 3 THEFT OF ID…               55353                  6.38                   24.9\n 4 BURGLARY FR…               53415                  6.16                   31.0\n 5 BURGLARY                   53030                  6.12                   37.2\n 6 VANDALISM -…               52686                  6.08                   43.2\n 7 ASSAULT WIT…               49165                  5.67                   48.9\n 8 THEFT PLAIN…               44529                  5.14                   54.0\n 9 INTIMATE PA…               43319                  5.00                   59.0\n10 THEFT FROM …               33712                  3.89                   62.9\n11 THEFT FROM …               30216                  3.49                   66.4\n12 ROBBERY                    29634                  3.42                   69.8\n13 THEFT-GRAND…               28529                  3.29                   73.1\n14 VANDALISM -…               22945                  2.65                   75.8\n15 SHOPLIFTING…               21859                  2.52                   78.3\n16 CRIMINAL TH…               17786                  2.05                   80.3\n17 BRANDISH WE…               13390                  1.54                   81.9\n18 TRESPASSING                12847                  1.48                   83.4\n19 INTIMATE PA…               11763                  1.36                   84.7\n20 VIOLATION O…               10886                  1.26                   86.0\n# ℹ abbreviated name: ¹​`Running Total Percentage`\n\n\nThe table shows that the top type reported is from stolen vehicles, followed by battery and identity theft. These three collectively make up 25% of all the reports. While a tourist like me might not be at risk of the first type, witnessing or being in the vicinity of a vehicle being stolen is still something I want to avoid. The location of the crimes, are then probably more important for someone like me.\n\n\nNumber of Crimes By Premise\nThe code below creates a similar table which shows the number of reports by Premise_Type.\n\nchart_data &lt;- count(la_crimes, `Premise_Type`) %&gt;%\n  arrange(desc(n)) %&gt;%\n  mutate(\n        Percentage = n / sum(n) * 100,\n        Running_Total_Percentage = cumsum(n / sum(n) * 100)\n    )\n\nchart_data &lt;- chart_data %&gt;%\n    rename(\n        `Number of Records` = n,\n        `Percentage of Total` = Percentage,\n        `Running Total Percentage` = Running_Total_Percentage\n    )\n\nprint(chart_data)\n\n# A tibble: 186 × 4\n   Premise_Type `Number of Records` `Percentage of Total` Running Total Percen…¹\n   &lt;chr&gt;                      &lt;int&gt;                 &lt;dbl&gt;                  &lt;dbl&gt;\n 1 Residential               267393                 30.8                    30.8\n 2 STREET                    218432                 25.2                    56.0\n 3 PARKING LOT                59710                  6.89                   62.9\n 4 Bus (or Sto…               49163                  5.67                   68.6\n 5 SIDEWALK                   37215                  4.29                   72.9\n 6 VEHICLE, PA…               25513                  2.94                   75.8\n 7 GARAGE/CARP…               16781                  1.94                   77.8\n 8 DRIVEWAY                   14115                  1.63                   79.4\n 9 RESTAURANT/…               10932                  1.26                   80.7\n10 DEPARTMENT …               10883                  1.26                   81.9\n# ℹ 176 more rows\n# ℹ abbreviated name: ¹​`Running Total Percentage`\n\n\nThe largest portion of reports makes up 31% of the total and are from residential premises. These are not very concerning for me as there is little chance that I will be exposed to these types of crime as a tourist who is only checked into a hotel. The same is true for any other location which is not public or in a hotel.\nI can create a subset of the data to only include those that happened in public by removing the ones that I think happen in private. I define a list of values private to exclude and then use filter() to remove them in the code chunk below. The list is not comprehensive but should at least exclude the top ones that are in premises that a tourist like myself will not encounter.\n\nprivate &lt;- c(\"Residential\", \"GARAGE/CARPORT\", \"OFFICE BUILDING/OFFICE\",\n             \"WAREHOUSE\", \"CONSTRUCTION SITE\", \"MAIL BOX\", \"STORAGE SHED\",\n             \"MEDICAL/DENTAL OFFICES\", \"MISSIONS/SHELTERS\", \"TRANSIENT ENCAMPMENT\",\n             \"GROUP HOME\", \"PROJECT/TENEMENT/PUBLIC HOUSING\",\n             \"DETENTION/JAIL FACILITY\", \"MANUFACTURING COMPANY\",\n             \"SHORT-TERM VACATION RENTAL\", \"FOSTER HOME BOYS OR GIRLS*\",\n             \"WATER FACILITY\", \"HOSPICE\", \"GARMENT MANUFACTURER\",\n             \"RETIRED (DUPLICATE) DO NOT USE THIS CODE\")\n\nla_crimes_public &lt;- la_crimes %&gt;%\n    filter(!Premise_Type %in% private)\n\n# Display the resulting dataframe\nhead(la_crimes_public)\n\n# A tibble: 6 × 9\n  Date                `Crime Type`        latitude longitude Premise  Year Month\n  &lt;dttm&gt;              &lt;chr&gt;                  &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;ord&gt;\n1 2020-03-01 00:00:00 VEHICLE - STOLEN        34.0     -118. STREET   2020 March\n2 2020-02-08 00:00:00 BURGLARY FROM VEHI…     34.0     -118. BUS ST…  2020 Febr…\n3 2020-03-10 00:00:00 SHOPLIFTING-GRAND …     34.2     -118. CLOTHI…  2020 March\n4 2020-09-09 00:00:00 VEHICLE - STOLEN        34.1     -118. STREET   2020 Sept…\n5 2020-05-02 00:00:00 VEHICLE - STOLEN        34.1     -118. STREET   2020 May  \n6 2020-07-07 00:00:00 ARSON                   34.1     -118. STREET   2020 July \n# ℹ 2 more variables: Day_of_Week &lt;ord&gt;, Premise_Type &lt;chr&gt;\n\n\n\n\nNumber of Crimes - Public\nI now want to rerun some of the earlier charts with this new dataset. Will I see the same trends and top types of crimes for just the ‘public’ ones?\nThe code below recreates the number of crimes reported by month.\n\nchart_data &lt;- la_crimes_public %&gt;%\n  group_by(Year, Month) %&gt;%\n  summarise(count = n(), .groups = 'drop')\n\nggplot(chart_data, aes(x = Month, y = count, color = factor(Year), group = Year)) +\n  geom_line() +\n  geom_point() +\n  labs(title = \"Number of Public Crimes Recorded by Month\", x = \"Month\", y = \"Number of Crimes\", color = \"Year\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1), panel.grid.major.x = element_blank())\n\n\n\n\n\n\n\n\nThere is no big difference in the distribution of the crimes across years or months. I can still see some alternating in the latter part of the year and the three years still appear to have very similar number of crimes reported.\nThe code below then recreates the number of reports by the type of crime.\n\nchart_data &lt;- count(la_crimes_public, `Crime Type`) %&gt;%\n  arrange(desc(n)) %&gt;%\n  mutate(\n        Percentage = n / sum(n) * 100,\n        Running_Total_Percentage = cumsum(n / sum(n) * 100)\n    ) %&gt;%\n  slice_head(n = 20)\n\nchart_data &lt;- chart_data %&gt;%\n    rename(\n        `Number of Records` = n,\n        `Percentage of Total` = Percentage,\n        `Running Total Percentage` = Running_Total_Percentage\n    )\n\nprint(chart_data)\n\n# A tibble: 20 × 4\n   `Crime Type` `Number of Records` `Percentage of Total` Running Total Percen…¹\n   &lt;chr&gt;                      &lt;int&gt;                 &lt;dbl&gt;                  &lt;dbl&gt;\n 1 VEHICLE - S…               88381                15.5                     15.5\n 2 BURGLARY FR…               45118                 7.91                    23.4\n 3 BATTERY - S…               44514                 7.81                    31.2\n 4 VANDALISM -…               41455                 7.27                    38.5\n 5 ASSAULT WIT…               38470                 6.75                    45.2\n 6 THEFT FROM …               30819                 5.40                    50.6\n 7 ROBBERY                    27324                 4.79                    55.4\n 8 THEFT FROM …               25465                 4.47                    59.9\n 9 THEFT PLAIN…               24423                 4.28                    64.2\n10 BURGLARY                   22959                 4.03                    68.2\n11 SHOPLIFTING…               21707                 3.81                    72.0\n12 THEFT-GRAND…               17994                 3.16                    75.2\n13 VANDALISM -…               14805                 2.60                    77.8\n14 THEFT OF ID…               13135                 2.30                    80.1\n15 INTIMATE PA…               12980                 2.28                    82.3\n16 BRANDISH WE…                8888                 1.56                    83.9\n17 CRIMINAL TH…                8542                 1.50                    85.4\n18 TRESPASSING                 6197                 1.09                    86.5\n19 SHOPLIFTING…                4312                 0.756                   87.2\n20 ATTEMPTED R…                4194                 0.735                   88.0\n# ℹ abbreviated name: ¹​`Running Total Percentage`\n\n\nStolen vehicles still appear as the top type of crime, but we don’t see identity theft follow it anymore. Burglary from vehicles and battery follow as the second and third most frequent type of crime reported.\nMoving forward, I would want to limit the analysis to 2023, or the most recent full year. This limits the amount of data I need to run, and should not be an issue since it is the most updated, and there is no big deviation from the two other years. (not counting 2021)\nThe code below keeps only the records with a Year of 2023 in la_crimes_public\n\nla_crimes_public &lt;- la_crimes_public[la_crimes_public$Year == 2023, ]"
  },
  {
    "objectID": "posts/la-crimes/index.html#removing-duplicates",
    "href": "posts/la-crimes/index.html#removing-duplicates",
    "title": "Identifying Crime Hotspots in LA",
    "section": "Removing Duplicates",
    "text": "Removing Duplicates\nBefore proceeding, I need to check that there are no duplicated points in the data as some of the functions do not work in their presence. If there are duplicates, then some transformation is needed to make the data fit for use.\nThe check can be done using the duplicated() function and it needs to be done at the geometry or location of the points.\n\nany(duplicated(la_crimes_sf$geometry))\n\n[1] TRUE\n\n\nThe output shows that there is indeed some duplication in the point locations. We can introduce ‘jitter’ or a small random shift in order to ensure that points do not occupy the same space. For sf objects, this can be done with the st_jitter() function where an amount can be specified for the range of jitter to be introduced.\n\nla_crimes_jitt &lt;- st_jitter(la_crimes_sf$geometry, 5)\n\nWe can doublecheck that there is no more duplication using the same function. Note that the new object la_crimes_jitt only contains the geometry or the points, so we can pass the whole object into duplicated().\n\nany(duplicated(la_crimes_jitt))\n\n[1] FALSE"
  },
  {
    "objectID": "posts/la-crimes/index.html#data-transformations-for-kde",
    "href": "posts/la-crimes/index.html#data-transformations-for-kde",
    "title": "Identifying Crime Hotspots in LA",
    "section": "Data Transformations for KDE",
    "text": "Data Transformations for KDE\nAdditional data transformations are required as spatstat does not directly work with sf objects. One option is to use the ppp format. Converting the crime report data to this format takes a few steps.\nThe first step is to create an owin object of the map of LA which defines the geographic boundaries. This is done by passing the boundaries into as.owin(). As the boundaries are required, we can use a function like st_union() to ensure only one object’s boundaries are defined.\n\nla_owin &lt;- as.owin(st_union(st_geometry(la_district)))\n\nplot(la_owin)\n\n\n\n\n\n\n\n\nThe next code chunk completes the transformation by converting the crime locations into a ppp object using as.ppp() and then combining it with the owin using the second line of code.\n\nla_crimes_ppp &lt;- as.ppp(la_crimes_jitt)\nla_crimes_ppp &lt;- la_crimes_ppp[la_owin]\n\nplot(la_crimes_ppp)\n\n\n\n\n\n\n\n\nIf we use the data as is, we will see that all the units will be based on meters, which means that density values will be very small. We can convert the distance units to kilometers using rescale.ppp() and pass in a factor of 1000 as a parameter.\n\nla_crimes_ppp &lt;- rescale.ppp(la_crimes_ppp, 1000, \"km\")"
  },
  {
    "objectID": "posts/la-crimes/index.html#kde-with-fixed-bandwidth",
    "href": "posts/la-crimes/index.html#kde-with-fixed-bandwidth",
    "title": "Identifying Crime Hotspots in LA",
    "section": "KDE with Fixed Bandwidth",
    "text": "KDE with Fixed Bandwidth\nThe KDE can be computed using the density() function. This is specifically for computing the KDE where the is a user fixed bandwidth. This bandwidth is passed into the sigma argument of the function. There are a number of other possible arguments, but I am just using two of these in the upcoming code chunk:\n\nThe edge argument indicates whether edge correction is applied.\nThe kernel argument specifies the smoothing kernel, or the function used to smooth out and interpolate the density values. The process will only compute for the density of existing points, so there needs to be a way to calculate the density in between. The most common method used is gaussian which represents a normal curve.\n\nThe code chunks below compute the KDE for four different bandwidths and then produces the plots by passing them into the plot() function.\n\n\nBW = 250m\n\nkde_la_crimes_250m &lt;- density(la_crimes_ppp,\n                         sigma = 0.25,\n                         edge = TRUE,\n                         kernel = \"gaussian\")\nplot(kde_la_crimes_250m, main = \"KDE with 250m bandwidth\")\n\n\n\n\n\n\n\n\n\n\nBW = 500m\n\nkde_la_crimes_500m &lt;- density(la_crimes_ppp,\n                         sigma = 0.5,\n                         edge = TRUE,\n                         kernel = \"gaussian\")\nplot(kde_la_crimes_500m, main = \"KDE with 500m bandwidth\")\n\n\n\n\n\n\n\n\n\n\nBW = 1km\n\nkde_la_crimes_1km &lt;- density(la_crimes_ppp,\n                         sigma = 1,\n                         edge = TRUE,\n                         kernel = \"gaussian\")\nplot(kde_la_crimes_1km, main = \"KDE with 1km bandwidth\")\n\n\n\n\n\n\n\n\n\n\nBW = 5km\n\nkde_la_crimes_5km &lt;- density(la_crimes_ppp,\n                         sigma = 5,\n                         edge = TRUE,\n                         kernel = \"gaussian\")\nplot(kde_la_crimes_5km, main = \"KDE with 5km bandwidth\")\n\n\n\n\n\n\n\n\n\n\nThe plots continue to show that the area around downtown LA has the highest density. The largest bandwidth produces a very smooth gradient which is not useful for identifying specific locations as hotspots. The lower bandwidths are more useful in this purpose.\nThese charts continue to show that the central area, including Downtown LA, is where the crime reports are most concentrated. There are a few spots in the north which have high incidence of reports, but most of the northern part is generally the least dense in terms of the number of crime reports.\nNote that there are more formal or scientific methods to determine the ideal bandwidth, but I will not cover them in this post."
  },
  {
    "objectID": "posts/la-crimes/index.html#kde-with-adaptive-bandwidth",
    "href": "posts/la-crimes/index.html#kde-with-adaptive-bandwidth",
    "title": "Identifying Crime Hotspots in LA",
    "section": "KDE with Adaptive Bandwidth",
    "text": "KDE with Adaptive Bandwidth\nThe downside of using a fixed bandwidth is that it might make it hard to see variations where points are dense, while making it produce large variances where points are scarce. An adaptive bandwidth can solve this downside as it adjusts the bandwidth used depending on the density of the points in the region.\nThe KDE with adaptive bandwidth can be computed by using adaptive.density(). The method argument specifies the estimation method used and can take on one of three values: “kernel”, “voronoi” or “nearest”.\n\nkde_la_crimes_adaptive &lt;- adaptive.density(la_crimes_ppp, method = \"kernel\")\n\n\nwrite_rds(kde_la_crimes_adaptive, \"data/rds/kde_la_crimes_adaptive.rds\")\n\n\nplot(kde_la_crimes_adaptive, main = \"KDE with Adaptive bandwidth\")\n\n\n\n\n\n\n\n\nThe output continues to highlight downtown LA as a hotspot, but also produced some hotspots scattered across the map which can be thought of as local hotspots."
  },
  {
    "objectID": "posts/la-crimes/index.html#deriving-weights",
    "href": "posts/la-crimes/index.html#deriving-weights",
    "title": "Identifying Crime Hotspots in LA",
    "section": "Deriving Weights",
    "text": "Deriving Weights\nInstead of just looking at each cells’ or objects value, spatial autocorrelation looks at that object together with its neighbors. The ‘value’ of a cell is then an average, sum or some combination of its and its neighbors’ values depending on the weights.\nThe first step is deriving the list of neighbors per neighborhood. If we observe the map, we see that some of the borders overlap and some of the borders don’t even touch. This makes the use of functions like st_contiguity(), which derives a list of neighbors based on adjacency, infeasible without cleaning up the data.\n\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\ntm_shape(la_nbhood) +\n  tm_borders(\"black\") +\n  tm_fill(\"lightblue\") +\ntm_shape(hotel_nbhood) +\n  tm_fill(\"green\")\n\n\n\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\n\nInstead of using contiguity, we will use distance to determine the list of neighbors. The first step is to determine the ideal distance to use. This ideal distance should ensure that each neighborhood has a neighbor.\n\nla_nbhood_poly &lt;- st_cast(la_nbhood, \"POLYGON\")\n\nWarning in st_cast.MULTIPOLYGON(X[[i]], ...): polygon from first part only\nWarning in st_cast.MULTIPOLYGON(X[[i]], ...): polygon from first part only\n\ncentroid_x &lt;- map_dbl(la_nbhood_poly$geometry, ~st_centroid(.x)[[1]])\ncentroid_y &lt;- map_dbl(la_nbhood_poly$geometry, ~st_centroid(.x)[[2]])\ncoords &lt;- cbind(centroid_x, centroid_y)\nhead(coords)\n\n     centroid_x centroid_y\n[1,]   144466.2  -418250.3\n[2,]   165487.4  -433628.8\n[3,]   157666.4  -429928.5\n[4,]   144533.1  -433198.1\n[5,]   165313.5  -440626.4\n[6,]   159993.3  -447374.8\n\n\nNext, I need to find the distance of each neighborhood to its nearest neighbors– which does not necessarily require them to be touching on the map. For this, I would need functions from the spdep package. The code below goes through the following:\n\nI look for each neighborhood’s nearest neighbors using knearneigh(). The output is converted into a neighbor class, nb, using knn2nb()\nReturn the distance to a neighbor using nbdists()\n\n\npacman::p_load(spdep)\n\nk1 &lt;- knn2nb(knearneigh(coords))\nk1dists &lt;- unlist(nbdists(k1, coords))\nsummary(k1dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  751.6  1731.0  2280.4  2546.8  3093.7  6088.8 \n\n\nThe output shows that the largest distance to a neighbor is 6088.8, which means that a distance of at least that number should ensure that each neighborhood will find at least one neighbor. I will use 6090.\nTo return a list of distance based neighbors, I use st_dist_band() in the code chunk below. This needs to be applied to the centroids so I create a new object for the neighborhood centroids and then define a range of 0 to 6090 meters for the neighbors.\n\nla_nbhood_centroids &lt;- st_centroid(la_nbhood)\n\nWarning: st_centroid assumes attributes are constant over geometries\n\nla_nbhood_centroids$nb &lt;- st_dist_band(la_nbhood_centroids,\n                                       lower = 0,\n                                       upper = 6090)\n\nThe next step is to assign weights which can be done using st_weights(). It requires a neighbor list as an input and then I have used “W” as an input to the style argument to specify equal weights to be used.\n\nla_nbhood_centroids$wt &lt;- st_weights(la_nbhood_centroids$nb, style = \"W\")"
  },
  {
    "objectID": "posts/la-crimes/index.html#performing-local-morans-i-test",
    "href": "posts/la-crimes/index.html#performing-local-morans-i-test",
    "title": "Identifying Crime Hotspots in LA",
    "section": "Performing Local Moran’s I Test",
    "text": "Performing Local Moran’s I Test\nI will be using Moran’s I as the local statistic to be used which is useful for identifying outliers and clusters. This can be done using the local_moran() function of sfdep. The function needs three required arguments, the values to be used, the neighbor list and the weights. The nsim argument instructs the test to perform a number of simulations. I defined a seed value at the top of the block in order to make sure I am able to reproduce the results.\n\nset.seed(1234)\n\nla_nbhood_centroids &lt;- la_nbhood_centroids %&gt;%\n  mutate(local_moran = local_moran(\n    Crimes, nb, wt, nsim = 99),\n    .before = 1) %&gt;%\n  unnest(local_moran)\n\nNote that the dataframe used does not contain the actual neighborhood maps so it is not suitable for mapping. We can transfer the results, which are in the first 11 columns of the output, into the original neighborhood map using the following code.\n\nresults &lt;- as.data.frame(la_nbhood_centroids)[1:11]\nla_nbhood_moran &lt;- bind_cols(la_nbhood, results)"
  },
  {
    "objectID": "posts/la-crimes/index.html#visualising-local-indicators",
    "href": "posts/la-crimes/index.html#visualising-local-indicators",
    "title": "Identifying Crime Hotspots in LA",
    "section": "Visualising Local Indicators",
    "text": "Visualising Local Indicators\nWe can visualize the results by using the tmap package. The test statistic is stored in the column ii while the p values for the simulations are stored in p_ii_sim. In general, values above zero for the test statistics indicate clustering while those below indicate dispersion.\n\ntm_shape(la_nbhood_moran) +\n  tm_fill(c(\"ii\", \"p_ii_sim\"), title = c(\"Local Moran's I\",\"P Value\")) +\n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(\n    main.title = \"Local Moran's I and P-values\",\n    legend.position = c(\"left\", \"bottom\"))\n\nVariable(s) \"ii\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\n\n\n\n\nOne useful output of local_moran() is the classification of elements. One is stored in the column mean. This classifies clusters and outliers and indicates whether they have high values or low. Note that these classifications are available for all elements regardless of whether the test finds them being significant.\nThe code chunk below produces an interactive map that shows the classifications for the neighborhoods where the p-value is less than 5%. The classification indicates clusters as “Low-Low” or “High-High”, while the two other classifications for outliers. Where there is a high valued neighborhood among low valued ones, it will show as “High-Low”. For the reverse, it will show “Low-High”.\n\nla_nbhood_moran_sig &lt;- la_nbhood_moran %&gt;%\n  filter(p_ii &lt; 0.05)\n\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\ntm_shape(la_nbhood_moran) +\n  tm_polygons(id = \"name\") +\n  tm_borders(alpha = 0.5) +\ntm_shape(la_nbhood_moran_sig) +\n  tm_fill(\"mean\", id = \"name\") +\n  tm_borders(alpha = 0.4)\n\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them).\n\n\n\n\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\n\nThe results show that there is a statistically significant cluster of with high crime report density– again with Downtown LA. There are four outliers identified around this cluster’s borders. There is also a significant low density cluster of neighborhoods a little north of the high density cluster."
  }
]